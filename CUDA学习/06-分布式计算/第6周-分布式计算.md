# 第6周：分布式计算

## 学习目标

掌握多GPU和分布式计算技术，包括GPU间通信、分布式计算框架、负载均衡等。

## 学习内容

### 1. 多GPU编程

#### 1.1 多GPU内存管理
```cuda
#include <cuda_runtime.h>
#include <iostream>

// 多GPU内存管理
class MultiGPUManager {
private:
    int deviceCount;
    int* devices;
    
public:
    MultiGPUManager() {
        cudaGetDeviceCount(&deviceCount);
        devices = new int[deviceCount];
        
        for (int i = 0; i < deviceCount; i++) {
            devices[i] = i;
        }
        
        printf("Found %d CUDA devices\n", deviceCount);
        for (int i = 0; i < deviceCount; i++) {
            cudaDeviceProp prop;
            cudaGetDeviceProperties(&prop, i);
            printf("Device %d: %s\n", i, prop.name);
        }
    }
    
    ~MultiGPUManager() {
        delete[] devices;
    }
    
    int getDeviceCount() const {
        return deviceCount;
    }
    
    void setDevice(int device) {
        cudaSetDevice(device);
    }
    
    void allocateMemory(int device, void** ptr, size_t size) {
        cudaSetDevice(device);
        cudaMalloc(ptr, size);
    }
    
    void freeMemory(int device, void* ptr) {
        cudaSetDevice(device);
        cudaFree(ptr);
    }
};
```

#### 1.2 GPU间通信
```cuda
// GPU间点对点通信
__global__ void gpuToGpuKernel(float* data, int size) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < size) {
        data[tid] = data[tid] * 2.0f;
    }
}

void gpuToGpuCommunication() {
    MultiGPUManager manager;
    
    if (manager.getDeviceCount() < 2) {
        printf("Need at least 2 GPUs for communication\n");
        return;
    }
    
    int size = 1024 * sizeof(float);
    float* d_data0, *d_data1;
    
    // 在GPU 0上分配内存
    manager.setDevice(0);
    cudaMalloc(&d_data0, size);
    
    // 在GPU 1上分配内存
    manager.setDevice(1);
    cudaMalloc(&d_data1, size);
    
    // 初始化数据
    float* h_data = new float[1024];
    for (int i = 0; i < 1024; i++) {
        h_data[i] = i;
    }
    
    // 复制数据到GPU 0
    manager.setDevice(0);
    cudaMemcpy(d_data0, h_data, size, cudaMemcpyHostToDevice);
    
    // GPU 0到GPU 1的直接传输
    cudaMemcpyPeer(d_data1, 1, d_data0, 0, size);
    
    // 在GPU 1上处理数据
    manager.setDevice(1);
    gpuToGpuKernel<<<4, 256>>>(d_data1, 1024);
    
    // 复制结果回CPU
    cudaMemcpy(h_data, d_data1, size, cudaMemcpyDeviceToHost);
    
    printf("Result: %f\n", h_data[0]);
    
    // 清理
    manager.setDevice(0);
    cudaFree(d_data0);
    manager.setDevice(1);
    cudaFree(d_data1);
    delete[] h_data;
}
```

#### 1.3 多GPU并行计算
```cuda
// 多GPU并行计算示例
__global__ void parallelComputationKernel(float* data, int size, int offset) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < size) {
        data[tid] = data[tid] * 2.0f + offset;
    }
}

void multiGPUParallelComputation() {
    MultiGPUManager manager;
    int deviceCount = manager.getDeviceCount();
    
    if (deviceCount < 2) {
        printf("Need at least 2 GPUs for parallel computation\n");
        return;
    }
    
    int totalSize = 1024;
    int sizePerGPU = totalSize / deviceCount;
    
    float* h_data = new float[totalSize];
    float** d_data = new float*[deviceCount];
    
    // 初始化数据
    for (int i = 0; i < totalSize; i++) {
        h_data[i] = i;
    }
    
    // 为每个GPU分配内存
    for (int i = 0; i < deviceCount; i++) {
        manager.setDevice(i);
        cudaMalloc(&d_data[i], sizePerGPU * sizeof(float));
        
        // 复制数据到GPU
        cudaMemcpy(d_data[i], h_data + i * sizePerGPU, 
                   sizePerGPU * sizeof(float), cudaMemcpyHostToDevice);
    }
    
    // 在每个GPU上并行计算
    for (int i = 0; i < deviceCount; i++) {
        manager.setDevice(i);
        parallelComputationKernel<<<4, 256>>>(d_data[i], sizePerGPU, i);
    }
    
    // 同步所有GPU
    for (int i = 0; i < deviceCount; i++) {
        manager.setDevice(i);
        cudaDeviceSynchronize();
    }
    
    // 收集结果
    for (int i = 0; i < deviceCount; i++) {
        manager.setDevice(i);
        cudaMemcpy(h_data + i * sizePerGPU, d_data[i], 
                   sizePerGPU * sizeof(float), cudaMemcpyDeviceToHost);
    }
    
    printf("Result: %f\n", h_data[0]);
    
    // 清理
    for (int i = 0; i < deviceCount; i++) {
        manager.setDevice(i);
        cudaFree(d_data[i]);
    }
    delete[] d_data;
    delete[] h_data;
}
```

### 2. 分布式计算框架

#### 2.1 MPI集成
```cuda
#include <mpi.h>
#include <cuda_runtime.h>

// MPI + CUDA 分布式计算
class MPICUDADistributed {
private:
    int rank, size;
    int localGPU;
    
public:
    MPICUDADistributed() {
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        MPI_Comm_size(MPI_COMM_WORLD, &size);
        
        // 每个进程使用不同的GPU
        localGPU = rank % 2;  // 假设有2个GPU
        cudaSetDevice(localGPU);
        
        printf("Process %d using GPU %d\n", rank, localGPU);
    }
    
    void distributedMatrixMultiply(int N) {
        // 计算每个进程负责的行数
        int rowsPerProcess = N / size;
        int startRow = rank * rowsPerProcess;
        int endRow = (rank == size - 1) ? N : (rank + 1) * rowsPerProcess;
        int localRows = endRow - startRow;
        
        // 分配内存
        float* h_A = new float[localRows * N];
        float* h_B = new float[N * N];
        float* h_C = new float[localRows * N];
        
        float* d_A, *d_B, *d_C;
        cudaMalloc(&d_A, localRows * N * sizeof(float));
        cudaMalloc(&d_B, N * N * sizeof(float));
        cudaMalloc(&d_C, localRows * N * sizeof(float));
        
        // 初始化数据
        for (int i = 0; i < localRows; i++) {
            for (int j = 0; j < N; j++) {
                h_A[i * N + j] = (startRow + i) * N + j;
            }
        }
        
        for (int i = 0; i < N; i++) {
            for (int j = 0; j < N; j++) {
                h_B[i * N + j] = i * N + j;
            }
        }
        
        // 复制到GPU
        cudaMemcpy(d_A, h_A, localRows * N * sizeof(float), cudaMemcpyHostToDevice);
        cudaMemcpy(d_B, h_B, N * N * sizeof(float), cudaMemcpyHostToDevice);
        
        // GPU矩阵乘法
        dim3 blockSize(16, 16);
        dim3 gridSize((N + blockSize.x - 1) / blockSize.x,
                      (localRows + blockSize.y - 1) / blockSize.y);
        
        matrixMultiplyKernel<<<gridSize, blockSize>>>(d_A, d_B, d_C, localRows, N, N);
        
        // 复制结果回CPU
        cudaMemcpy(h_C, d_C, localRows * N * sizeof(float), cudaMemcpyDeviceToHost);
        
        // 收集结果到进程0
        if (rank == 0) {
            float* h_result = new float[N * N];
            
            // 复制自己的结果
            memcpy(h_result, h_C, localRows * N * sizeof(float));
            
            // 接收其他进程的结果
            for (int i = 1; i < size; i++) {
                int otherRows = (i == size - 1) ? N - i * rowsPerProcess : rowsPerProcess;
                MPI_Recv(h_result + i * rowsPerProcess * N, 
                         otherRows * N, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }
            
            printf("Matrix multiplication completed\n");
            delete[] h_result;
        } else {
            // 发送结果到进程0
            MPI_Send(h_C, localRows * N, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);
        }
        
        // 清理
        cudaFree(d_A);
        cudaFree(d_B);
        cudaFree(d_C);
        delete[] h_A;
        delete[] h_B;
        delete[] h_C;
    }
};
```

#### 2.2 NCCL集成
```cuda
#include <nccl.h>
#include <cuda_runtime.h>

// NCCL分布式通信
class NCCLDistributed {
private:
    ncclComm_t comm;
    int rank, size;
    
public:
    NCCLDistributed() {
        // 初始化NCCL
        ncclUniqueId ncclId;
        if (rank == 0) {
            ncclGetUniqueId(&ncclId);
        }
        
        MPI_Bcast(&ncclId, sizeof(ncclId), MPI_BYTE, 0, MPI_COMM_WORLD);
        
        ncclCommInitRank(&comm, size, ncclId, rank);
        
        printf("NCCL initialized for rank %d\n", rank);
    }
    
    ~NCCLDistributed() {
        ncclCommDestroy(comm);
    }
    
    void allReduce(float* data, int size) {
        float* d_data;
        cudaMalloc(&d_data, size * sizeof(float));
        cudaMemcpy(d_data, data, size * sizeof(float), cudaMemcpyHostToDevice);
        
        // NCCL AllReduce
        ncclAllReduce(d_data, d_data, size, ncclFloat, ncclSum, comm, 0);
        
        cudaMemcpy(data, d_data, size * sizeof(float), cudaMemcpyDeviceToHost);
        cudaFree(d_data);
    }
    
    void allGather(float* sendData, float* recvData, int sendSize) {
        float* d_send, *d_recv;
        cudaMalloc(&d_send, sendSize * sizeof(float));
        cudaMalloc(&d_recv, sendSize * size * sizeof(float));
        
        cudaMemcpy(d_send, sendData, sendSize * sizeof(float), cudaMemcpyHostToDevice);
        
        // NCCL AllGather
        ncclAllGather(d_send, d_recv, sendSize, ncclFloat, comm, 0);
        
        cudaMemcpy(recvData, d_recv, sendSize * size * sizeof(float), cudaMemcpyDeviceToHost);
        
        cudaFree(d_send);
        cudaFree(d_recv);
    }
};
```

### 3. 负载均衡

#### 3.1 动态负载均衡
```cuda
// 动态负载均衡器
class DynamicLoadBalancer {
private:
    int deviceCount;
    float* deviceLoad;
    int* deviceTasks;
    
public:
    DynamicLoadBalancer(int deviceCount) : deviceCount(deviceCount) {
        deviceLoad = new float[deviceCount];
        deviceTasks = new int[deviceCount];
        
        for (int i = 0; i < deviceCount; i++) {
            deviceLoad[i] = 0.0f;
            deviceTasks[i] = 0;
        }
    }
    
    ~DynamicLoadBalancer() {
        delete[] deviceLoad;
        delete[] deviceTasks;
    }
    
    int getBestDevice() {
        int bestDevice = 0;
        float minLoad = deviceLoad[0];
        
        for (int i = 1; i < deviceCount; i++) {
            if (deviceLoad[i] < minLoad) {
                minLoad = deviceLoad[i];
                bestDevice = i;
            }
        }
        
        return bestDevice;
    }
    
    void updateLoad(int device, float load) {
        deviceLoad[device] += load;
        deviceTasks[device]++;
    }
    
    void printStats() {
        printf("Device Load Statistics:\n");
        for (int i = 0; i < deviceCount; i++) {
            printf("Device %d: Load=%.2f, Tasks=%d\n", i, deviceLoad[i], deviceTasks[i]);
        }
    }
};
```

#### 3.2 任务分配策略
```cuda
// 任务分配策略
class TaskScheduler {
private:
    DynamicLoadBalancer* balancer;
    int totalTasks;
    
public:
    TaskScheduler(int deviceCount) {
        balancer = new DynamicLoadBalancer(deviceCount);
        totalTasks = 0;
    }
    
    ~TaskScheduler() {
        delete balancer;
    }
    
    void scheduleTask(int taskSize) {
        int device = balancer->getBestDevice();
        
        // 执行任务
        executeTaskOnDevice(device, taskSize);
        
        // 更新负载
        balancer->updateLoad(device, taskSize);
        totalTasks++;
    }
    
    void executeTaskOnDevice(int device, int taskSize) {
        cudaSetDevice(device);
        
        // 模拟任务执行
        float* d_data;
        cudaMalloc(&d_data, taskSize * sizeof(float));
        
        // 简单的计算任务
        dim3 blockSize(256);
        dim3 gridSize((taskSize + blockSize.x - 1) / blockSize.x);
        
        simpleTaskKernel<<<gridSize, blockSize>>>(d_data, taskSize);
        
        cudaFree(d_data);
    }
    
    void printStats() {
        balancer->printStats();
        printf("Total tasks scheduled: %d\n", totalTasks);
    }
};

__global__ void simpleTaskKernel(float* data, int size) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < size) {
        data[tid] = tid * 2.0f;
    }
}
```

### 4. 分布式算法

#### 4.1 分布式排序
```cuda
// 分布式排序算法
class DistributedSort {
private:
    int rank, size;
    
public:
    DistributedSort() {
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        MPI_Comm_size(MPI_COMM_WORLD, &size);
    }
    
    void distributedSort(float* data, int size) {
        int localSize = size / this->size;
        int startIdx = rank * localSize;
        int endIdx = (rank == this->size - 1) ? size : (rank + 1) * localSize;
        int localSizeActual = endIdx - startIdx;
        
        float* localData = new float[localSizeActual];
        
        // 分发数据
        if (rank == 0) {
            // 主进程分发数据
            for (int i = 1; i < this->size; i++) {
                int otherStart = i * localSize;
                int otherEnd = (i == this->size - 1) ? size : (i + 1) * localSize;
                int otherSize = otherEnd - otherStart;
                
                MPI_Send(data + otherStart, otherSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
            }
            
            // 复制自己的数据
            memcpy(localData, data + startIdx, localSizeActual * sizeof(float));
        } else {
            // 其他进程接收数据
            MPI_Recv(localData, localSizeActual, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        
        // 本地排序
        localSort(localData, localSizeActual);
        
        // 归并排序
        mergeSort(localData, localSizeActual);
        
        // 收集结果
        if (rank == 0) {
            // 主进程收集结果
            memcpy(data + startIdx, localData, localSizeActual * sizeof(float));
            
            for (int i = 1; i < this->size; i++) {
                int otherStart = i * localSize;
                int otherEnd = (i == this->size - 1) ? size : (i + 1) * localSize;
                int otherSize = otherEnd - otherStart;
                
                MPI_Recv(data + otherStart, otherSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }
        } else {
            // 其他进程发送结果
            MPI_Send(localData, localSizeActual, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);
        }
        
        delete[] localData;
    }
    
    void localSort(float* data, int size) {
        // 使用GPU进行本地排序
        float* d_data;
        cudaMalloc(&d_data, size * sizeof(float));
        cudaMemcpy(d_data, data, size * sizeof(float), cudaMemcpyHostToDevice);
        
        // 使用Thrust排序
        thrust::device_ptr<float> d_ptr(d_data);
        thrust::sort(d_ptr, d_ptr + size);
        
        cudaMemcpy(data, d_data, size * sizeof(float), cudaMemcpyDeviceToHost);
        cudaFree(d_data);
    }
    
    void mergeSort(float* data, int size) {
        // 实现分布式归并排序
        // 这里简化实现
        for (int step = 1; step < this->size; step *= 2) {
            if (rank % (2 * step) == 0) {
                // 接收数据并归并
                int partner = rank + step;
                if (partner < this->size) {
                    int partnerSize = size / this->size;
                    float* partnerData = new float[partnerSize];
                    
                    MPI_Recv(partnerData, partnerSize, MPI_FLOAT, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                    
                    // 归并两个有序数组
                    merge(data, size, partnerData, partnerSize);
                    
                    delete[] partnerData;
                }
            } else {
                // 发送数据
                int partner = rank - step;
                MPI_Send(data, size, MPI_FLOAT, partner, 0, MPI_COMM_WORLD);
                break;
            }
        }
    }
    
    void merge(float* data, int size1, float* data2, int size2) {
        float* temp = new float[size1 + size2];
        int i = 0, j = 0, k = 0;
        
        while (i < size1 && j < size2) {
            if (data[i] <= data2[j]) {
                temp[k++] = data[i++];
            } else {
                temp[k++] = data2[j++];
            }
        }
        
        while (i < size1) {
            temp[k++] = data[i++];
        }
        
        while (j < size2) {
            temp[k++] = data2[j++];
        }
        
        memcpy(data, temp, (size1 + size2) * sizeof(float));
        delete[] temp;
    }
};
```

## 实践项目

### 项目1：多GPU并行计算
实现多GPU并行计算系统，包括负载均衡和任务分配。

### 项目2：分布式计算框架
实现基于MPI和NCCL的分布式计算框架。

### 项目3：分布式算法
实现分布式排序、归约等算法。

## 每日学习任务

### 第1天：多GPU编程
- 学习多GPU内存管理
- 掌握GPU间通信
- 理解多GPU并行计算

### 第2天：GPU间通信
- 学习GPU间点对点通信
- 掌握cudaMemcpyPeer使用
- 理解GPU间数据传输

### 第3天：分布式计算框架
- 学习MPI集成
- 掌握NCCL使用
- 理解分布式通信

### 第4天：负载均衡
- 学习动态负载均衡
- 掌握任务分配策略
- 理解负载均衡算法

### 第5天：分布式算法
- 学习分布式排序
- 掌握分布式归约
- 理解分布式算法设计

### 第6天：性能优化
- 学习分布式性能优化
- 掌握通信优化技术
- 理解可扩展性设计

### 第7天：综合练习
- 完成所有实践项目
- 综合运用分布式技术
- 准备下周学习

## 检查点

### 第6周结束时的能力要求
- [ ] 能够实现多GPU编程
- [ ] 掌握GPU间通信技术
- [ ] 能够使用分布式计算框架
- [ ] 掌握负载均衡技术
- [ ] 能够实现分布式算法
- [ ] 理解分布式性能优化
- [ ] 完成项目1-3
- [ ] 具备分布式计算能力

## 常见问题解答

### Q: 多GPU编程如何优化？
A: 使用GPU间直接传输，合理分配任务，避免不必要的CPU-GPU传输。

### Q: 分布式计算如何选择框架？
A: 根据应用需求选择：MPI适合通用分布式计算，NCCL适合GPU集群。

### Q: 负载均衡如何实现？
A: 监控各GPU负载，动态分配任务，使用合适的调度算法。

### Q: 分布式算法如何设计？
A: 考虑通信开销，减少数据传输，使用合适的同步机制。

---

**学习时间**：第6周  
**预计完成时间**：2024-03-22
