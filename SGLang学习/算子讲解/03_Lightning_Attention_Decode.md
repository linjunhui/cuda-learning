# Lightning Attention Decode ç®—å­è¯¦è§£

## ğŸ“– ç®—å­æ¦‚è¿°

**Lightning Attention Decode** æ˜¯ LLM **è§£ç é˜¶æ®µ**çš„æ³¨æ„åŠ›è®¡ç®—ç®—å­ã€‚ç›¸æ¯”é¢„å¡«å……é˜¶æ®µçš„æ³¨æ„åŠ›ï¼Œè§£ç é˜¶æ®µæ›´ç®€å•ä½†æ›´é¢‘ç¹ï¼ˆæ¯ä¸ª token éƒ½è¦è®¡ç®—ä¸€æ¬¡ï¼‰ã€‚

**ç”¨é€”**ï¼š
- LLM ç”Ÿæˆæ—¶çš„é€ token æ³¨æ„åŠ›è®¡ç®—
- KV Cache çš„å¢é‡æ›´æ–°
- æ”¯æŒæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆé€šè¿‡è¡°å‡å› å­ï¼‰

**ç‰¹ç‚¹**ï¼š
- **å¢é‡è®¡ç®—**ï¼šåªè®¡ç®—å½“å‰ token çš„æ³¨æ„åŠ›
- **KV Cache æ›´æ–°**ï¼šèåˆæ›´æ–°æ“ä½œï¼Œå‡å°‘å†…å­˜è®¿é—®
- **å…±äº«å†…å­˜ä¼˜åŒ–**ï¼šqã€kã€v è½½å…¥å…±äº«å†…å­˜å¤ç”¨
- **é«˜æ•ˆè®¾è®¡**ï¼šä¸“é—¨ä¸ºè§£ç é˜¶æ®µä¼˜åŒ–

---

## ğŸ”¢ å…¬å¼ä¸ç®—æ³•

### æ•°å­¦å…¬å¼

#### æ ‡å‡†æ³¨æ„åŠ›å…¬å¼ï¼ˆè§£ç é˜¶æ®µï¼‰

å¯¹äºå½“å‰ token `t`ï¼š

```
Attention(Q_t, K, V) = softmax(Q_t @ K^T / âˆšd_k) @ V
```

å…¶ä¸­ï¼š
- `Q_t`ï¼šå½“å‰ token çš„ queryï¼ˆå½¢çŠ¶ `[1, heads, d]`ï¼‰
- `K`ï¼šæ‰€æœ‰è¿‡å» token çš„ keyï¼ˆå½¢çŠ¶ `[seq_len, heads, d]`ï¼‰
- `V`ï¼šæ‰€æœ‰è¿‡å» token çš„ valueï¼ˆå½¢çŠ¶ `[seq_len, heads, d]`ï¼‰

#### æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆè¡°å‡ï¼‰

Lightning Attention ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œé€šè¿‡è¡°å‡å› å­ï¼š

```
new_kv = ratio * old_kv + k_t @ v_t^T
```

å…¶ä¸­ï¼š
- `ratio = exp(-slope)`ï¼šè¡°å‡å› å­
- `k_t`ï¼šå½“å‰ token çš„ key
- `v_t`ï¼šå½“å‰ token çš„ value
- `old_kv`ï¼šè¿‡å»çš„ `k @ v^T` ç´¯ç§¯

#### æ³¨æ„åŠ›è¾“å‡º

```
output = Q_t @ new_kv
```

**å…³é”®ç‚¹**ï¼š
- `new_kv` çš„å½¢çŠ¶æ˜¯ `[qk_dim, v_dim]`ï¼ˆä¸æ˜¯ `[seq_len, qk_dim, v_dim]`ï¼‰
- è¿™æ˜¯ä¸€ä¸ª**çŸ©é˜µ-å‘é‡ä¹˜æ³•**ï¼š`q` (å‘é‡) Ã— `kv` (çŸ©é˜µ) = `output` (å‘é‡)

---

## ğŸ§  ç®—æ³•åŸç†

### æ ¸å¿ƒæ€æƒ³

è§£ç é˜¶æ®µçš„æ³¨æ„åŠ›ä¸é¢„å¡«å……é˜¶æ®µä¸åŒï¼š

| é˜¶æ®µ | Query | Key/Value | è¾“å‡º |
|------|-------|-----------|------|
| **é¢„å¡«å……** | `[seq_len, heads, d]` | `[seq_len, heads, d]` | `[seq_len, heads, d]` |
| **è§£ç ** | `[1, heads, d]` | `[seq_len, heads, d]` | `[1, heads, d]` |

**å…³é”®å·®å¼‚**ï¼š
- è§£ç é˜¶æ®µæ¯æ¬¡åªå¤„ç†**ä¸€ä¸ª token**
- å¯ä»¥åˆ©ç”¨è¿™ä¸ªç‰¹ç‚¹åšå¾ˆå¤šä¼˜åŒ–

### ç®—æ³•æµç¨‹

```
1. åŠ è½½å½“å‰ token çš„ q, k, v åˆ°å…±äº«å†…å­˜
   â†“
2. è®¡ç®—æ–°çš„ KVï¼šnew_kv = ratio * old_kv + k @ v^T
   â†“
3. æ›´æ–° KV Cacheï¼špast_kv = new_kv
   â†“
4. è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼šoutput = q @ new_kv
```

### æ»‘åŠ¨çª—å£æœºåˆ¶

**ä¼ ç»Ÿæ³¨æ„åŠ›**ï¼š
- éœ€è¦å­˜å‚¨æ‰€æœ‰å†å² token çš„ kã€v
- å†…å­˜å ç”¨ï¼šO(seq_len)

**æ»‘åŠ¨çª—å£æ³¨æ„åŠ›**ï¼š
- åªå­˜å‚¨ç´¯ç§¯çš„ `k @ v^T`
- é€šè¿‡è¡°å‡å› å­è‡ªåŠ¨"å¿˜è®°"æ—§ä¿¡æ¯
- å†…å­˜å ç”¨ï¼šO(1)

**è¡°å‡å…¬å¼**ï¼š
```
new_kv = exp(-slope) * old_kv + k_new @ v_new^T
```

**å«ä¹‰**ï¼š
- `slope = 0`ï¼šä¸è¡°å‡ï¼Œè®°ä½æ‰€æœ‰å†å²
- `slope > 0`ï¼šé€æ¸å¿˜è®°æ—§ä¿¡æ¯
- `slope = âˆ`ï¼šåªè®°ä½å½“å‰ tokenï¼ˆç›¸å½“äºæ— å†å²ï¼‰

### çŸ©é˜µä¹˜æ³•ä¼˜åŒ–

**è®¡ç®—**ï¼š`output = q @ kv`

```
q:      [qk_dim]           (å‘é‡)
kv:     [qk_dim, v_dim]    (çŸ©é˜µ)
output: [v_dim]            (å‘é‡)
```

**å¹¶è¡ŒåŒ–**ï¼š
- æ¯ä¸ªçº¿ç¨‹è®¡ç®— `output` çš„ä¸€ä¸ªå…ƒç´ 
- çº¿ç¨‹ `i` è®¡ç®—ï¼š`output[i] = Î£(q[j] * kv[j][i])` for all j

---

## ğŸ’» ä»£ç å®ç°

### æºç ä½ç½®

`SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu`

### å®Œæ•´ Kernel ä»£ç 

```25:113:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
template <typename T>
__global__ void lightning_attention_decode_kernel(
    const T* __restrict__ q,            // [b, h, 1, d]
    const T* __restrict__ k,            // [b, h, 1, d]
    const T* __restrict__ v,            // [b, h, 1, e]
    const float* __restrict__ past_kv,  // [b, h, d, e]
    const float* __restrict__ slope,    // [h, 1, 1]
    T* __restrict__ output,             // [b, h, 1, e]
    float* __restrict__ new_kv,         // [b, h, d, e]
    const int batch_size,
    const int num_heads,
    const int qk_dim,
    const int v_dim) {
  extern __shared__ char smem[];
  T* __restrict__ q_shared = reinterpret_cast<T*>(smem);
  T* __restrict__ k_shared = reinterpret_cast<T*>(smem + qk_dim * sizeof(T));
  T* __restrict__ v_shared = reinterpret_cast<T*>(smem + 2 * qk_dim * sizeof(T));
  float* __restrict__ new_kv_shared = reinterpret_cast<float*>(smem + (2 * qk_dim + v_dim) * sizeof(T));
  T* __restrict__ output_shared =
      reinterpret_cast<T*>(smem + (2 * qk_dim + v_dim) * sizeof(T) + qk_dim * (v_dim + 1) * sizeof(float));

  const int32_t tid = threadIdx.x;
  const int32_t current_head = blockIdx.x;
  const int32_t b = current_head / num_heads;
  const int32_t h = current_head % num_heads;

  if (b >= batch_size) return;

  const int32_t qk_offset = b * num_heads * qk_dim + h * qk_dim;
  const int32_t v_offset = b * num_heads * v_dim + h * v_dim;
  const int32_t kv_offset = b * num_heads * qk_dim * v_dim + h * qk_dim * v_dim;

  // Load q, k, v into shared memory
  for (int d = tid; d < qk_dim; d += blockDim.x) {
    q_shared[d] = q[qk_offset + d];
    k_shared[d] = k[qk_offset + d];
  }
  for (int e = tid; e < v_dim; e += blockDim.x) {
    v_shared[e] = v[v_offset + e];
  }

  __syncthreads();

  const float ratio = expf(-1.0f * slope[h]);

  // Compute new_kv
  for (int d = tid; d < qk_dim; d += blockDim.x) {
    const T k_val = k_shared[d];
    for (int e = 0; e < v_dim; ++e) {
      const int past_kv_idx = kv_offset + d * v_dim + e;
      const T v_val = v_shared[e];
      const float new_val = ratio * past_kv[past_kv_idx] + k_val * v_val;
      const int shared_idx = d * (v_dim + 1) + e;
      new_kv_shared[shared_idx] = new_val;
    }
  }

  __syncthreads();

  // Store new_kv to global memory
  for (int idx = tid; idx < qk_dim * v_dim; idx += blockDim.x) {
    const int d = idx / v_dim;
    const int e = idx % v_dim;
    const int shared_idx = d * (v_dim + 1) + e;
    const int global_idx = kv_offset + idx;
    new_kv[global_idx] = new_kv_shared[shared_idx];
  }

  __syncthreads();

  // Compute output
  for (int e = tid; e < v_dim; e += blockDim.x) {
    float sum = 0.0f;
    for (int d = 0; d < qk_dim; ++d) {
      const int shared_idx = d * (v_dim + 1) + e;
      sum += q_shared[d] * new_kv_shared[shared_idx];
    }
    output_shared[e] = static_cast<T>(sum);
  }

  __syncthreads();

  // Store output to global memory
  if (tid == 0) {
    for (int e = 0; e < v_dim; ++e) {
      output[v_offset + e] = output_shared[e];
    }
  }
}
```

---

## ğŸ“ ä»£ç è¯¦ç»†è§£æ

### ç¬¬ä¸€æ­¥ï¼šå…±äº«å†…å­˜å¸ƒå±€

```38:44:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  extern __shared__ char smem[];
  T* __restrict__ q_shared = reinterpret_cast<T*>(smem);
  T* __restrict__ k_shared = reinterpret_cast<T*>(smem + qk_dim * sizeof(T));
  T* __restrict__ v_shared = reinterpret_cast<T*>(smem + 2 * qk_dim * sizeof(T));
  float* __restrict__ new_kv_shared = reinterpret_cast<float*>(smem + (2 * qk_dim + v_dim) * sizeof(T));
  T* __restrict__ output_shared =
      reinterpret_cast<T*>(smem + (2 * qk_dim + v_dim) * sizeof(T) + qk_dim * (v_dim + 1) * sizeof(float));
```

**å…±äº«å†…å­˜å¸ƒå±€**ï¼ˆå‡è®¾ `qk_dim=128`, `v_dim=128`, `T=half`ï¼‰ï¼š

```
å…±äº«å†…å­˜å¸ƒå±€ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ q_shared:       [qk_dim] = 128 * 2 = 256 å­—èŠ‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ k_shared:       [qk_dim] = 128 * 2 = 256 å­—èŠ‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ v_shared:       [v_dim]  = 128 * 2 = 256 å­—èŠ‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ new_kv_shared:  [qk_dim, v_dim+1] = 128 * 129 * 4       â”‚
â”‚                = 66,048 å­—èŠ‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output_shared:  [v_dim] = 128 * 2 = 256 å­—èŠ‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ€»å…±äº«å†…å­˜ï¼šçº¦ 67 KB
```

**ä¸ºä»€ä¹ˆ `new_kv_shared` æ˜¯ `[qk_dim, v_dim+1]`ï¼Ÿ**
- å¯èƒ½æ˜¯ä¸ºäº†å†…å­˜å¯¹é½
- æˆ–è€…é¢„ç•™ç©ºé—´ç”¨äºå…¶ä»–ç”¨é€”

**å…³é”®ç‚¹**ï¼š
- **æ‰‹åŠ¨å¸ƒå±€**ï¼šå®Œå…¨æ§åˆ¶å†…å­˜å¸ƒå±€
- **ç±»å‹è½¬æ¢**ï¼šä½¿ç”¨ `reinterpret_cast` åœ¨ä¸åŒç±»å‹é—´åˆ‡æ¢
- **å¯¹é½ä¼˜åŒ–**ï¼šç¡®ä¿è®¿é—®å¯¹é½

### ç¬¬äºŒæ­¥ï¼šè®¡ç®—çº¿ç¨‹ç´¢å¼•

```46:51:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  const int32_t tid = threadIdx.x;
  const int32_t current_head = blockIdx.x;
  const int32_t b = current_head / num_heads;
  const int32_t h = current_head % num_heads;

  if (b >= batch_size) return;
```

**è®¾è®¡æ¨¡å¼**ï¼š
- **æ¯ä¸ª head ä¸€ä¸ª block**ï¼š`grid = batch_size * num_heads`
- **Batch å’Œ Head çš„ç´¢å¼•**ï¼š
  - `b = blockIdx.x / num_heads`ï¼šbatch ç´¢å¼•
  - `h = blockIdx.x % num_heads`ï¼šhead ç´¢å¼•

**ç¤ºä¾‹**ï¼š
- `batch_size=2`, `num_heads=4`
- Block 0-3ï¼šbatch 0 çš„ 4 ä¸ª heads
- Block 4-7ï¼šbatch 1 çš„ 4 ä¸ª heads

### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—å†…å­˜åç§»

```53:55:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  const int32_t qk_offset = b * num_heads * qk_dim + h * qk_dim;
  const int32_t v_offset = b * num_heads * v_dim + h * v_dim;
  const int32_t kv_offset = b * num_heads * qk_dim * v_dim + h * qk_dim * v_dim;
```

**å¼ é‡å¸ƒå±€**ï¼š
- `q, k`: `[batch, heads, 1, qk_dim]`
- `v`: `[batch, heads, 1, v_dim]`
- `past_kv, new_kv`: `[batch, heads, qk_dim, v_dim]`

**åç§»è®¡ç®—**ï¼š
- `qk_offset`ï¼šå®šä½åˆ° `q[b, h, 0, :]` æˆ– `k[b, h, 0, :]`
- `v_offset`ï¼šå®šä½åˆ° `v[b, h, 0, :]`
- `kv_offset`ï¼šå®šä½åˆ° `past_kv[b, h, :, :]` çš„èµ·å§‹ä½ç½®

### ç¬¬å››æ­¥ï¼šåŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜

```57:65:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  // Load q, k, v into shared memory
  for (int d = tid; d < qk_dim; d += blockDim.x) {
    q_shared[d] = q[qk_offset + d];
    k_shared[d] = k[qk_offset + d];
  }
  for (int e = tid; e < v_dim; e += blockDim.x) {
    v_shared[e] = v[v_offset + e];
  }

  __syncthreads();
```

**åä½œåŠ è½½æ¨¡å¼**ï¼š
- **å¤šçº¿ç¨‹åä½œ**ï¼š`for (int d = tid; d < qk_dim; d += blockDim.x)`
- **æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ **ï¼šå…ƒç´ é—´éš”æ˜¯ `blockDim.x`
- **åˆå¹¶è®¿é—®**ï¼šå¦‚æœ `qk_dim` æ˜¯ `blockDim.x` çš„å€æ•°ï¼Œä¼šäº§ç”Ÿå®Œç¾çš„åˆå¹¶è®¿é—®

**ç¤ºä¾‹**ï¼š
- `qk_dim=128`, `blockDim.x=32`
- çº¿ç¨‹ 0ï¼šåŠ è½½ 0, 32, 64, 96
- çº¿ç¨‹ 1ï¼šåŠ è½½ 1, 33, 65, 97
- ...

**ä¸ºä»€ä¹ˆéœ€è¦åŒæ­¥ï¼Ÿ**
- ç¡®ä¿æ‰€æœ‰æ•°æ®åŠ è½½å®Œæˆ
- åç»­è®¡ç®—éœ€è¦è¿™äº›æ•°æ®

### ç¬¬äº”æ­¥ï¼šè®¡ç®—æ–°çš„ KVï¼ˆæ ¸å¿ƒï¼‰

```68:80:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  const float ratio = expf(-1.0f * slope[h]);

  // Compute new_kv
  for (int d = tid; d < qk_dim; d += blockDim.x) {
    const T k_val = k_shared[d];
    for (int e = 0; e < v_dim; ++e) {
      const int past_kv_idx = kv_offset + d * v_dim + e;
      const T v_val = v_shared[e];
      const float new_val = ratio * past_kv[past_kv_idx] + k_val * v_val;
      const int shared_idx = d * (v_dim + 1) + e;
      new_kv_shared[shared_idx] = new_val;
    }
  }
```

**è®¡ç®—å…¬å¼**ï¼š
```cpp
new_kv[d][e] = ratio * old_kv[d][e] + k[d] * v[e]
```

**å¹¶è¡ŒåŒ–**ï¼š
- å¤–å±‚å¾ªç¯ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç† `qk_dim / blockDim.x` è¡Œ
- å†…å±‚å¾ªç¯ï¼šæ¯ä¸ªçº¿ç¨‹è®¡ç®—æ•´è¡Œï¼ˆ`v_dim` ä¸ªå…ƒç´ ï¼‰

**å†…å­˜è®¿é—®**ï¼š
- **è¯»å–**ï¼š`past_kv`ï¼ˆå…¨å±€å†…å­˜ï¼‰ã€`k_shared`ã€`v_shared`ï¼ˆå…±äº«å†…å­˜ï¼‰
- **å†™å…¥**ï¼š`new_kv_shared`ï¼ˆå…±äº«å†…å­˜ï¼‰

**æ€§èƒ½è€ƒè™‘**ï¼š
- `past_kv` æ˜¯å…¨å±€å†…å­˜è®¿é—®ï¼ˆè¾ƒæ…¢ï¼‰
- `k_shared`ã€`v_shared` æ˜¯å…±äº«å†…å­˜ï¼ˆå¾ˆå¿«ï¼‰
- å†…å±‚å¾ªç¯å¯èƒ½æœ‰å¯„å­˜å™¨å‹åŠ›

### ç¬¬å…­æ­¥ï¼šå†™å›æ–°çš„ KV

```82:92:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  __syncthreads();

  // Store new_kv to global memory
  for (int idx = tid; idx < qk_dim * v_dim; idx += blockDim.x) {
    const int d = idx / v_dim;
    const int e = idx % v_dim;
    const int shared_idx = d * (v_dim + 1) + e;
    const int global_idx = kv_offset + idx;
    new_kv[global_idx] = new_kv_shared[shared_idx];
  }
```

**å†™å›æ¨¡å¼**ï¼š
- å°† `new_kv_shared` å†™å›å…¨å±€å†…å­˜ `new_kv`
- ä½¿ç”¨æ‰å¹³åŒ–çš„ç´¢å¼•ï¼š`idx = d * v_dim + e`

**æ³¨æ„**ï¼š
- `shared_idx = d * (v_dim + 1) + e`ï¼ˆä½¿ç”¨ `v_dim+1`ï¼‰
- `global_idx = kv_offset + idx = kv_offset + d * v_dim + e`ï¼ˆä½¿ç”¨ `v_dim`ï¼‰
- è¿™æ˜¯å› ä¸ºå…±äº«å†…å­˜å¸ƒå±€å’Œå…¨å±€å†…å­˜å¸ƒå±€ä¸åŒ

### ç¬¬ä¸ƒæ­¥ï¼šè®¡ç®—æ³¨æ„åŠ›è¾“å‡º

```94:104:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  __syncthreads();

  // Compute output
  for (int e = tid; e < v_dim; e += blockDim.x) {
    float sum = 0.0f;
    for (int d = 0; d < qk_dim; ++d) {
      const int shared_idx = d * (v_dim + 1) + e;
      sum += q_shared[d] * new_kv_shared[shared_idx];
    }
    output_shared[e] = static_cast<T>(sum);
  }
```

**çŸ©é˜µå‘é‡ä¹˜æ³•**ï¼š
```
output[e] = Î£(q[d] * new_kv[d][e]) for all d
```

**å¹¶è¡ŒåŒ–**ï¼š
- æ¯ä¸ªçº¿ç¨‹è®¡ç®— `output` çš„ä¸€ä¸ªå…ƒç´ 
- çº¿ç¨‹ `i` è®¡ç®— `output[i]`

**ç´¯åŠ **ï¼š
- ä½¿ç”¨ `float sum` åœ¨å¯„å­˜å™¨ä¸­ç´¯åŠ 
- æœ€åè½¬å›ç±»å‹ `T`ï¼ˆå¯èƒ½æ˜¯ `half`ï¼‰

**å†…å­˜è®¿é—®**ï¼š
- `q_shared`ï¼šå…±äº«å†…å­˜ï¼Œå¿«é€Ÿ
- `new_kv_shared`ï¼šå…±äº«å†…å­˜ï¼Œå¿«é€Ÿ
- æ‰€æœ‰æ•°æ®éƒ½åœ¨å…±äº«å†…å­˜ä¸­ï¼Œè®¿é—®å¾ˆå¿«

### ç¬¬å…«æ­¥ï¼šå†™å›è¾“å‡º

```105:113:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu
  __syncthreads();

  // Store output to global memory
  if (tid == 0) {
    for (int e = 0; e < v_dim; ++e) {
      output[v_offset + e] = output_shared[e];
    }
  }
}
```

**è®¾è®¡é€‰æ‹©**ï¼š
- åªç”¨ç¬¬ä¸€ä¸ªçº¿ç¨‹å†™å›ï¼ˆ`tid == 0`ï¼‰
- ä¸²è¡Œå†™å…¥ï¼Œä½† `v_dim` é€šå¸¸ä¸å¤§ï¼ˆå¦‚ 128ï¼‰

**ä¸ºä»€ä¹ˆä¸å¹¶è¡Œå†™å›ï¼Ÿ**
- å¯èƒ½éœ€è¦æ›´å¤šçš„å…±äº«å†…å­˜åŒæ­¥
- å•ä¸ªçº¿ç¨‹ä¸²è¡Œå†™å›ç®€å•ä¸”è¶³å¤Ÿå¿«
- å¦‚æœ `v_dim` å¾ˆå¤§ï¼Œå¯ä»¥è€ƒè™‘å¹¶è¡Œå†™å›

---

## ğŸ¯ è®¾è®¡è¦ç‚¹ä¸ä¼˜åŒ–

### 1. å…±äº«å†…å­˜çš„ä½¿ç”¨

**ä¸ºä»€ä¹ˆä½¿ç”¨å…±äº«å†…å­˜ï¼Ÿ**
- **å¤ç”¨**ï¼šqã€kã€v è¢«å¤šæ¬¡è®¿é—®
- **é€Ÿåº¦**ï¼šå…±äº«å†…å­˜æ¯”å…¨å±€å†…å­˜å¿« 10-100x
- **å¸¦å®½**ï¼šå‡å°‘å…¨å±€å†…å­˜è®¿é—®æ¬¡æ•°

**å…±äº«å†…å­˜å¤§å°**ï¼š
```
æ€»å¤§å° â‰ˆ 2*qk_dim*sizeof(T) + v_dim*sizeof(T) + qk_dim*(v_dim+1)*sizeof(float) + v_dim*sizeof(T)
```

**ç¤ºä¾‹**ï¼ˆ`qk_dim=128`, `v_dim=128`, `T=half`ï¼‰ï¼š
```
= 2*128*2 + 128*2 + 128*129*4 + 128*2
= 512 + 256 + 66,048 + 256
â‰ˆ 67 KB
```

**é™åˆ¶**ï¼š
- å…±äº«å†…å­˜é€šå¸¸åªæœ‰ 48-164 KBï¼ˆå–å†³äº GPUï¼‰
- å¦‚æœå¤ªå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ block å¤§å°

### 2. çº¿ç¨‹åˆ†é…ç­–ç•¥

**å½“å‰ç­–ç•¥**ï¼šæ¯ä¸ª head ä¸€ä¸ª block

**ä¼˜ç‚¹**ï¼š
- ç®€åŒ–åŒæ­¥ï¼ˆblock å†…åŒæ­¥å³å¯ï¼‰
- æ¯ä¸ª head ç‹¬ç«‹å¤„ç†
- å®¹æ˜“ç†è§£å’Œè°ƒè¯•

**ç¼ºç‚¹**ï¼š
- å¦‚æœ `qk_dim` æˆ– `v_dim` å¾ˆå°ï¼Œblock åˆ©ç”¨ç‡ä½
- å¦‚æœ batch å¾ˆå¤§ï¼Œéœ€è¦å¾ˆå¤š blocks

**æ›¿ä»£æ–¹æ¡ˆ**ï¼š
- å¤šä¸ª heads å…±äº«ä¸€ä¸ª blockï¼ˆéœ€è¦æ›´å¤æ‚çš„åŒæ­¥ï¼‰
- åŠ¨æ€è°ƒæ•´ block å¤§å°

### 3. å†…å­˜è®¿é—®æ¨¡å¼

**è¯»å–æ¨¡å¼**ï¼š

| æ•°æ® | ä½ç½® | è®¿é—®æ¨¡å¼ | æ€§èƒ½ |
|------|------|---------|------|
| `q, k, v` | å…¨å±€â†’å…±äº« | åˆå¹¶è®¿é—® | â­â­â­â­â­ |
| `past_kv` | å…¨å±€ | éƒ¨åˆ†åˆå¹¶ | â­â­â­ |
| `q_shared` | å…±äº« | å¹¿æ’­ | â­â­â­â­â­ |
| `new_kv_shared` | å…±äº« | éšæœºè®¿é—® | â­â­â­â­ |

**ä¼˜åŒ–å»ºè®®**ï¼š
- âœ… ä½¿ç”¨å…±äº«å†…å­˜å¤ç”¨æ•°æ®
- âœ… åˆå¹¶å…¨å±€å†…å­˜è®¿é—®
- âš ï¸ `past_kv` çš„è®¿é—®å¯èƒ½ä¸æ˜¯å®Œå…¨åˆå¹¶çš„ï¼ˆå–å†³äº `v_dim`ï¼‰

---

## ğŸ“Š ç®—æ³•å¤æ‚åº¦åˆ†æ

### æ—¶é—´å¤æ‚åº¦

```
æ­¥éª¤ 1ï¼šåŠ è½½ q, k, v â†’ O(qk_dim + v_dim) / blockDim.x
æ­¥éª¤ 2ï¼šè®¡ç®— new_kv  â†’ O(qk_dim * v_dim) / blockDim.x
æ­¥éª¤ 3ï¼šå†™å› new_kv  â†’ O(qk_dim * v_dim) / blockDim.x
æ­¥éª¤ 4ï¼šè®¡ç®— output  â†’ O(qk_dim * v_dim) / blockDim.x
æ­¥éª¤ 5ï¼šå†™å› output  â†’ O(v_dim)

æ€»å¤æ‚åº¦ï¼šO(qk_dim * v_dim) ï¼ˆå¹¶è¡Œåï¼‰
```

### ç©ºé—´å¤æ‚åº¦

```
å…±äº«å†…å­˜ï¼šO(qk_dim + v_dim + qk_dim * v_dim)
å…¨å±€å†…å­˜ï¼šO(batch * heads * qk_dim * v_dim)
```

---

## ğŸ’¡ ç®€åŒ–ç‰ˆæœ¬ï¼ˆç†è§£æ ¸å¿ƒé€»è¾‘ï¼‰

å¦‚æœä½ æƒ³ç†è§£æ ¸å¿ƒé€»è¾‘ï¼Œè¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼š

```cpp
#include <cuda_runtime.h>
#include <stdio.h>

// ç®€åŒ–çš„ Lightning Attention Decode
__global__ void simple_attention_decode_kernel(
    const float* q,           // [qk_dim]
    const float* k,           // [qk_dim]
    const float* v,           // [v_dim]
    const float* past_kv,     // [qk_dim, v_dim]
    float* new_kv,            // [qk_dim, v_dim]
    float* output,            // [v_dim]
    float ratio,              // è¡°å‡å› å­
    int qk_dim,
    int v_dim) {
    
    extern __shared__ float smem[];
    float* q_shared = smem;
    float* k_shared = smem + qk_dim;
    float* v_shared = smem + 2 * qk_dim;
    float* new_kv_shared = smem + 2 * qk_dim + v_dim;
    
    int tid = threadIdx.x;
    
    // 1. åŠ è½½ q, k, v åˆ°å…±äº«å†…å­˜
    for (int i = tid; i < qk_dim; i += blockDim.x) {
        q_shared[i] = q[i];
        k_shared[i] = k[i];
    }
    for (int i = tid; i < v_dim; i += blockDim.x) {
        v_shared[i] = v[i];
    }
    __syncthreads();
    
    // 2. è®¡ç®— new_kv = ratio * old_kv + k @ v^T
    for (int d = tid; d < qk_dim; d += blockDim.x) {
        for (int e = 0; e < v_dim; ++e) {
            int idx = d * v_dim + e;
            new_kv_shared[idx] = ratio * past_kv[idx] + k_shared[d] * v_shared[e];
        }
    }
    __syncthreads();
    
    // 3. å†™å› new_kv
    for (int idx = tid; idx < qk_dim * v_dim; idx += blockDim.x) {
        new_kv[idx] = new_kv_shared[idx];
    }
    __syncthreads();
    
    // 4. è®¡ç®— output = q @ new_kv
    for (int e = tid; e < v_dim; e += blockDim.x) {
        float sum = 0.0f;
        for (int d = 0; d < qk_dim; ++d) {
            int idx = d * v_dim + e;
            sum += q_shared[d] * new_kv_shared[idx];
        }
        output[e] = sum;
    }
}

void simple_attention_decode_host(
    const float* d_q,
    const float* d_k,
    const float* d_v,
    const float* d_past_kv,
    float* d_new_kv,
    float* d_output,
    float ratio,
    int qk_dim,
    int v_dim) {
    
    int threads = 128;
    size_t smem_size = (2 * qk_dim + v_dim + qk_dim * v_dim) * sizeof(float);
    
    simple_attention_decode_kernel<<<1, threads, smem_size>>>(
        d_q, d_k, d_v, d_past_kv, d_new_kv, d_output,
        ratio, qk_dim, v_dim);
    
    cudaDeviceSynchronize();
}
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µ

1. **å¢é‡æ³¨æ„åŠ›**ï¼šåªè®¡ç®—å½“å‰ token çš„æ³¨æ„åŠ›
2. **KV Cache æ›´æ–°**ï¼š`new_kv = ratio * old_kv + k @ v^T`
3. **å…±äº«å†…å­˜å¤ç”¨**ï¼šqã€kã€v è½½å…¥å…±äº«å†…å­˜ï¼Œå¤šæ¬¡è®¿é—®
4. **çŸ©é˜µå‘é‡ä¹˜æ³•**ï¼š`output = q @ kv`

### å…³é”®ä¼˜åŒ–

- âœ… **å…±äº«å†…å­˜**ï¼šå‡å°‘å…¨å±€å†…å­˜è®¿é—®
- âœ… **èåˆæ“ä½œ**ï¼šåŒæ—¶è®¡ç®—æ³¨æ„åŠ›å’Œæ›´æ–° cache
- âœ… **æ»‘åŠ¨çª—å£**ï¼šé€šè¿‡è¡°å‡å› å­å®ç°
- âœ… **æ¯ä¸ª head ä¸€ä¸ª block**ï¼šç®€åŒ–åŒæ­¥

### å­¦ä¹ ä»·å€¼

Lightning Attention Decode å±•ç¤ºäº†ï¼š
- å¤æ‚çš„å…±äº«å†…å­˜ä½¿ç”¨
- å¤šé˜¶æ®µè®¡ç®—æµç¨‹
- å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
- èåˆæ“ä½œçš„è®¾è®¡æ€è·¯

---

## ğŸ”— ç›¸å…³èµ„æº

- **ä¸‹ä¸€ä¸ªç®—å­**ï¼š[04_RoPEç®—å­.md](./04_RoPEç®—å­.md)
- **Flash Attention**ï¼šè§£ç é˜¶æ®µæ³¨æ„åŠ›çš„å¦ä¸€ç§å®ç°
- **æ»‘åŠ¨çª—å£æ³¨æ„åŠ›**ï¼šSWA æœºåˆ¶è¯¦è§£

