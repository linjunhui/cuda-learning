# Fused Add RMSNorm ç®—å­è¯¦è§£

## ğŸ“– ç®—å­æ¦‚è¿°

**Fused Add RMSNorm** æ˜¯ LLM ä¸­çš„èåˆæ“ä½œï¼Œå°†ä¸¤ä¸ªå¸¸è§æ“ä½œåˆå¹¶ï¼š
1. **Addï¼ˆæ®‹å·®è¿æ¥ï¼‰**ï¼š`x = input + residual`
2. **RMSNormï¼ˆå½’ä¸€åŒ–ï¼‰**ï¼š`x = x / sqrt(mean(xÂ²) + eps) * weight`

**ç”¨é€”**ï¼š
- Transformer çš„å±‚å½’ä¸€åŒ–
- æ®‹å·®è¿æ¥çš„èåˆå®ç°
- å‡å°‘å†…å­˜è®¿é—®å’Œ kernel å¯åŠ¨æ¬¡æ•°

**ç‰¹ç‚¹**ï¼š
- **èåˆæ“ä½œ**ï¼šä¸¤ä¸ªæ“ä½œåˆå¹¶ä¸ºä¸€ä¸ª kernel
- **å‡å°‘å†…å­˜è®¿é—®**ï¼šä¸­é—´ç»“æœä¿ç•™åœ¨å¯„å­˜å™¨ä¸­
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘ kernel å¯åŠ¨å¼€é”€

---

## ğŸ”¢ å…¬å¼ä¸ç®—æ³•

### æ•°å­¦å…¬å¼

#### æ­¥éª¤ 1ï¼šæ®‹å·®è¿æ¥

```
x = input + residual
```

**å«ä¹‰**ï¼šå°†è¾“å…¥ä¸æ®‹å·®ç›¸åŠ ã€‚

#### æ­¥éª¤ 2ï¼šRMSNorm

**RMSï¼ˆRoot Mean Squareï¼‰**ï¼š
```
rms = sqrt(mean(xÂ²) + eps)
```

å…¶ä¸­ï¼š
- `mean(xÂ²) = sum(x_iÂ²) / n`
- `eps`ï¼šé˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆå¦‚ 1e-6ï¼‰

**å½’ä¸€åŒ–**ï¼š
```
x_norm = x / rms
```

**ç¼©æ”¾**ï¼š
```
output = x_norm * weight
```

#### å®Œæ•´å…¬å¼

```
x = input + residual
rms = sqrt(sum(xÂ²) / n + eps)
output = (x / rms) * weight
```

**å‘é‡å½¢å¼**ï¼š
```
x_i = input_i + residual_i
rms = sqrt((1/n) * Î£(x_iÂ²) + eps)
output_i = (x_i / rms) * weight_i
```

---

## ğŸ§  ç®—æ³•åŸç†

### åŸºæœ¬åŸç†

RMSNorm æ˜¯ LayerNorm çš„ç®€åŒ–ç‰ˆæœ¬ï¼š

| å½’ä¸€åŒ–æ–¹å¼ | å…¬å¼ | ç‰¹ç‚¹ |
|-----------|------|------|
| **LayerNorm** | `(x - mean(x)) / std(x) * Î³ + Î²` | å‡å‡å€¼ï¼Œé™¤æ ‡å‡†å·® |
| **RMSNorm** | `x / rms(x) * weight` | åªé™¤ RMSï¼Œä¸å‡å‡å€¼ |

**ä¸ºä»€ä¹ˆç”¨ RMSNormï¼Ÿ**
- **è®¡ç®—æ›´å¿«**ï¼šä¸éœ€è¦è®¡ç®—å‡å€¼
- **æ•ˆæœç›¸è¿‘**ï¼šåœ¨å¾ˆå¤šæƒ…å†µä¸‹æ€§èƒ½ç›¸ä¼¼
- **æ•°å€¼ç¨³å®š**ï¼šé¿å…å‡å‡å€¼å¸¦æ¥çš„ç²¾åº¦é—®é¢˜

### ç®—æ³•æµç¨‹

```
1. Addï¼ˆæ®‹å·®è¿æ¥ï¼‰
   x = input + residual
   â†“
2. è®¡ç®—å¹³æ–¹å’Œï¼ˆå¹¶è¡Œå½’çº¦ï¼‰
   sum_sq = Î£(x_iÂ²)
   â†“
3. è®¡ç®— RMS
   rms = sqrt(sum_sq / n + eps)
   â†“
4. å½’ä¸€åŒ–å’Œç¼©æ”¾
   output_i = (x_i / rms) * weight_i
```

### èåˆæ“ä½œçš„ä¼˜åŠ¿

**åˆ†ç¦»ç‰ˆæœ¬**ï¼š
```
1. add_kernel<<<...>>>(input, residual, temp)      // Kernel 1
   cudaDeviceSynchronize()
2. rmsnorm_kernel<<<...>>>(temp, weight, output)   // Kernel 2
```

**é—®é¢˜**ï¼š
- éœ€è¦ä¸­é—´å†…å­˜ `temp`ï¼ˆO(n)ï¼‰
- ä¸¤æ¬¡å†…å­˜è¯»å†™ï¼ˆå†™å…¥ tempï¼Œè¯»å– tempï¼‰
- ä¸¤æ¬¡ kernel å¯åŠ¨å¼€é”€

**èåˆç‰ˆæœ¬**ï¼š
```
fused_add_rmsnorm_kernel<<<...>>>(input, residual, weight, output)
```

**ä¼˜åŠ¿**ï¼š
- âœ… **æ— ä¸­é—´å†…å­˜**ï¼šä¸­é—´ç»“æœä¿ç•™åœ¨å¯„å­˜å™¨ä¸­
- âœ… **ä¸€æ¬¡å†…å­˜è¯»å†™**ï¼šå‡å°‘å†…å­˜è®¿é—®
- âœ… **ä¸€æ¬¡ kernel å¯åŠ¨**ï¼šé™ä½å¼€é”€
- âœ… **æ›´å¥½çš„ç¼“å­˜**ï¼šæ•°æ®åœ¨ç¼“å­˜ä¸­å¤ç”¨

---

## ğŸ’» ä»£ç å®ç°

### æºç ä½ç½®

`SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu`

### ä¸»æœºç«¯è°ƒç”¨

```24:59:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu
void sgl_fused_add_rmsnorm(
    torch::Tensor input, torch::Tensor residual, torch::Tensor weight, double eps, bool enable_pdl) {
  CHECK_INPUT(input);
  CHECK_INPUT(residual);
  CHECK_INPUT(weight);
  auto device = input.device();
  CHECK_EQ(residual.device(), device);
  CHECK_EQ(weight.device(), device);
  CHECK_DIM(2, input);     // input: (batch_size, hidden_size)
  CHECK_DIM(2, residual);  // residual: (batch_size, hidden_size)
  CHECK_DIM(1, weight);    // weight: (hidden_size)
  CHECK_EQ(input.size(0), residual.size(0));
  CHECK_EQ(input.size(1), residual.size(1));
  CHECK_EQ(input.size(1), weight.size(0));
  unsigned int batch_size = input.size(0);
  unsigned int hidden_size = input.size(1);

  cudaStream_t torch_current_stream = at::cuda::getCurrentCUDAStream();
  // support float16, bfloat16 and float32
  DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
    cudaError_t status = norm::FusedAddRMSNorm(
        static_cast<c_type*>(input.data_ptr()),
        static_cast<c_type*>(residual.data_ptr()),
        static_cast<c_type*>(weight.data_ptr()),
        batch_size,
        hidden_size,
        input.stride(0),
        residual.stride(0),
        eps,
        enable_pdl,
        torch_current_stream);
    TORCH_CHECK(
        status == cudaSuccess, "FusedAddRMSNorm failed with error code " + std::string(cudaGetErrorString(status)));
    return true;
  });
}
```

**å…³é”®å‚æ•°**ï¼š
- `input`: è¾“å…¥å¼ é‡ `[batch_size, hidden_size]`
- `residual`: æ®‹å·®å¼ é‡ `[batch_size, hidden_size]`
- `weight`: æƒé‡ `[hidden_size]`
- `eps`: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

**æ³¨æ„**ï¼šSGLang ä½¿ç”¨äº† FlashInfer åº“çš„å®ç°ï¼Œå®é™…çš„ kernel åœ¨ FlashInfer ä¸­ã€‚

### ç®€åŒ–å®ç°ï¼ˆå±•ç¤ºæ ¸å¿ƒé€»è¾‘ï¼‰

```cpp
#include <cuda_runtime.h>
#include <stdio.h>
#include <math.h>

template<typename T>
__global__ void fused_add_rmsnorm_kernel(
    const T* input,        // [batch, hidden]
    const T* residual,     // [batch, hidden]
    const T* weight,       // [hidden]
    T* output,             // [batch, hidden]
    float eps,
    int batch_size,
    int hidden_size) {
    
    extern __shared__ float smem[];
    float* sum_sq = smem;  // ç”¨äºå­˜å‚¨å¹³æ–¹å’Œ
    
    int tid = threadIdx.x;
    int bid = blockIdx.x;  // batch ç´¢å¼•
    
    if (bid >= batch_size) return;
    
    // 1. Addï¼ˆæ®‹å·®è¿æ¥ï¼‰å¹¶è®¡ç®—å¹³æ–¹ï¼ˆå¹¶è¡Œå½’çº¦ï¼‰
    float local_sum_sq = 0.0f;
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        int idx = bid * hidden_size + i;
        float x = (float)input[idx] + (float)residual[idx];
        local_sum_sq += x * x;  // åŒæ—¶è®¡ç®—å¹³æ–¹å’Œ
        // æš‚å­˜åœ¨å…±äº«å†…å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        smem[i] = x;  // å‡è®¾æœ‰è¶³å¤Ÿå…±äº«å†…å­˜
    }
    
    // 2. Block å†…å½’çº¦ï¼ˆè®¡ç®—å¹³æ–¹å’Œï¼‰
    // ä½¿ç”¨æ ‘å½¢å½’çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sum_sq[tid] += sum_sq[tid + stride];
        }
        __syncthreads();
    }
    
    // 3. è®¡ç®— RMSï¼ˆç¬¬ä¸€ä¸ªçº¿ç¨‹ï¼‰
    float rms = 1.0f;
    if (tid == 0) {
        float mean_sq = sum_sq[0] / hidden_size;
        rms = sqrtf(mean_sq + eps);
        sum_sq[0] = rms;  // å­˜å‚¨ RMS ä¾›æ‰€æœ‰çº¿ç¨‹ä½¿ç”¨
    }
    __syncthreads();
    
    rms = sum_sq[0];  // æ‰€æœ‰çº¿ç¨‹è¯»å– RMS
    
    // 4. å½’ä¸€åŒ–å’Œç¼©æ”¾
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        int idx = bid * hidden_size + i;
        float x = smem[i];  // ä»å…±äº«å†…å­˜è¯»å–
        float x_norm = x / rms;
        output[idx] = (T)(x_norm * (float)weight[i]);
    }
}
```

**æ³¨æ„**ï¼šè¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…çš„å®ç°æ›´å¤æ‚ï¼Œéœ€è¦è€ƒè™‘ï¼š
- å‘é‡åŒ–åŠ è½½
- æ›´å¥½çš„å½’çº¦æ–¹å¼
- å…±äº«å†…å­˜å¤§å°é™åˆ¶

---

## ğŸ“ å®Œæ•´å®ç°ï¼ˆè€ƒè™‘æ‰€æœ‰ç»†èŠ‚ï¼‰

### ä¼˜åŒ–ç‰ˆæœ¬

```cpp
template<typename T>
__global__ void fused_add_rmsnorm_optimized(
    const T* input,
    const T* residual,
    const T* weight,
    T* output,
    float eps,
    int batch_size,
    int hidden_size) {
    
    extern __shared__ char smem[];
    float* sum_sq_shared = (float*)smem;
    float* x_shared = (float*)(smem + blockDim.x * sizeof(float));
    
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    
    if (bid >= batch_size) return;
    
    // 1. Add å’Œè®¡ç®—å¹³æ–¹å’Œï¼ˆèåˆï¼‰
    float sum_sq = 0.0f;
    const int vec_size = 4;  // ä¸€æ¬¡å¤„ç† 4 ä¸ªå…ƒç´ 
    const int num_vectors = (hidden_size + vec_size - 1) / vec_size;
    
    for (int vec_idx = 0; vec_idx < num_vectors; vec_idx++) {
        int base_idx = vec_idx * vec_size;
        float x_vec[vec_size];
        float sq_vec[vec_size];
        
        // å‘é‡åŒ–åŠ è½½å’Œè®¡ç®—
        for (int i = 0; i < vec_size && base_idx + i < hidden_size; i++) {
            int idx = bid * hidden_size + base_idx + i;
            x_vec[i] = (float)input[idx] + (float)residual[idx];
            sq_vec[i] = x_vec[i] * x_vec[i];
            sum_sq += sq_vec[i];
        }
        
        // å­˜å‚¨åˆ°å…±äº«å†…å­˜ï¼ˆç”¨äºåç»­å½’ä¸€åŒ–ï¼‰
        if (vec_idx * blockDim.x + tid < hidden_size) {
            int store_idx = vec_idx * blockDim.x + tid;
            if (store_idx < hidden_size) {
                x_shared[store_idx] = x_vec[tid % vec_size];
            }
        }
    }
    
    // 2. Block å†…å½’çº¦å¹³æ–¹å’Œ
    sum_sq_shared[tid] = sum_sq;
    __syncthreads();
    
    // æ ‘å½¢å½’çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sum_sq_shared[tid] += sum_sq_shared[tid + stride];
        }
        __syncthreads();
    }
    
    // 3. è®¡ç®— RMS
    float rms = 1.0f;
    if (tid == 0) {
        float mean_sq = sum_sq_shared[0] / hidden_size;
        rms = sqrtf(mean_sq + eps);
        sum_sq_shared[0] = rms;
    }
    __syncthreads();
    
    rms = sum_sq_shared[0];
    
    // 4. å½’ä¸€åŒ–å’Œç¼©æ”¾
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        int idx = bid * hidden_size + i;
        float x = x_shared[i];
        float x_norm = x / rms;
        output[idx] = (T)(x_norm * (float)weight[i]);
    }
}
```

---

## ğŸ¯ è®¾è®¡è¦ç‚¹ä¸ä¼˜åŒ–

### 1. èåˆæ“ä½œçš„ä¼˜åŠ¿

**å†…å­˜è®¿é—®å¯¹æ¯”**ï¼š

| æ“ä½œ | åˆ†ç¦»ç‰ˆæœ¬ | èåˆç‰ˆæœ¬ |
|------|---------|---------|
| **è¯»å–** | input, residual, temp, weight | input, residual, weight |
| **å†™å…¥** | temp, output | output |
| **æ€»è®¿é—®** | 5 æ¬¡ï¼ˆ4è¯»1å†™ï¼‰ | 3 æ¬¡ï¼ˆ2è¯»1å†™ï¼‰ |

**æ€§èƒ½æå‡**ï¼š
- å‡å°‘ 40% çš„å†…å­˜è®¿é—®
- æ— ä¸­é—´å†…å­˜åˆ†é…
- æ›´å¥½çš„ç¼“å­˜åˆ©ç”¨ç‡

### 2. å½’çº¦ä¼˜åŒ–

**å¹³æ–¹å’Œè®¡ç®—**ï¼š
- æ¯ä¸ªçº¿ç¨‹è®¡ç®—å±€éƒ¨å¹³æ–¹å’Œ
- ä½¿ç”¨å…±äº«å†…å­˜åš block å†…å½’çº¦
- å¦‚æœ hidden_size å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦å¤šçº§å½’çº¦

**ä¼˜åŒ–æŠ€å·§**ï¼š
- å‘é‡åŒ–è®¡ç®—ï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªå…ƒç´ 
- å¤šä¸ªç´¯åŠ å™¨ï¼šå‡å°‘å¾ªç¯ä¾èµ–
- æ ‘å½¢å½’çº¦ï¼šO(log n) å¤æ‚åº¦

### 3. æ•°å€¼ç¨³å®šæ€§

**EPS çš„ä½œç”¨**ï¼š
```cpp
rms = sqrt(mean_sq + eps);
```

**ä¸ºä»€ä¹ˆéœ€è¦ epsï¼Ÿ**
- é˜²æ­¢ `mean_sq = 0` æ—¶é™¤é›¶
- æé«˜æ•°å€¼ç¨³å®šæ€§
- é€šå¸¸ `eps = 1e-6`

### 4. å‘é‡åŒ–å®ç°

**å‘é‡åŒ–åŠ è½½**ï¼š
```cpp
// ä¸€æ¬¡åŠ è½½ 4 ä¸ª float
float4 vec_input = *((float4*)&input[idx]);
float4 vec_residual = *((float4*)&residual[idx]);
```

**å‘é‡åŒ–è®¡ç®—**ï¼š
```cpp
float4 vec_x;
vec_x.x = vec_input.x + vec_residual.x;
vec_x.y = vec_input.y + vec_residual.y;
// ...
```

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### å¤æ‚åº¦

**æ—¶é—´å¤æ‚åº¦**ï¼š
- Addï¼šO(hidden_size) / threads
- å¹³æ–¹å’Œå½’çº¦ï¼šO(hidden_size) / threads + O(log threads)
- å½’ä¸€åŒ–ï¼šO(hidden_size) / threads
- æ€»å¤æ‚åº¦ï¼šO(hidden_size) / threads

**ç©ºé—´å¤æ‚åº¦**ï¼š
- å…±äº«å†…å­˜ï¼šO(threads)ï¼ˆå­˜å‚¨å½’çº¦ä¸­é—´ç»“æœï¼‰
- å…¨å±€å†…å­˜ï¼šO(batch * hidden_size)

### æ€§èƒ½ç“¶é¢ˆ

1. **å†…å­˜è®¿é—®**ï¼šè¯»å– inputã€residualï¼Œå†™å…¥ output
2. **å½’çº¦æ“ä½œ**ï¼šéœ€è¦å…±äº«å†…å­˜åŒæ­¥
3. **é™¤æ³•è¿ç®—**ï¼š`x / rms` ç›¸å¯¹æ…¢

### ä¼˜åŒ–å»ºè®®

1. **å‘é‡åŒ–**ï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªå…ƒç´ 
2. **å…±äº«å†…å­˜**ï¼šå¤ç”¨æ•°æ®ï¼Œå‡å°‘å…¨å±€å†…å­˜è®¿é—®
3. **å¿«é€Ÿæ•°å­¦å‡½æ•°**ï¼šä½¿ç”¨ `__fdividef`ï¼ˆå¿«é€Ÿé™¤æ³•ï¼‰

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µ

1. **èåˆæ“ä½œ**ï¼šå°†å¤šä¸ªæ“ä½œåˆå¹¶ä¸ºä¸€ä¸ª kernel
2. **æ®‹å·®è¿æ¥**ï¼š`x = input + residual`
3. **RMSNorm**ï¼š`output = (x / rms) * weight`
4. **å¹¶è¡Œå½’çº¦**ï¼šè®¡ç®—å¹³æ–¹å’Œ

### å…³é”®ä¼˜åŒ–

- âœ… **å‡å°‘å†…å­˜è®¿é—®**ï¼šæ— ä¸­é—´ç»“æœ
- âœ… **èåˆè®¡ç®—**ï¼šä¸€æ¬¡ kernel å¯åŠ¨
- âœ… **å…±äº«å†…å­˜**ï¼šå¿«é€Ÿå½’çº¦
- âœ… **å‘é‡åŒ–**ï¼šæé«˜å¸¦å®½åˆ©ç”¨ç‡

### å­¦ä¹ ä»·å€¼

Fused Add RMSNorm å±•ç¤ºäº†ï¼š
- èåˆæ“ä½œçš„è®¾è®¡æ€è·¯
- å¹¶è¡Œå½’çº¦çš„å®ç°
- å†…å­˜è®¿é—®ä¼˜åŒ–æŠ€å·§
- æ•°å€¼ç¨³å®šæ€§çš„è€ƒè™‘

---

## ğŸ”— ç›¸å…³èµ„æº

- **RMSNorm è®ºæ–‡**ï¼šRoot Mean Square Layer Normalization
- **LayerNorm vs RMSNorm**ï¼šæ€§èƒ½å¯¹æ¯”
- **ä¸‹ä¸€ä¸ªç®—å­**ï¼š[README.md](./README.md)ï¼ˆç›®å½•ç´¢å¼•ï¼‰

