# Copy ç®—å­è¯¦è§£

## ğŸ“– ç®—å­æ¦‚è¿°

**Copy** ç®—å­æ˜¯æœ€ç®€å•çš„ç®—å­ä¹‹ä¸€ï¼Œå®ƒçš„åŠŸèƒ½æ˜¯å°†æ•°æ®ä» CPU å†…å­˜å¤åˆ¶åˆ° GPU å†…å­˜ï¼ˆæˆ–ä» GPU å¤åˆ¶åˆ° GPUï¼‰ã€‚

**ç”¨é€”**ï¼š
- å°† CPU ä¸Šçš„å°æ•°ç»„å¤åˆ¶åˆ° GPU
- è®¾å¤‡é—´æ•°æ®ä¼ è¾“
- å¸¸é‡æ•°æ®åˆå§‹åŒ–

**ç‰¹ç‚¹**ï¼š
- æœ€ç®€å•çš„ CUDA kernel
- ç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹çš„åŸºç¡€
- æ²¡æœ‰å¤æ‚çš„è®¡ç®—ï¼Œåªæœ‰æ•°æ®ç§»åŠ¨

---

## ğŸ”¢ å…¬å¼ä¸ç®—æ³•

### æ•°å­¦å…¬å¼

```
output[i] = input[i],  âˆ€ i âˆˆ [0, N-1]
```

**å«ä¹‰**ï¼šå°†è¾“å…¥æ•°ç»„çš„æ¯ä¸ªå…ƒç´ åŸæ ·å¤åˆ¶åˆ°è¾“å‡ºæ•°ç»„ã€‚

### ç®—æ³•æ­¥éª¤

```
1. è·å–å½“å‰çº¿ç¨‹çš„å…¨å±€ç´¢å¼• idx
2. æ£€æŸ¥è¾¹ç•Œï¼šif (idx < N)
3. å¤åˆ¶ï¼šoutput[idx] = input[idx]
```

**å¤æ‚åº¦**ï¼š
- **æ—¶é—´å¤æ‚åº¦**ï¼šO(N)
- **ç©ºé—´å¤æ‚åº¦**ï¼šO(1)
- **å¹¶è¡Œåº¦**ï¼šN ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ

---

## ğŸ§  ç®—æ³•åŸç†

### åŸºæœ¬åŸç†

Copy ç®—å­æ˜¯æœ€ç®€å•çš„æ•°æ®å¹¶è¡Œæ“ä½œï¼š
- **æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ **ï¼šçº¿ç¨‹ `i` è´Ÿè´£å¤åˆ¶ `input[i]` åˆ° `output[i]`
- **æ— ä¾èµ–å…³ç³»**ï¼šæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å·¥ä½œï¼Œä¸éœ€è¦åŒæ­¥
- **å†…å­˜è®¿é—®æ¨¡å¼ç®€å•**ï¼šè¿ç»­çš„å†…å­˜è®¿é—®

### çº¿ç¨‹åˆ†é…

```
è¾“å…¥æ•°ç»„: [0][1][2][3][4][5][6][7][8][9]
          â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“
çº¿ç¨‹åˆ†é…: [T0][T1][T2][T3][T4][T5][T6][T7][T8][T9]
          â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“
è¾“å‡ºæ•°ç»„: [0][1][2][3][4][5][6][7][8][9]
```

**å›¾ç¤º**ï¼š
- æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ 
- 1:1 æ˜ å°„å…³ç³»
- å¹¶è¡Œæ‰§è¡Œï¼Œäº’ä¸å¹²æ‰°

---

## ğŸ’» ä»£ç å®ç°

### æºç ä½ç½®

`SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/copy.cu`

### å®Œæ•´ä»£ç 

```12:18:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/copy.cu
template <int N>
__global__ void copy_to_gpu_no_ce_kernel(const InputArray<N> input_array, int* output) {
  int idx = threadIdx.x + blockIdx.x * blockDim.x;
  if (idx < N) {
    output[idx] = input_array.values[idx];
  }
}
```

### ä»£ç é€è¡Œè§£æ

#### ç¬¬ 1 è¡Œï¼šKernel å®šä¹‰

```cpp
template <int N>
__global__ void copy_to_gpu_no_ce_kernel(const InputArray<N> input_array, int* output)
```

**å…³é”®ç‚¹**ï¼š
- **`template <int N>`**ï¼šæ¨¡æ¿å‚æ•°ï¼Œç¼–è¯‘æ—¶ç¡®å®šæ•°ç»„å¤§å°
- **`__global__`**ï¼šCUDA kernel å‡½æ•°ï¼Œä»ä¸»æœºç«¯è°ƒç”¨ï¼Œåœ¨è®¾å¤‡ç«¯æ‰§è¡Œ
- **å‚æ•°**ï¼š
  - `InputArray<N>`ï¼šè¾“å…¥çš„å¸¸é‡æ•°ç»„ï¼ˆåœ¨ GPU å¸¸é‡å†…å­˜ä¸­ï¼‰
  - `int* output`ï¼šè¾“å‡ºæ•°ç»„æŒ‡é’ˆï¼ˆåœ¨ GPU å…¨å±€å†…å­˜ä¸­ï¼‰

**ä¸ºä»€ä¹ˆç”¨æ¨¡æ¿ï¼Ÿ**
- ç¼–è¯‘æ—¶ç¡®å®šå¤§å°ï¼Œç¼–è¯‘å™¨å¯ä»¥ä¼˜åŒ–
- å°æ•°ç»„å¯ä»¥ä½¿ç”¨å¸¸é‡å†…å­˜ï¼ˆæ›´å¿«ï¼‰
- é¿å…è¿è¡Œæ—¶æ£€æŸ¥

#### ç¬¬ 2 è¡Œï¼šè®¡ç®—çº¿ç¨‹ç´¢å¼•

```cpp
int idx = threadIdx.x + blockIdx.x * blockDim.x;
```

**å«ä¹‰**ï¼š
- `threadIdx.x`ï¼šçº¿ç¨‹åœ¨ block å†…çš„ç´¢å¼•ï¼ˆ0 åˆ° blockDim.x-1ï¼‰
- `blockIdx.x`ï¼šblock åœ¨æ•´ä¸ª grid ä¸­çš„ç´¢å¼•
- `blockDim.x`ï¼šæ¯ä¸ª block çš„çº¿ç¨‹æ•°
- **`idx`**ï¼šçº¿ç¨‹åœ¨æ•´ä¸ª grid ä¸­çš„å…¨å±€ç´¢å¼•

**ç¤ºä¾‹**ï¼š
- Block 0, Thread 2ï¼š`idx = 0 Ã— 256 + 2 = 2`
- Block 1, Thread 2ï¼š`idx = 1 Ã— 256 + 2 = 258`

#### ç¬¬ 3 è¡Œï¼šè¾¹ç•Œæ£€æŸ¥

```cpp
if (idx < N) {
```

**ä¸ºä»€ä¹ˆéœ€è¦è¾¹ç•Œæ£€æŸ¥ï¼Ÿ**
- Grid çš„çº¿ç¨‹æ•°å¯èƒ½å¤§äºæ•°ç»„å¤§å° `N`
- ä¾‹å¦‚ï¼š`N=10`ï¼Œä½†å¯åŠ¨äº† 256 ä¸ªçº¿ç¨‹
- è¾¹ç•Œæ£€æŸ¥é˜²æ­¢è¶Šç•Œè®¿é—®

**ç¤ºä¾‹**ï¼š
- `N=10`, `grid=1`, `block=256`
- çº¿ç¨‹ 0-9ï¼šä¼šæ‰§è¡Œå¤åˆ¶
- çº¿ç¨‹ 10-255ï¼šè·³è¿‡ï¼ˆè¾¹ç•Œæ£€æŸ¥å¤±è´¥ï¼‰

#### ç¬¬ 4 è¡Œï¼šæ‰§è¡Œå¤åˆ¶

```cpp
output[idx] = input_array.values[idx];
```

**æ“ä½œ**ï¼š
- ä»å¸¸é‡å†…å­˜è¯»å– `input_array.values[idx]`
- å†™å…¥å…¨å±€å†…å­˜ `output[idx]`

**å†…å­˜è®¿é—®**ï¼š
- **è¯»å–**ï¼šå¸¸é‡å†…å­˜ï¼ˆ`const InputArray`ï¼‰ï¼Œåªè¯»ï¼Œç¼“å­˜å‹å¥½
- **å†™å…¥**ï¼šå…¨å±€å†…å­˜ï¼ˆ`int*`ï¼‰ï¼Œå¯å†™

### InputArray ç»“æ„ä½“

```7:10:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/copy.cu
template <int N>
struct InputArray {
  int values[N];
};
```

**ä½œç”¨**ï¼š
- å°è£…å›ºå®šå¤§å°çš„æ•°ç»„
- å¯ä»¥åœ¨ kernel å¯åŠ¨æ—¶ç›´æ¥ä¼ é€’ï¼ˆæ‹·è´åˆ° GPU å¸¸é‡å†…å­˜ï¼‰
- é¿å…é¢å¤–çš„å†…å­˜åˆ†é…

### ä¸»æœºç«¯è°ƒç”¨

```20:46:SGLangå­¦ä¹ /sglang/sgl-kernel/csrc/elementwise/copy.cu
template <int N>
void copy_to_gpu_no_ce_impl(const at::Tensor& input, at::Tensor& output) {
  TORCH_CHECK(input.dim() == 1, "input must be 1-D");
  TORCH_CHECK(static_cast<int>(input.numel()) == N, "input numel must equal template N");
  TORCH_CHECK(input.is_contiguous(), "input must be contiguous");
  TORCH_CHECK(input.dtype() == torch::kInt32, "input dtype must be int32");

  TORCH_CHECK(output.dim() == 1, "output dim");
  TORCH_CHECK(static_cast<int>(output.numel()) == N, "output size");
  TORCH_CHECK(output.is_contiguous(), "output contiguous");
  TORCH_CHECK(output.dtype() == torch::kInt32, "output dtype");

  TORCH_CHECK(input.device().is_cpu(), "input must be a CPU tensor");
  TORCH_CHECK(output.device().is_cuda(), "output must be a CUDA tensor");

  InputArray<N> input_array;
  const int* input_ptr = input.data_ptr<int>();
  for (int i = 0; i < N; ++i)
    input_array.values[i] = input_ptr[i];

  // may use multi thread blocks if performance bottleneck
  dim3 grid(1);
  dim3 block(static_cast<int>(input.numel()));
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
  copy_to_gpu_no_ce_kernel<<<grid, block, 0, stream>>>(input_array, output.data_ptr<int>());
  C10_CUDA_KERNEL_LAUNCH_CHECK();
}
```

#### å…³é”®æ­¥éª¤è§£æ

**æ­¥éª¤ 1ï¼šå‚æ•°éªŒè¯**
```cpp
TORCH_CHECK(input.dim() == 1, "input must be 1-D");
TORCH_CHECK(input.is_contiguous(), "input must be contiguous");
```
- æ£€æŸ¥è¾“å…¥æ˜¯å¦æ˜¯ä¸€ç»´æ•°ç»„
- æ£€æŸ¥æ˜¯å¦è¿ç»­ï¼ˆä¿è¯å†…å­˜è®¿é—®æ•ˆç‡ï¼‰

**æ­¥éª¤ 2ï¼šå‡†å¤‡æ•°æ®**
```cpp
InputArray<N> input_array;
const int* input_ptr = input.data_ptr<int>();
for (int i = 0; i < N; ++i)
  input_array.values[i] = input_ptr[i];
```
- åœ¨ä¸»æœºç«¯åˆ›å»º `InputArray` å¯¹è±¡
- ä» PyTorch Tensor å¤åˆ¶æ•°æ®åˆ°ç»“æ„ä½“
- å¯åŠ¨ kernel æ—¶ï¼Œè¿™ä¸ªç»“æ„ä½“ä¼šæ‹·è´åˆ° GPU å¸¸é‡å†…å­˜

**æ­¥éª¤ 3ï¼šé…ç½® Kernel å‚æ•°**
```cpp
dim3 grid(1);        // 1 ä¸ª block
dim3 block(N);       // N ä¸ªçº¿ç¨‹ï¼ˆN æ˜¯æ•°ç»„å¤§å°ï¼‰
```
- **Grid é…ç½®**ï¼šä½¿ç”¨ 1 ä¸ª block
- **Block é…ç½®**ï¼šä½¿ç”¨ `N` ä¸ªçº¿ç¨‹ï¼ˆæ¯ä¸ªå…ƒç´ ä¸€ä¸ªçº¿ç¨‹ï¼‰

**æ­¥éª¤ 4ï¼šå¯åŠ¨ Kernel**
```cpp
copy_to_gpu_no_ce_kernel<<<grid, block, 0, stream>>>(
    input_array, 
    output.data_ptr<int>()
);
```
- **`<<<grid, block, shared_mem, stream>>>`**ï¼šCUDA kernel å¯åŠ¨è¯­æ³•
- `input_array`ï¼šä½œä¸ºå‚æ•°ä¼ é€’ï¼ˆä¼šæ‹·è´åˆ° GPUï¼‰
- `output.data_ptr<int>()`ï¼šGPU å†…å­˜ä¸­çš„è¾“å‡ºæŒ‡é’ˆ

---

## ğŸ¯ è®¾è®¡è¦ç‚¹

### 1. ä¸ºä»€ä¹ˆä½¿ç”¨å¸¸é‡å†…å­˜ï¼Ÿ

**å¸¸é‡å†…å­˜çš„ç‰¹ç‚¹**ï¼š
- **åªè¯»**ï¼š`const InputArray`
- **ç¼“å­˜**ï¼šæœ‰ä¸“é—¨çš„å¸¸é‡ç¼“å­˜ï¼Œè®¿é—®æ›´å¿«
- **å¹¿æ’­**ï¼šæ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä½ç½®æ—¶ï¼Œåªéœ€è¦ä¸€æ¬¡å†…å­˜è®¿é—®

**é€‚ç”¨åœºæ™¯**ï¼š
- å°æ•°ç»„ï¼ˆé€šå¸¸ < 100 ä¸ªå…ƒç´ ï¼‰
- æ‰€æœ‰çº¿ç¨‹éœ€è¦ç›¸åŒçš„æ•°æ®
- åªè¯»æ•°æ®

### 2. ä¸ºä»€ä¹ˆç”¨æ¨¡æ¿ï¼Ÿ

**æ¨¡æ¿çš„ä¼˜åŠ¿**ï¼š
- **ç¼–è¯‘æ—¶ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨çŸ¥é“æ•°ç»„å¤§å°ï¼Œå¯ä»¥å±•å¼€å¾ªç¯
- **ç±»å‹å®‰å…¨**ï¼šç¼–è¯‘æ—¶æ£€æŸ¥ï¼Œé¿å…è¿è¡Œæ—¶é”™è¯¯
- **æ€§èƒ½**ï¼šé¿å…åŠ¨æ€åˆ†é…å’Œæ£€æŸ¥

**é™åˆ¶**ï¼š
- æ•°ç»„å¤§å°å¿…é¡»åœ¨ç¼–è¯‘æ—¶ç¡®å®š
- éœ€è¦ä¸ºæ¯ä¸ªå¤§å°åˆ›å»ºæ¨¡æ¿å®ä¾‹åŒ–

### 3. ä¸ºä»€ä¹ˆåªç”¨ä¸€ä¸ª Blockï¼Ÿ

**å½“å‰å®ç°**ï¼š
```cpp
dim3 grid(1);        // åªæœ‰ 1 ä¸ª block
dim3 block(N);       // N ä¸ªçº¿ç¨‹
```

**åŸå› **ï¼š
- æ•°ç»„é€šå¸¸å¾ˆå°ï¼ˆå¦‚ 64ã€72ï¼‰
- ä¸€ä¸ª block è¶³å¤Ÿå¤„ç†
- ç®€åŒ–ä»£ç ï¼Œä¸éœ€è¦å¤š block åè°ƒ

**æ³¨æ„**ï¼šSGLang çš„ Copy ç®—å­**åªé€‚ç”¨äºå°æ•°ç»„**ï¼ˆ64 å’Œ 72ï¼‰ã€‚è™½ç„¶ kernel ä»£ç æœ¬èº«æ”¯æŒå¤š blockï¼ˆä½¿ç”¨äº† `blockIdx.x`ï¼‰ï¼Œä½†å®é™…è°ƒç”¨æ—¶åªé…ç½®äº†å• blockï¼ˆ`grid(1)`ï¼‰ï¼Œå¹¶ä¸”å…¥å£å‡½æ•°åªæ”¯æŒè¿™ä¸¤ä¸ªç‰¹å®šå¤§å°ã€‚å¦‚æœæ•°ç»„å¾ˆå¤§ï¼ˆè¶…è¿‡ 1024ï¼‰ï¼Œå½“å‰å®ç°ä¼šå¤±è´¥ã€‚å¤„ç†å¤§æ•°ç»„çš„é€šç”¨æ–¹æ¡ˆè¯·å‚è€ƒä¸‹é¢çš„"æ‰©å±•å†…å®¹"ç« èŠ‚ã€‚

### 4. ä¸ºä»€ä¹ˆæ˜¯ 64 å’Œ 72ï¼Ÿ

**è¿™ä¸¤ä¸ªæ•°å­—çš„æ¥æº**ï¼š

è¿™ä¸¤ä¸ªæ•°å­—æ¥æºäº **MoEï¼ˆMixture of Expertsï¼‰æ¨¡å‹**çš„å®é™…ä½¿ç”¨åœºæ™¯ï¼š

1. **64 ä¸ª experts**ï¼š
   - è¿™æ˜¯è®¸å¤šå¤§å‹ MoE æ¨¡å‹çš„æ ‡å‡†é…ç½®
   - ä¾‹å¦‚ï¼š**DeepSeek-V2**ã€**Qwen2-MoE** ç­‰ä¸»æµæ¨¡å‹ä½¿ç”¨ 64 ä¸ª experts
   - `num_recv_tokens_per_expert` æ•°ç»„çš„é•¿åº¦ç­‰äº expert æ•°é‡

2. **72 ä¸ª experts**ï¼š
   - å¯èƒ½æ˜¯ **64 ä¸ªåŸºç¡€ experts + 8 ä¸ªå†—ä½™ experts** = 72
   - åœ¨ Expert Parallel (EP) æ¨¡å¼ä¸‹ï¼Œä¸ºäº†å®¹é”™å’Œè´Ÿè½½å‡è¡¡ï¼Œä¼šæ·»åŠ å†—ä½™ experts
   - ä»£ç ä¸­å¯ä»¥çœ‹åˆ°ï¼š`num_local_experts + ep_num_redundant_experts`

**å®é™…ä½¿ç”¨åœºæ™¯**ï¼š

```python
# åœ¨ deep_gemm.py ä¸­çš„å®é™…ä½¿ç”¨
def copy_list_to_gpu_no_ce(arr: List[int]):
    # arr æ˜¯ num_recv_tokens_per_expertï¼Œé•¿åº¦ä¸º expert æ•°é‡
    # æ¯ä¸ªå…ƒç´ è¡¨ç¤ºè¯¥ expert æ¥æ”¶åˆ°çš„ token æ•°é‡
    tensor_cpu = torch.tensor(arr, dtype=torch.int32, device="cpu")
    tensor_gpu = torch.empty_like(tensor_cpu, device="cuda")
    copy_to_gpu_no_ce(tensor_cpu, tensor_gpu)  # éœ€è¦ arr çš„é•¿åº¦ä¸º 64 æˆ– 72
    return tensor_gpu
```

**è®¾è®¡è€ƒè™‘**ï¼š
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šé’ˆå¯¹ç‰¹å®šå¤§å°ç¼–è¯‘æ¨¡æ¿ï¼Œç¼–è¯‘å™¨å¯ä»¥å……åˆ†ä¼˜åŒ–
- âœ… **å¸¸é‡å†…å­˜**ï¼šå°æ•°ç»„é€‚åˆæ”¾åœ¨å¸¸é‡å†…å­˜ä¸­ï¼Œè®¿é—®æ›´å¿«
- âœ… **å®é™…éœ€æ±‚**ï¼šè¦†ç›–äº† SGLang æ”¯æŒçš„ä¸»è¦ MoE æ¨¡å‹çš„ expert æ•°é‡
- âš ï¸ **æ‰©å±•æ€§**ï¼šä»£ç æ³¨é‡Š `// Can use macro if there are more N needed` è¡¨æ˜ï¼Œå¦‚æœæœªæ¥éœ€è¦æ”¯æŒå…¶ä»–å¤§å°ï¼Œå¯ä»¥é€šè¿‡å®æˆ–æ¨¡æ¿å®ä¾‹åŒ–æ·»åŠ 

**å¸¸è§ MoE æ¨¡å‹çš„ expert æ•°é‡**ï¼š
- Mixtral: 8 experts
- DeepSeek-V2: 64 experts
- Qwen2-MoE: 64 experts
- DBRX: 16 experts
- PhiMoE: 16 experts

---

## ğŸ“š æ‰©å±•å†…å®¹ï¼šå¤„ç†è¶…å¤§æ•°ç»„çš„é€šç”¨æ–¹æ¡ˆ

> **æ³¨æ„**ï¼šä»¥ä¸‹å†…å®¹æ˜¯é€šç”¨çš„ CUDA ç¼–ç¨‹çŸ¥è¯†ï¼Œ**ä¸æ˜¯ SGLang ä¸­ Copy ç®—å­çš„å®ç°**ã€‚
> 
> SGLang çš„ Copy ç®—å­è®¾è®¡ç”¨äº**å°æ•°ç»„**ï¼ˆåªæ”¯æŒ 64ã€72 ä¸¤ä¸ªå¤§å°ï¼‰ï¼Œè™½ç„¶ kernel ä»£ç æŠ€æœ¯ä¸Šæ”¯æŒå¤š blockï¼Œä½†å®é™…é…ç½®ä¸ºå• Blockã€‚å…¥å£å‡½æ•°ä¹Ÿåªæ”¯æŒè¿™ä¸¤ä¸ªç‰¹å®šå¤§å°ï¼Œå…¶ä»–å¤§å°ä¼šç›´æ¥æŠ¥é”™ã€‚
> 
> å¦‚æœä½ éœ€è¦å¤„ç†è¶…å¤§æ•°ç»„ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹é€šç”¨æ–¹æ¡ˆï¼Œä½† SGLang æœ¬èº«ä¸æä¾›è¿™äº›å®ç°ã€‚

### ğŸš¨ é—®é¢˜ï¼šè¶…è¿‡æœ€å¤§çº¿ç¨‹æ•°çš„æƒ…å†µ

### CUDA çº¿ç¨‹é™åˆ¶

**å…³é”®é™åˆ¶**ï¼š
- **æ¯ä¸ª Block æœ€å¤§çº¿ç¨‹æ•°**ï¼š1024ï¼ˆæ‰€æœ‰ç°ä»£ GPUï¼‰
- **Grid æœ€å¤§ Block æ•°**ï¼šä¸€ç»´ç½‘æ ¼æœ€å¤š 2Â³Â¹-1 ä¸ª blocks
- **å½“å‰å®ç°çš„é—®é¢˜**ï¼šå¦‚æœ `N > 1024`ï¼Œä½¿ç”¨ `dim3 block(N)` ä¼š**å¯åŠ¨å¤±è´¥**

**ç¤ºä¾‹é—®é¢˜åœºæ™¯**ï¼š
```cpp
// å¦‚æœ N = 5000
dim3 grid(1);
dim3 block(5000);  // âŒ é”™è¯¯ï¼è¶…è¿‡ 1024 çš„é™åˆ¶
// å¯åŠ¨ kernel æ—¶ä¼šæŠ¥é”™ï¼šinvalid configuration argument
```

### è§£å†³æ–¹æ¡ˆ 1ï¼šå¤š Block å¹¶è¡Œï¼ˆé€‚åˆä¸­ç­‰å¤§å°æ•°ç»„ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæ•°ç»„å¤§å°åœ¨ 1024 åˆ°å‡ ç™¾ä¸‡ä¹‹é—´

**å®ç°æ–¹æ³•**ï¼š
```cpp
template <int N>
void copy_to_gpu_no_ce_impl_multi_block(const at::Tensor& input, at::Tensor& output) {
  // ... å‰é¢çš„éªŒè¯ä»£ç ç›¸åŒ ...
  
  InputArray<N> input_array;
  const int* input_ptr = input.data_ptr<int>();
  for (int i = 0; i < N; ++i)
    input_array.values[i] = input_ptr[i];

  // ä½¿ç”¨å¤šä¸ª blocks
  const int threads_per_block = 256;  // æ¯ä¸ª block 256 ä¸ªçº¿ç¨‹
  int num_blocks = (N + threads_per_block - 1) / threads_per_block;  // å‘ä¸Šå–æ•´
  
  dim3 grid(num_blocks);
  dim3 block(threads_per_block);
  
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
  copy_to_gpu_no_ce_kernel<<<grid, block, 0, stream>>>(
      input_array, output.data_ptr<int>());
  C10_CUDA_KERNEL_LAUNCH_CHECK();
}
```

**å·¥ä½œåŸç†**ï¼š
- **çº¿ç¨‹ç´¢å¼•è®¡ç®—**ï¼š`idx = blockIdx.x * blockDim.x + threadIdx.x`
- **ç¤ºä¾‹**ï¼š`N=5000`, `threads_per_block=256`
  - `num_blocks = (5000 + 256 - 1) / 256 = 20`
  - Block 0: çº¿ç¨‹ 0-255 å¤„ç†å…ƒç´  0-255
  - Block 1: çº¿ç¨‹ 0-255 å¤„ç†å…ƒç´  256-511
  - ...
  - Block 19: çº¿ç¨‹ 0-255 å¤„ç†å…ƒç´  4864-5119ï¼ˆè¾¹ç•Œæ£€æŸ¥ç¡®ä¿ä¸è¶Šç•Œï¼‰

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ç›´æ¥ï¼Œæ˜“äºç†è§£
- âœ… é€‚åˆä¸­ç­‰å¤§å°æ•°ç»„ï¼ˆå‡ ç™¾ä¸‡å…ƒç´ ä»¥å†…ï¼‰
- âœ… æ¯ä¸ªçº¿ç¨‹åªå¤„ç†ä¸€ä¸ªå…ƒç´ ï¼Œé€»è¾‘æ¸…æ™°

**é™åˆ¶**ï¼š
- âš ï¸ å¦‚æœæ•°ç»„éå¸¸å¤§ï¼ˆå¦‚ 10 äº¿å…ƒç´ ï¼‰ï¼Œéœ€è¦åˆ›å»ºå¤§é‡ blocks
- âš ï¸ Grid å¤§å°æœ‰ç¡¬ä»¶é™åˆ¶ï¼ˆé€šå¸¸ 65535 ä¸ª blocksï¼‰

### è§£å†³æ–¹æ¡ˆ 2ï¼šGrid-Stride Loopï¼ˆé€‚åˆè¶…å¤§æ•°ç»„ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæ•°ç»„å¤§å°å¯èƒ½éå¸¸å¤§ï¼ˆå‡ ç™¾ä¸‡åˆ°å‡ åäº¿å…ƒç´ ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯ 1:1 æ˜ å°„

**Kernel å®ç°**ï¼š
```cpp
template <int N>
__global__ void copy_to_gpu_no_ce_kernel_grid_stride(
    const InputArray<N> input_array, int* output) {
  // è®¡ç®—æ€»çº¿ç¨‹æ•°
  int total_threads = blockDim.x * gridDim.x;
  
  // è®¡ç®—å½“å‰çº¿ç¨‹çš„èµ·å§‹ç´¢å¼•
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  
  // Grid-Stride Loopï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ 
  for (int i = idx; i < N; i += total_threads) {
    output[i] = input_array.values[i];
  }
}
```

**ä¸»æœºç«¯è°ƒç”¨**ï¼š
```cpp
template <int N>
void copy_to_gpu_no_ce_impl_grid_stride(const at::Tensor& input, at::Tensor& output) {
  // ... éªŒè¯ä»£ç  ...
  
  InputArray<N> input_array;
  const int* input_ptr = input.data_ptr<int>();
  for (int i = 0; i < N; ++i)
    input_array.values[i] = input_ptr[i];

  // å›ºå®šæ•°é‡çš„ blocksï¼Œä¸éšæ•°ç»„å¤§å°å˜åŒ–
  const int threads_per_block = 256;
  const int num_blocks = 1024;  // å›ºå®šä½¿ç”¨ 1024 ä¸ª blocks
  
  dim3 grid(num_blocks);
  dim3 block(threads_per_block);
  
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
  copy_to_gpu_no_ce_kernel_grid_stride<<<grid, block, 0, stream>>>(
      input_array, output.data_ptr<int>());
  C10_CUDA_KERNEL_LAUNCH_CHECK();
}
```

**å·¥ä½œåŸç†è¯¦è§£**ï¼š

å‡è®¾ `N = 10000`ï¼Œ`threads_per_block = 256`ï¼Œ`num_blocks = 1024`ï¼š

```
æ€»çº¿ç¨‹æ•° = 1024 Ã— 256 = 262144
stride = 262144ï¼ˆæ¯ä¸ªçº¿ç¨‹çš„æ­¥é•¿ï¼‰

çº¿ç¨‹ 0ï¼šå¤„ç†å…ƒç´  0, 262144, 524288, ...ï¼ˆä½†åªæœ‰ 0 < 10000ï¼Œæ‰€ä»¥åªå¤„ç†å…ƒç´  0ï¼‰
çº¿ç¨‹ 1ï¼šå¤„ç†å…ƒç´  1, 262145, 524289, ...ï¼ˆä½†åªæœ‰ 1 < 10000ï¼Œæ‰€ä»¥åªå¤„ç†å…ƒç´  1ï¼‰
...
çº¿ç¨‹ 9999ï¼šå¤„ç†å…ƒç´  9999, 272143, ...ï¼ˆä½†åªæœ‰ 9999 < 10000ï¼Œæ‰€ä»¥åªå¤„ç†å…ƒç´  9999ï¼‰
çº¿ç¨‹ 10000-262143ï¼šå¾ªç¯æ¡ä»¶ i < 10000 ä¸æ»¡è¶³ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- âœ… **å›ºå®š Grid å¤§å°**ï¼šæ— è®ºæ•°ç»„å¤šå¤§ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„ grid é…ç½®
- âœ… **å¯æ‰©å±•æ€§å¼º**ï¼šå¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„æ•°ç»„ï¼ˆåªè¦ä¸è¶…è¿‡ int64 èŒƒå›´ï¼‰
- âœ… **è´Ÿè½½å‡è¡¡**ï¼šæ‰€æœ‰çº¿ç¨‹å‡åŒ€åˆ†é…å·¥ä½œ
- âœ… **é¿å… Grid é™åˆ¶**ï¼šä¸éœ€è¦åˆ›å»ºå¤§é‡ blocks

**æ€§èƒ½è€ƒè™‘**ï¼š
- å¯¹äºå°æ•°ç»„ï¼ˆ< 1000ï¼‰ï¼ŒGrid-Stride Loop å¯èƒ½æœ‰è½»å¾®å¼€é”€ï¼ˆå¾ªç¯æ£€æŸ¥ï¼‰
- å¯¹äºå¤§æ•°ç»„ï¼ˆ> 100ä¸‡ï¼‰ï¼Œæ€§èƒ½ä¸å¤š Block æ–¹æ¡ˆç›¸å½“æˆ–æ›´å¥½
- å¯¹äºè¶…å¤§æ•°ç»„ï¼ˆ> 1äº¿ï¼‰ï¼ŒGrid-Stride Loop æ˜¯å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆ

### ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | å¤š Block æ–¹æ¡ˆ | Grid-Stride Loop æ–¹æ¡ˆ |
|------|--------------|---------------------|
| **é€‚ç”¨æ•°ç»„å¤§å°** | 1024 ~ å‡ ç™¾ä¸‡ | ä»»æ„å¤§å°ï¼ˆå‡ ç™¾ä¸‡åˆ°å‡ åäº¿ï¼‰ |
| **Grid é…ç½®** | åŠ¨æ€ï¼ˆéš N å˜åŒ–ï¼‰ | å›ºå®šï¼ˆå¦‚ 1024 blocksï¼‰ |
| **æ¯ä¸ªçº¿ç¨‹å¤„ç†** | 1 ä¸ªå…ƒç´  | å¤šä¸ªå…ƒç´ ï¼ˆå¾ªç¯ï¼‰ |
| **ä»£ç å¤æ‚åº¦** | ç®€å• | ç¨å¤æ‚ï¼ˆéœ€è¦å¾ªç¯ï¼‰ |
| **ç¡¬ä»¶é™åˆ¶** | å— Grid å¤§å°é™åˆ¶ | ä¸å—é™åˆ¶ |
| **å°æ•°ç»„æ€§èƒ½** | æ›´å¥½ï¼ˆæ— å¾ªç¯å¼€é”€ï¼‰ | ç¨å·®ï¼ˆæœ‰å¾ªç¯æ£€æŸ¥ï¼‰ |
| **å¤§æ•°ç»„æ€§èƒ½** | ç›¸å½“ | ç›¸å½“æˆ–æ›´å¥½ |

### å®é™…åº”ç”¨å»ºè®®

**é€‰æ‹©ç­–ç•¥**ï¼š

1. **å°æ•°ç»„ï¼ˆN < 1024ï¼‰**ï¼š
   - ä½¿ç”¨å½“å‰å®ç°ï¼ˆå• Blockï¼‰
   - æœ€ç®€å•ï¼Œæ€§èƒ½æœ€å¥½

2. **ä¸­ç­‰æ•°ç»„ï¼ˆ1024 â‰¤ N < 1000ä¸‡ï¼‰**ï¼š
   - ä½¿ç”¨å¤š Block æ–¹æ¡ˆ
   - ä»£ç ç®€å•ï¼Œæ€§èƒ½å¥½

3. **è¶…å¤§æ•°ç»„ï¼ˆN â‰¥ 1000ä¸‡ï¼‰**ï¼š
   - ä½¿ç”¨ Grid-Stride Loop æ–¹æ¡ˆ
   - å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆï¼Œé¿å…ç¡¬ä»¶é™åˆ¶

**PyTorch çš„å®è·µ**ï¼š
PyTorch å†…éƒ¨å¹¿æ³›ä½¿ç”¨ Grid-Stride Loop æ¨¡å¼ï¼Œå› ä¸ºå®ƒï¼š
- å¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„ Tensor
- ä»£ç æ›´é€šç”¨ï¼Œä¸éœ€è¦ä¸ºä¸åŒå¤§å°å†™ä¸åŒç‰ˆæœ¬
- æ€§èƒ½ç»è¿‡å……åˆ†ä¼˜åŒ–

**å‚è€ƒå®ç°**ï¼š
```cpp
// PyTorch é£æ ¼çš„ Grid-Stride Loop
#define CUDA_KERNEL_LOOP(i, n) \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
       i < (n); \
       i += blockDim.x * gridDim.x)

__global__ void copy_kernel(const int* input, int* output, int N) {
  CUDA_KERNEL_LOOP(idx, N) {
    output[idx] = input[idx];
  }
}
```

### ğŸ’¡ æ€»ç»“

**SGLang Copy ç®—å­çš„è®¾è®¡å“²å­¦**ï¼š
- âœ… **ä¸“ä¸ºå°æ•°ç»„ä¼˜åŒ–**ï¼šä½¿ç”¨å• Blockï¼Œç®€å•é«˜æ•ˆ
- âœ… **ç¼–è¯‘æ—¶ç¡®å®šå¤§å°**ï¼šé€šè¿‡æ¨¡æ¿å‚æ•°ï¼Œç¼–è¯‘å™¨å¯ä»¥å……åˆ†ä¼˜åŒ–
- âœ… **ä½¿ç”¨å¸¸é‡å†…å­˜**ï¼šå°æ•°ç»„é€‚åˆæ”¾åœ¨å¸¸é‡å†…å­˜ä¸­ï¼Œè®¿é—®æ›´å¿«

**å¦‚æœä½ éœ€è¦å¤„ç†å¤§æ•°ç»„**ï¼š
- å¯ä»¥è€ƒè™‘ä½¿ç”¨ PyTorch çš„é€šç”¨ CUDA æ“ä½œï¼ˆå¦‚ `torch.copy_`ï¼‰
- æˆ–è€…å‚è€ƒä¸Šè¿°é€šç”¨æ–¹æ¡ˆè‡ªè¡Œå®ç°
- ä½†è¦æ³¨æ„ï¼Œå¤§æ•°ç»„ä¸é€‚åˆä½¿ç”¨å¸¸é‡å†…å­˜ï¼ˆ`InputArray` ç»“æ„ä½“ï¼‰ï¼Œéœ€è¦æ”¹ç”¨å…¨å±€å†…å­˜æŒ‡é’ˆ

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### å†…å­˜è®¿é—®æ¨¡å¼

**è¯»å–ï¼ˆå¸¸é‡å†…å­˜ï¼‰**ï¼š
- æ‰€æœ‰çº¿ç¨‹è¯»å–ç›¸åŒçš„æ•°æ®ï¼ˆå¹¿æ’­ï¼‰
- ä½¿ç”¨å¸¸é‡ç¼“å­˜ï¼Œé€Ÿåº¦å¾ˆå¿«
- å¦‚æœç¼“å­˜å‘½ä¸­ï¼Œå‡ ä¹æ— å»¶è¿Ÿ

**å†™å…¥ï¼ˆå…¨å±€å†…å­˜ï¼‰**ï¼š
- æ¯ä¸ªçº¿ç¨‹å†™å…¥ä¸åŒçš„ä½ç½®
- è¿ç»­å†™å…¥ï¼Œå†…å­˜åˆå¹¶è®¿é—®
- å¸¦å®½åˆ©ç”¨ç‡é«˜

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨å¸¸é‡å†…å­˜**ï¼šå¯¹äºå°æ•°ç»„ï¼Œå·²ç»å®ç° âœ“
2. **å†…å­˜åˆå¹¶è®¿é—®**ï¼šè¿ç»­è®¿é—®ï¼Œå·²å®ç° âœ“
3. **å‘é‡åŒ–**ï¼šå¯¹äºæ›´å¤§çš„æ•°ç»„ï¼Œå¯ä»¥ä½¿ç”¨ `float4` ä¸€æ¬¡å¤åˆ¶ 4 ä¸ªå…ƒç´ 

**å‘é‡åŒ–ç‰ˆæœ¬ç¤ºä¾‹**ï¼š
```cpp
__global__ void copy_vectorized(int* output, const int* input, int N) {
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    if (idx + 3 < N) {
        // ä¸€æ¬¡å¤åˆ¶ 4 ä¸ªå…ƒç´ 
        *((float4*)&output[idx]) = *((float4*)&input[idx]);
    }
}
```

---

## ğŸ” ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸ä¾èµ– PyTorchï¼‰

å¦‚æœä½ æƒ³ç†è§£æ ¸å¿ƒé€»è¾‘ï¼Œè¿™é‡Œæ˜¯çº¯ CUDA ç‰ˆæœ¬ï¼š

```cpp
#include <cuda_runtime.h>
#include <stdio.h>

// æœ€ç®€å•çš„ Copy Kernel
__global__ void copy_kernel(const int* input, int* output, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        output[idx] = input[idx];
    }
}

int main() {
    const int N = 1000;
    size_t size = N * sizeof(int);
    
    // ä¸»æœºç«¯æ•°æ®
    int* h_input = (int*)malloc(size);
    int* h_output = (int*)malloc(size);
    
    // åˆå§‹åŒ–è¾“å…¥
    for (int i = 0; i < N; i++) {
        h_input[i] = i;
    }
    
    // è®¾å¤‡ç«¯æ•°æ®
    int* d_input;
    int* d_output;
    cudaMalloc(&d_input, size);
    cudaMalloc(&d_output, size);
    
    // å¤åˆ¶è¾“å…¥åˆ°è®¾å¤‡
    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);
    
    // é…ç½® kernel
    int threads_per_block = 256;
    int blocks = (N + threads_per_block - 1) / threads_per_block;
    
    // å¯åŠ¨ kernel
    copy_kernel<<<blocks, threads_per_block>>>(
        d_input, d_output, N);
    
    // åŒæ­¥ç­‰å¾…
    cudaDeviceSynchronize();
    
    // å¤åˆ¶ç»“æœå›ä¸»æœº
    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);
    
    // éªŒè¯ç»“æœ
    bool success = true;
    for (int i = 0; i < N; i++) {
        if (h_input[i] != h_output[i]) {
            printf("Error at index %d: %d != %d\n", i, h_input[i], h_output[i]);
            success = false;
        }
    }
    
    if (success) {
        printf("âœ“ Copy successful!\n");
    }
    
    // æ¸…ç†
    free(h_input);
    free(h_output);
    cudaFree(d_input);
    cudaFree(d_output);
    
    return 0;
}
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µ

1. **CUDA Kernel**ï¼š`__global__` å‡½æ•°ï¼Œåœ¨ GPU ä¸Šæ‰§è¡Œ
2. **çº¿ç¨‹ç´¢å¼•**ï¼š`idx = blockIdx.x * blockDim.x + threadIdx.x`
3. **è¾¹ç•Œæ£€æŸ¥**ï¼š`if (idx < N)` é˜²æ­¢è¶Šç•Œ
4. **å†…å­˜ç±»å‹**ï¼š
   - å¸¸é‡å†…å­˜ï¼š`const` å‚æ•°ï¼Œåªè¯»ï¼Œæœ‰ç¼“å­˜
   - å…¨å±€å†…å­˜ï¼šæŒ‡é’ˆå‚æ•°ï¼Œå¯è¯»å†™

### å…³é”®ç‚¹

- âœ… **æœ€ç®€å•çš„å¹¶è¡Œæ¨¡å¼**ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ 
- âœ… **æ— ä¾èµ–**ï¼šçº¿ç¨‹é—´ä¸éœ€è¦åŒæ­¥
- âœ… **å†…å­˜åˆå¹¶**ï¼šè¿ç»­è®¿é—®ï¼Œé«˜æ•ˆ
- âœ… **å¸¸é‡å†…å­˜ä¼˜åŒ–**ï¼šå°æ•°ç»„ä½¿ç”¨å¸¸é‡å†…å­˜

### å­¦ä¹ ä»·å€¼

Copy ç®—å­æ˜¯å­¦ä¹  CUDA çš„**æœ€ä½³èµ·ç‚¹**ï¼Œå› ä¸ºå®ƒï¼š
- å±•ç¤ºäº† CUDA kernel çš„åŸºæœ¬ç»“æ„
- è¯´æ˜äº†çº¿ç¨‹ç´¢å¼•çš„è®¡ç®—
- æ¼”ç¤ºäº†å†…å­˜è®¿é—®æ¨¡å¼
- æ²¡æœ‰å¤æ‚çš„è®¡ç®—ï¼Œå®¹æ˜“ç†è§£

---

## ğŸ”— ç›¸å…³èµ„æº

- **CUDA ç¼–ç¨‹æ¨¡å‹**ï¼šç†è§£ threadã€blockã€grid çš„æ¦‚å¿µ
- **å†…å­˜å±‚æ¬¡**ï¼šå¸¸é‡å†…å­˜ vs å…¨å±€å†…å­˜
- **ä¸‹ä¸€ä¸ªç®—å­**ï¼š[02_Activationç®—å­.md](./02_Activationç®—å­.md)ï¼ˆSiLUã€GELUï¼‰

