# Fill ç®—å­æ¼”ç¤º

## ğŸ“– ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Fill ç®—å­å®ç°ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ CUDA å®ç°å¼ é‡å¡«å……æ“ä½œã€‚

## ğŸ¯ åŠŸèƒ½è¯´æ˜

Fill ç®—å­ä½¿ç”¨æŒ‡å®šçš„æ ‡é‡å€¼å¡«å……æ•´ä¸ªå¼ é‡ï¼š
- **è¾“å…¥**ï¼šå¼ é‡ï¼ˆGPU å†…å­˜ï¼‰å’Œå¡«å……å€¼
- **è¾“å‡º**ï¼šå¡«å……åçš„å¼ é‡

## ğŸ”§ ç¼–è¯‘

### å‰ç½®æ¡ä»¶
- CUDA Toolkitï¼ˆæ¨è 11.0 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰
- NVIDIA GPU
- GCC æˆ– Clang ç¼–è¯‘å™¨

### ç¼–è¯‘å‘½ä»¤

```bash
# ä½¿ç”¨ nvcc ç¼–è¯‘
nvcc -o fill_demo fill_demo.cu -arch=sm_75

# æˆ–è€…æŒ‡å®šæ›´é€šç”¨çš„æ¶æ„
nvcc -o fill_demo fill_demo.cu -arch=sm_60

# å¦‚æœéœ€è¦è°ƒè¯•ä¿¡æ¯
nvcc -g -G -o fill_demo fill_demo.cu -arch=sm_75

# ä¼˜åŒ–ç‰ˆæœ¬
nvcc -O3 -o fill_demo fill_demo.cu -arch=sm_75
```

### æ¶æ„é€‰æ‹©

æ ¹æ®ä½ çš„ GPU æ¶æ„é€‰æ‹©ï¼š
- **sm_60**: Pascal (GTX 10xx)
- **sm_75**: Turing (RTX 20xx, GTX 16xx)
- **sm_80**: Ampere (RTX 30xx, A100)
- **sm_86**: Ampere (RTX 30xx ç§»åŠ¨ç‰ˆ)
- **sm_89**: Ada Lovelace (RTX 40xx)

æŸ¥çœ‹ä½ çš„ GPU æ¶æ„ï¼š
```bash
nvidia-smi --query-gpu=compute_cap --format=csv
```

## ğŸš€ è¿è¡Œ

```bash
./fill_demo
```

## ğŸ“ ä»£ç ç»“æ„

### 1. FillFunctorï¼ˆå‡½æ•°å¯¹è±¡ï¼‰

```cpp
template<typename T>
struct FillFunctor {
    T value;
    FillFunctor(T v): value(v) {}
    __device__ __forceinline__ T operator() () const {
        return value;
    }
};
```

**è¯´æ˜**ï¼š
- è¿™æ˜¯ C++ çš„ Functor æ¨¡å¼
- é‡è½½ `operator()` ä½¿å¯¹è±¡å¯åƒå‡½æ•°ä¸€æ ·è°ƒç”¨
- `__device__` è¡¨ç¤ºå¯åœ¨ GPU ä¸Šæ‰§è¡Œ

### 2. CUDA Kernelï¼ˆä¸¤ä¸ªç‰ˆæœ¬ï¼‰

#### ç‰ˆæœ¬ 1ï¼šä½¿ç”¨ Functor
```cpp
template<typename T>
__global__ void fill_kernel(T* output, int64_t numel, FillFunctor<T> functor) {
    // Grid-Stride Loop æ¨¡å¼
    // æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ 
}
```

#### ç‰ˆæœ¬ 2ï¼šç›´æ¥ä¼ é€’å€¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
```cpp
template<typename T>
__global__ void fill_kernel_simple(T* output, int64_t numel, T value) {
    // æ›´ç®€å•ï¼Œé¿å… functor æ‹·è´
}
```

**Grid-Stride Loop æ¨¡å¼**ï¼š
- æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„å…ƒç´ æ•° = æ€»å…ƒç´ æ•° / (blocks Ã— threads)
- æ”¯æŒä»»æ„å¤§å°çš„å¼ é‡
- è‡ªåŠ¨å¤„ç†è¾¹ç•Œæƒ…å†µ

### 3. ä¸»æœºç«¯å°è£…å‡½æ•°

- `fill_cuda()`: ä½¿ç”¨ Functor ç‰ˆæœ¬
- `fill_cuda_simple()`: ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬

## ğŸ§ª æµ‹è¯•è¯´æ˜

ç¨‹åºåŒ…å«ä¸‰ä¸ªæµ‹è¯•ï¼š

1. **æµ‹è¯• 1**: ä½¿ç”¨ FillFunctor å¡«å…… float ç±»å‹å¼ é‡ä¸º 1.0
2. **æµ‹è¯• 2**: ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬å¡«å…… float ç±»å‹å¼ é‡ä¸º 2.5
3. **æµ‹è¯• 3**: æµ‹è¯• int ç±»å‹ï¼Œå¡«å……å€¼ä¸º 42

æ¯ä¸ªæµ‹è¯•éƒ½ä¼šï¼š
- åœ¨ GPU ä¸Šåˆ†é…å†…å­˜
- å¯åŠ¨ kernel æ‰§è¡Œå¡«å……
- å°†ç»“æœå¤åˆ¶å› CPU
- éªŒè¯ç»“æœæ˜¯å¦æ­£ç¡®

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–è¯´æ˜

### Grid-Stride Loop çš„ä¼˜åŠ¿

```cpp
int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
int64_t stride = blockDim.x * gridDim.x;
for (int64_t i = idx; i < numel; i += stride) {
    // å¤„ç†å…ƒç´ 
}
```

**ä¼˜ç‚¹**ï¼š
- æ”¯æŒä»»æ„å¤§å°çš„æ•°ç»„ï¼ˆä¸éœ€è¦æ˜¯çº¿ç¨‹æ•°çš„å€æ•°ï¼‰
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡
- ç®€åŒ–è¾¹ç•Œæ£€æŸ¥

### å‚æ•°é€‰æ‹©

- **threads_per_block = 256**: ç»éªŒå€¼ï¼Œå¹³è¡¡å ç”¨ç‡å’Œå¯„å­˜å™¨ä½¿ç”¨
- **max_blocks = 1024**: é™åˆ¶æœ€å¤§ block æ•°ï¼Œé¿å…è¿‡åº¦å¹¶è¡Œ

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æ·»åŠ  CUDA é”™è¯¯æ£€æŸ¥

```cpp
cudaError_t err = cudaGetLastError();
if (err != cudaSuccess) {
    fprintf(stderr, "Error: %s\n", cudaGetErrorString(err));
}
```

### 2. ä½¿ç”¨ CUDA-GDB è°ƒè¯•

```bash
# ç¼–è¯‘è°ƒè¯•ç‰ˆæœ¬
nvcc -g -G -o fill_demo fill_demo.cu

# ä½¿ç”¨ cuda-gdb
cuda-gdb ./fill_demo
```

### 3. ä½¿ç”¨ nsight-compute åˆ†ææ€§èƒ½

```bash
ncu --set full ./fill_demo
```

### 4. æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆåœ¨ kernel ä¸­ï¼‰

```cpp
__global__ void fill_kernel(...) {
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        printf("Debug info\n");
    }
}
```

## ğŸ“š æ‰©å±•é˜…è¯»

### ä¸ PyTorch å®ç°çš„å¯¹æ¯”

PyTorch çš„ Fill ç®—å­ä½¿ç”¨ `TensorIterator` æ¥å¤„ç†ï¼š
- è‡ªåŠ¨å¹¿æ’­
- ä¸åŒå†…å­˜å¸ƒå±€
- ç±»å‹è½¬æ¢

æˆ‘ä»¬çš„ç®€åŒ–ç‰ˆæœ¬åªå¤„ç†äº†ç®€å•çš„ contiguous å†…å­˜å¸ƒå±€ã€‚

### ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **æ”¯æŒé contiguous å†…å­˜å¸ƒå±€**ï¼šéœ€è¦ stride ä¿¡æ¯
2. **æ”¯æŒå¹¿æ’­**ï¼šå¤„ç†ä¸åŒå½¢çŠ¶çš„å¼ é‡
3. **æ”¯æŒå¤šç§æ•°æ®ç±»å‹**ï¼šè‡ªåŠ¨ç±»å‹è½¬æ¢
4. **å‘é‡åŒ–ä¼˜åŒ–**ï¼šä¸€æ¬¡åŠ è½½/å­˜å‚¨å¤šä¸ªå…ƒç´ 

## âš ï¸ å¸¸è§é—®é¢˜

### é—®é¢˜ 1: "no kernel image is available"

**åŸå› **: GPU æ¶æ„ä¸åŒ¹é…

**è§£å†³**: é‡æ–°ç¼–è¯‘æ—¶æŒ‡å®šæ­£ç¡®çš„æ¶æ„ï¼Œæˆ–ä½¿ç”¨æ›´é€šç”¨çš„æ¶æ„ï¼š
```bash
nvcc -arch=sm_60 -o fill_demo fill_demo.cu
```

### é—®é¢˜ 2: ç»“æœä¸æ­£ç¡®

**å¯èƒ½åŸå› **:
- å†…å­˜æœªæ­£ç¡®åˆ†é…/é‡Šæ”¾
- Kernel å¯åŠ¨å‚æ•°é”™è¯¯
- æ²¡æœ‰åŒæ­¥ç­‰å¾… kernel å®Œæˆ

**æ£€æŸ¥**:
- æ·»åŠ  `cudaDeviceSynchronize()` ç¡®ä¿ kernel å®Œæˆ
- æ£€æŸ¥ CUDA é”™è¯¯

### é—®é¢˜ 3: æ€§èƒ½ä¸ä½³

**ä¼˜åŒ–å»ºè®®**:
- å¢åŠ æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„å…ƒç´ æ•°ï¼ˆå‘é‡åŒ–ï¼‰
- ä½¿ç”¨å…±äº«å†…å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
- è°ƒæ•´ block å¤§å°

## ğŸ“– å‚è€ƒèµ„æ–™

- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [PyTorch CUDA ç®—å­å®ç°è¯¦è§£](../CUDAç®—å­å®ç°è¯¦è§£.md)
- [Grid-Stride Loop Pattern](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)

