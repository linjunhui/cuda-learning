# PyTorch CUDA ç®—å­å®ç°è¯¦è§£

## ğŸ“– æ¦‚è¿°

æœ¬æ–‡æ¡£ä»ç®€å•åˆ°å¤æ‚è¯¦ç»†ä»‹ç» PyTorch ä¸­ CUDA ç®—å­çš„å®ç°æ–¹å¼ã€‚CUDA ç®—å­æ˜¯ PyTorch åœ¨ GPU ä¸Šæ‰§è¡Œè®¡ç®—çš„æ ¸å¿ƒï¼Œç†è§£å®ƒä»¬çš„å®ç°æœ‰åŠ©äºï¼š

- ç†è§£ PyTorch çš„ GPU è®¡ç®—æœºåˆ¶
- å­¦ä¹ å¦‚ä½•å®ç°é«˜æ•ˆçš„ CUDA ç®—å­
- ä¼˜åŒ–è‡ªå®šä¹‰ç®—å­æ€§èƒ½

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µï¼šFunctorï¼ˆå‡½æ•°å¯¹è±¡ï¼‰

åœ¨æ·±å…¥ CUDA ç®—å­ä¹‹å‰ï¼Œéœ€è¦ç†è§£ä¸€ä¸ªé‡è¦çš„ C++ æ¦‚å¿µï¼š**Functorï¼ˆå‡½æ•°å¯¹è±¡ï¼‰**ã€‚

### ä»€ä¹ˆæ˜¯ Functorï¼Ÿ

**Functor** æ˜¯é€šè¿‡é‡è½½ `operator()` æ“ä½œç¬¦æ¥ä½¿å¯¹è±¡å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨çš„ C++ ç±»æˆ–ç»“æ„ä½“ã€‚

### ç¤ºä¾‹è¯´æ˜

```cpp
// å®šä¹‰ä¸€ä¸ª Functor
struct AddFunctor {
    int value;
    AddFunctor(int v) : value(v) {}
    
    // é‡è½½ () æ“ä½œç¬¦ï¼Œä½¿å¯¹è±¡å¯åƒå‡½æ•°ä¸€æ ·è°ƒç”¨
    int operator() (int x) const {
        return x + value;
    }
};

// ä½¿ç”¨æ–¹å¼
AddFunctor add_5(5);  // åˆ›å»ºå‡½æ•°å¯¹è±¡ï¼Œvalue = 5
int result = add_5(10);  // è°ƒç”¨ operator(10)ï¼Œè¿”å› 15
// ç­‰ä»·äºï¼šresult = 10 + 5
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Functorï¼Ÿ

1. **æºå¸¦çŠ¶æ€**ï¼šå¯ä»¥å°†æ•°æ®å­˜å‚¨åœ¨å¯¹è±¡ä¸­ï¼ˆå¦‚ `FillFunctor` å­˜å‚¨å¡«å……å€¼ï¼‰
2. **æ¨¡æ¿å‹å¥½**ï¼šå¯ä»¥ç”¨æ¨¡æ¿å‚æ•°æŒ‡å®šç±»å‹ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨å¯ä»¥æ›´å¥½åœ°å†…è”ä¼˜åŒ–
4. **CUDA å…¼å®¹**ï¼šå¯ä»¥åœ¨ GPU ä¸Šæ‰§è¡Œï¼ˆä½¿ç”¨ `__device__` æ ‡è®°ï¼‰

### PyTorch ä¸­çš„ Functor æ¨¡å¼

PyTorch ä¸­çš„æ‰€æœ‰ CUDA ç®—å­éƒ½ä½¿ç”¨ Functor æ¨¡å¼ï¼š
- **FillFunctor**ï¼š`operator() ()` - æ— å‚æ•°ï¼Œè¿”å›å›ºå®šå€¼
- **AbsFunctor**ï¼š`operator() (scalar_t a)` - ä¸€ä¸ªå‚æ•°ï¼Œè¿”å›ç»å¯¹å€¼
- **MulFunctor**ï¼š`operator() (scalar_t a, scalar_t b)` - ä¸¤ä¸ªå‚æ•°ï¼Œè¿”å›ä¹˜ç§¯

è¿™ç§ç»Ÿä¸€çš„è®¾è®¡ä½¿å¾—æ‰€æœ‰ç®—å­éƒ½å¯ä»¥é€šè¿‡ç›¸åŒçš„æ¥å£ï¼ˆ`gpu_kernel`ï¼‰æ¥å¯åŠ¨ã€‚

---

## ğŸ—ï¸ CUDA ç®—å­çš„åŸºæœ¬æ¶æ„

PyTorch ä¸­çš„ CUDA ç®—å­å®ç°éµå¾ªä»¥ä¸‹å±‚æ¬¡ç»“æ„ï¼š

```
Python API è°ƒç”¨ (torch.fill, torch.abs ç­‰)
    â†“
ATen åˆ†å‘ç³»ç»Ÿ (æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹©å®ç°)
    â†“
CUDA ç®—å­å…¥å£å‡½æ•° (å¦‚ fill_kernel_cuda, abs_kernel_cuda)
    â†“
gpu_kernel/gpu_reduce_kernel (é€šç”¨å†…æ ¸å¯åŠ¨å‡½æ•°)
    â†“
å®é™… CUDA Kernel (åœ¨ GPU ä¸Šæ‰§è¡Œçš„ä»£ç )
```

### å…³é”®ç»„ä»¶

1. **TensorIterator**ï¼šç»Ÿä¸€å¤„ç†å¼ é‡è¿­ä»£çš„å·¥å…·ï¼Œè‡ªåŠ¨å¤„ç†å¹¿æ’­ã€å†…å­˜å¸ƒå±€ç­‰
2. **Loops.cuh / CUDALoops.cuh**ï¼šæä¾›é€šç”¨çš„å†…æ ¸å¯åŠ¨æ¡†æ¶
3. **Functor**ï¼šå°è£…å®é™…è®¡ç®—é€»è¾‘çš„å‡½æ•°å¯¹è±¡
4. **Dispatch ç³»ç»Ÿ**ï¼šæ ¹æ®æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©å®ç°

---

## 1ï¸âƒ£ æœ€ç®€å•çš„ç®—å­ï¼šFillï¼ˆå¡«å……ï¼‰

### 1.1 ç®—å­è¯´æ˜

**Fill** ç®—å­æ˜¯æœ€ç®€å•çš„ CUDA ç®—å­ä¹‹ä¸€ï¼Œå®ƒçš„åŠŸèƒ½æ˜¯ä½¿ç”¨æŒ‡å®šçš„æ ‡é‡å€¼å¡«å……æ•´ä¸ªå¼ é‡ã€‚

**åŠŸèƒ½**ï¼š`out[i] = value` å¯¹æ‰€æœ‰ `i`

**ç‰¹ç‚¹**ï¼š
- å•è¾“å‡ºï¼Œæ— è¾“å…¥å¼ é‡ï¼ˆåªæœ‰æ ‡é‡å‚æ•°ï¼‰
- æ¯ä¸ªè¾“å‡ºå…ƒç´ çš„è®¡ç®—ç›¸äº’ç‹¬ç«‹
- å†…å­˜è®¿é—®æ¨¡å¼ç®€å•ï¼ˆåªå†™è¾“å‡ºï¼‰

### 1.2 æºç å®ç°

#### å…¥å£å‡½æ•°ï¼š`fill_kernel_cuda`

```1:30:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/FillKernel.cu
#define TORCH_ASSERT_NO_OPERATORS
#include <ATen/Dispatch.h>
#include <ATen/Dispatch_v2.h>
#include <ATen/native/cuda/Loops.cuh>
#include <ATen/native/DispatchStub.h>
#include <ATen/native/TensorIterator.h>
#include <ATen/native/Fill.h>
#include <c10/core/Scalar.h>

namespace at::native {

template<typename scalar_t>
struct FillFunctor {
  FillFunctor(scalar_t v): value(v) {}
  __device__ __forceinline__ scalar_t operator() () const {
    return value;
  }
  private:
    scalar_t value;
};

void fill_kernel_cuda(TensorIterator& iter, const Scalar& value) {
  AT_DISPATCH_V2(iter.dtype(), "fill_cuda", AT_WRAP([&]() {
    gpu_kernel(iter, FillFunctor<scalar_t>(value.to<scalar_t>()));
  }), AT_EXPAND(AT_ALL_TYPES_AND_COMPLEX), kComplexHalf, kBool, kHalf, kBFloat16, AT_EXPAND(AT_FLOAT8_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES));
}

REGISTER_DISPATCH(fill_stub, &fill_kernel_cuda)

} // namespace at::native
```

#### ä»£ç è§£æ

1. **FillFunctor ç»“æ„ä½“**ï¼š
   - è¿™æ˜¯ä¸€ä¸ª**å‡½æ•°å¯¹è±¡ï¼ˆFunctorï¼‰**ï¼Œç”¨äºå°è£…å¡«å……æ“ä½œ
   - æ„é€ å‡½æ•°æ¥æ”¶å¡«å……å€¼ `value` å¹¶å­˜å‚¨åœ¨æˆå‘˜å˜é‡ä¸­
   - **`operator() ()` é‡è½½**ï¼šè¿™æ˜¯å‡½æ•°è°ƒç”¨æ“ä½œç¬¦çš„é‡è½½ï¼Œä½¿å¾— `FillFunctor` å¯¹è±¡å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨
     ```cpp
     FillFunctor<float> functor(5.0f);
     float result = functor();  // è°ƒç”¨ operator()ï¼Œè¿”å› 5.0f
     ```
   - `const` å…³é”®å­—è¡¨ç¤ºè¯¥å‡½æ•°ä¸ä¼šä¿®æ”¹å¯¹è±¡çŠ¶æ€
   - `__device__` è¡¨ç¤ºè¯¥å‡½æ•°å¯ä»¥åœ¨ GPU ä¸Šæ‰§è¡Œ
   - `__forceinline__` æç¤ºç¼–è¯‘å™¨å°½å¯èƒ½å†…è”è¯¥å‡½æ•°
   - è¿™æ˜¯ CUDA kernel ä¸­å®é™…è°ƒç”¨çš„å‡½æ•°ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½ä¼šè°ƒç”¨å®ƒæ¥è·å–å¡«å……å€¼

2. **`AT_DISPATCH_V2` å®**ï¼š
   - æ ¹æ®å¼ é‡çš„æ•°æ®ç±»å‹è‡ªåŠ¨åˆ†å‘åˆ°å¯¹åº”çš„æ¨¡æ¿å®ä¾‹åŒ–
   - æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ˆfloat32, int64, complex ç­‰ï¼‰

3. **`gpu_kernel` å‡½æ•°**ï¼š
   - é€šç”¨çš„ CUDA å†…æ ¸å¯åŠ¨å‡½æ•°
   - è‡ªåŠ¨å¤„ç†å†…å­˜è®¿é—®ã€çº¿ç¨‹åˆ†é…ç­‰ç»†èŠ‚
   - æ¥æ”¶ `TensorIterator` å’Œ `Functor`

4. **`REGISTER_DISPATCH`**ï¼š
   - æ³¨å†Œç®—å­åˆ°åˆ†å‘ç³»ç»Ÿï¼Œä½¿å…¶èƒ½è¢«è°ƒç”¨

---

## 2ï¸âƒ£ ä¸€å…ƒç®—å­ï¼šAbsï¼ˆç»å¯¹å€¼ï¼‰

### 2.1 ç®—å­è¯´æ˜

**Abs** ç®—å­è®¡ç®—è¾“å…¥å¼ é‡ä¸­æ¯ä¸ªå…ƒç´ çš„ç»å¯¹å€¼ã€‚

**åŠŸèƒ½**ï¼š`out[i] = |in[i]|`

**ç‰¹ç‚¹**ï¼š
- å•è¾“å…¥å•è¾“å‡º
- å…ƒç´ çº§æ“ä½œï¼ˆelement-wiseï¼‰ï¼Œæ¯ä¸ªå…ƒç´ ç‹¬ç«‹è®¡ç®—
- æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬å¤æ•°ï¼ˆå¤æ•°çš„ç»å¯¹å€¼æ˜¯å…¶æ¨¡é•¿ï¼‰

### 2.2 æºç å®ç°

#### å…¥å£å‡½æ•°ï¼š`abs_kernel_cuda`

```1:51:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/AbsKernel.cu
#define TORCH_ASSERT_NO_OPERATORS
#include <ATen/native/UnaryOps.h>
#include <ATen/native/cuda/Loops.cuh>
#include <ATen/native/cuda/JitLoops.cuh>
#include <ATen/Dispatch.h>
#include <ATen/native/DispatchStub.h>
#include <ATen/native/TensorIterator.h>

namespace at::native {

template<typename scalar_t>
struct AbsFunctor {
  __device__ __forceinline__ scalar_t operator() (const scalar_t a) const {
    return std::abs(a);
  }
};

constexpr char abs_name[] = "abs_kernel";
void abs_kernel_cuda(TensorIteratorBase& iter) {
  auto dtype = iter.dtype();
  if (at::isComplexType(dtype)) {
#if AT_USE_JITERATOR()
    static const auto abs_string = jiterator_stringify(
        template <typename T> T abs_kernel(T x) { return std::abs(x); });
    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
      jitted_gpu_kernel<
          /*name=*/abs_name,
          /*return_dtype=*/scalar_t,
          /*common_dtype=*/scalar_t,
          /*arity=*/1>(iter, abs_string);
    });
#else
    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
      using opmath_t = at::opmath_type<scalar_t>;
      gpu_kernel(iter, AbsFunctor<opmath_t>());
    });
#endif
  } else {
    AT_DISPATCH_ALL_TYPES_AND3(
        ScalarType::Half,
        ScalarType::BFloat16,
        ScalarType::Bool,
        iter.dtype(),
        "abs_cuda",
        [&]() { gpu_kernel(iter, AbsFunctor<scalar_t>()); });
  }
}

  REGISTER_DISPATCH(abs_stub, &abs_kernel_cuda)

} // namespace at::native
```

#### ä»£ç è§£æ

1. **AbsFunctor ç»“æ„ä½“**ï¼š
   - `operator()` æ¥æ”¶ä¸€ä¸ªå‚æ•° `a`ï¼ˆè¾“å…¥å…ƒç´ ï¼‰
   - ä½¿ç”¨ `std::abs()` è®¡ç®—ç»å¯¹å€¼
   - `__device__ __forceinline__` è¡¨ç¤ºè¿™æ˜¯è®¾å¤‡ç«¯å‡½æ•°ï¼Œç¼–è¯‘å™¨ä¼šå°è¯•å†…è”

2. **å¤æ•°ç±»å‹ç‰¹æ®Šå¤„ç†**ï¼š
   - å¤æ•°ç±»å‹å¯ä»¥ä½¿ç”¨ JIT ç¼–è¯‘ï¼ˆJIteratorï¼‰æ¥æé«˜æ€§èƒ½
   - JIterator æ˜¯è¿è¡Œæ—¶ç¼–è¯‘ç³»ç»Ÿï¼Œå¯ä»¥å‡å°‘äºŒè¿›åˆ¶å¤§å°

3. **æ•°æ®ç±»å‹åˆ†å‘**ï¼š
   - å¯¹å¤æ•°ç±»å‹ä½¿ç”¨ `AT_DISPATCH_COMPLEX_TYPES_AND`
   - å¯¹å®æ•°ç±»å‹ä½¿ç”¨ `AT_DISPATCH_ALL_TYPES_AND3`

### 2.3 åº•å±‚å®ç°ï¼šgpu_kernel å¦‚ä½•å·¥ä½œ

è™½ç„¶æˆ‘ä»¬åœ¨å®ç°ä¸­åªå†™äº† Functorï¼Œä½† `gpu_kernel` å‡½æ•°ä¼šå¤„ç†æ‰€æœ‰çš„åº•å±‚ç»†èŠ‚ï¼š

```105:126:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Loops.cuh
template <typename func_t>
void gpu_kernel(TensorIteratorBase& iter, const func_t& f) {

  for (int arg = 0; arg < iter.ntensors(); arg++) {
    TORCH_INTERNAL_ASSERT(
      iter.device(arg).is_cuda(),
      "argument ", arg, ": expected a CUDA device but found ", iter.device(arg));
  }

  if (iter.numel() == 0) {
    return;
  }

  if (!iter.can_use_32bit_indexing()) {
    for (auto& sub_iter : iter.with_32bit_indexing()) {
      gpu_kernel(sub_iter, f);
    }
    return;
  }

  gpu_kernel_impl(iter, f);
}
```

**å…³é”®æ­¥éª¤**ï¼š
1. **è®¾å¤‡æ£€æŸ¥**ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨ CUDA è®¾å¤‡ä¸Š
2. **ç©ºå¼ é‡å¤„ç†**ï¼šå¦‚æœå…ƒç´ æ•°ä¸º 0ï¼Œç›´æ¥è¿”å›
3. **32 ä½ç´¢å¼•é™åˆ¶**ï¼šå¦‚æœå¼ é‡å¤ªå¤§æ— æ³•ç”¨ 32 ä½ç´¢å¼•ï¼Œéœ€è¦åˆ†å—å¤„ç†
4. **å®é™…å†…æ ¸å¯åŠ¨**ï¼šè°ƒç”¨ `gpu_kernel_impl` å¯åŠ¨ CUDA kernel

---

## 3ï¸âƒ£ äºŒå…ƒç®—å­ï¼šMulï¼ˆä¹˜æ³•ï¼‰

### 3.1 ç®—å­è¯´æ˜

**Mul** ç®—å­è®¡ç®—ä¸¤ä¸ªè¾“å…¥å¼ é‡çš„é€å…ƒç´ ä¹˜ç§¯ã€‚

**åŠŸèƒ½**ï¼š`out[i] = a[i] * b[i]`

**ç‰¹ç‚¹**ï¼š
- åŒè¾“å…¥å•è¾“å‡º
- æ”¯æŒæ ‡é‡å¹¿æ’­ï¼ˆå¦‚ `tensor * 5`ï¼‰
- æ”¯æŒä¸åŒç±»å‹ä¹‹é—´çš„è¿ç®—ï¼ˆå¦‚ `float32 * float64`ï¼‰
- å¯¹äºå¤æ•°ï¼Œè®¡ç®—å¤æ•°çš„ä¹˜æ³•

### 3.2 æºç å®ç°

#### å…¥å£å‡½æ•°ï¼š`mul_kernel_cuda`

```1:48:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/BinaryMulKernel.cu
#define TORCH_ASSERT_NO_OPERATORS
#include <ATen/AccumulateType.h>
#include <ATen/Dispatch.h>
#include <ATen/native/BinaryOps.h>
#include <ATen/native/DispatchStub.h>
#include <ATen/native/TensorIterator.h>
#include <ATen/native/cuda/BinaryInternal.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/cuda/CUDAMathCompat.h>
#include <c10/util/TypeSafeSignMath.h>
#include <ATen/native/cuda/JitLoops.cuh>
#include <ATen/native/cuda/Loops.cuh>

#include <type_traits>

// NOTE: CUDA on Windows requires that the enclosing function
// of a __device__ lambda not have internal linkage.

namespace at::native {

constexpr char mul_name[] = "mul_kernel";
void mul_kernel_cuda(TensorIteratorBase& iter) {
  auto common_dtype = iter.common_dtype();
  if (common_dtype == kComplexHalf) {
    using scalar_t = c10::complex<at::Half>;
#if AT_USE_JITERATOR()
    static const auto mul_string = jiterator_stringify(
        template <typename T> T mul_kernel(T a, T b) { return a * b; });
    opmath_jitted_gpu_kernel_with_scalars<mul_name, scalar_t, scalar_t>(
        iter, mul_string);
#else
    using opmath_t = at::opmath_type<scalar_t>;
    opmath_symmetric_gpu_kernel_with_scalars<scalar_t>(
        iter, binary_internal::MulFunctor<opmath_t>());
#endif
  } else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
        kHalf, kBFloat16, kBool, iter.common_dtype(), "mul_cuda", [&]() {
          using opmath_t = at::opmath_type<scalar_t>;
          opmath_symmetric_gpu_kernel_with_scalars<scalar_t>(
              iter, binary_internal::MulFunctor<opmath_t>());
        });
  }
}

REGISTER_DISPATCH(mul_stub, &mul_kernel_cuda)

} // namespace at::native
```

#### ä»£ç è§£æ

1. **`opmath_symmetric_gpu_kernel_with_scalars`**ï¼š
   - è¿™æ˜¯ä¸“é—¨å¤„ç†äºŒå…ƒæ“ä½œçš„å‡½æ•°
   - **symmetric** è¡¨ç¤ºæ“ä½œæ˜¯å¯¹ç§°çš„ï¼ˆ`a * b == b * a`ï¼‰ï¼Œå¯ä»¥ä¼˜åŒ–æ ‡é‡å‚æ•°çš„ä½ç½®
   - **with_scalars** è¡¨ç¤ºæ”¯æŒå…¶ä¸­ä¸€ä¸ªå‚æ•°æ˜¯æ ‡é‡ï¼ˆCPU ä¸Šçš„æ ‡é‡ï¼‰

2. **`common_dtype`**ï¼š
   - äºŒå…ƒæ“ä½œéœ€è¦å¤„ç†ä¸¤ä¸ªè¾“å…¥å¯èƒ½ç±»å‹ä¸åŒçš„æƒ…å†µ
   - `common_dtype` æ˜¯ä¸¤ä¸ªè¾“å…¥ç±»å‹æå‡åçš„å…±åŒç±»å‹

3. **`opmath_type`**ï¼š
   - ä¸ºäº†æé«˜æ•°å€¼ç²¾åº¦ï¼ŒPyTorch ä½¿ç”¨æ“ä½œæ•°å­¦ç±»å‹
   - ä¾‹å¦‚ï¼Œ`float16` çš„è®¡ç®—å¯èƒ½ä½¿ç”¨ `float32` è¿›è¡Œï¼Œæœ€åå†è½¬æ¢å› `float16`

#### MulFunctor å®ç°ï¼ˆåœ¨ BinaryInternal.h ä¸­ï¼‰

è™½ç„¶æºç ä¸­æ²¡æœ‰ç›´æ¥æ˜¾ç¤ºï¼Œä½† `binary_internal::MulFunctor` å¤§è‡´å¦‚ä¸‹ï¼š

```cpp
template<typename scalar_t>
struct MulFunctor {
  __device__ __forceinline__ scalar_t operator() (const scalar_t a, const scalar_t b) const {
    return a * b;
  }
};
```

#### æ ‡é‡å¤„ç†

`opmath_symmetric_gpu_kernel_with_scalars` å†…éƒ¨ä¼šæ£€æŸ¥æ˜¯å¦æœ‰æ ‡é‡å‚æ•°ï¼š

```203:241:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Loops.cuh
template <typename scalar_t, typename return_t = scalar_t, typename func_t>
void opmath_symmetric_gpu_kernel_with_scalars(TensorIteratorBase& iter, const func_t& f) {
  // Use symmetric property of the functor to reduce number of kernels,
  // requires f(a, b) == f(b, a)
  TORCH_INTERNAL_ASSERT(iter.ntensors() == 3);

  using traits = function_traits<func_t>;
  using opmath_arg_t = typename traits::template arg<0>::type;
  static_assert(
      traits::arity == 2,
      "gpu_kernel_with_scalars only supports two input arguments");
  static_assert(std::is_same_v<opmath_arg_t, typename traits::template arg<1>::type>,
                "f is not symmetric");

  OptionalDeviceGuard device_guard;
  opmath_arg_t scalar_val{};

  if (iter.is_cpu_scalar(1)) {
    scalar_val = iter.scalar_value<opmath_arg_t>(1);
    iter.remove_operand(1);

    // TODO: When all kernels that use gpu_kernel_with_scalars are
    // ported to structured, this device guard can be deleted.  This
    // works around incorrect device guard generation for pre-structured
    // kernels device guards, but structured kernels do it right and
    // we can assume the device is already set correctly
    device_guard.reset_device(iter.device(1));
  } else if (iter.is_cpu_scalar(2)) {
    scalar_val = iter.scalar_value<opmath_arg_t>(2);
    iter.remove_operand(2);
  }

  if (iter.ninputs() == 2) {
    gpu_kernel(iter, BinaryFunctor<scalar_t, scalar_t, return_t, func_t>(f));
  } else {
    AUnaryFunctor<scalar_t, scalar_t, return_t, func_t> unary_f(f, scalar_val);
    gpu_kernel(iter, unary_f);
  }
}
```

**å…³é”®ç‚¹**ï¼š
- å¦‚æœæ£€æµ‹åˆ°æ ‡é‡å‚æ•°ï¼Œå°†å…¶æå–å‡ºæ¥
- å°†äºŒå…ƒ Functor è½¬æ¢ä¸ºä¸€å…ƒ Functorï¼ˆå…¶ä¸­ä¸€ä¸ªå‚æ•°å·²å›ºå®šä¸ºæ ‡é‡å€¼ï¼‰
- è¿™æ ·å¯ä»¥é¿å…åœ¨ GPU å†…å­˜ä¸­å­˜å‚¨æ ‡é‡ï¼Œæé«˜æ€§èƒ½

---

## 4ï¸âƒ£ å½’çº¦ç®—å­ï¼šSumï¼ˆæ±‚å’Œï¼‰

### 4.1 ç®—å­è¯´æ˜

**Sum** ç®—å­æ˜¯ä¸€ä¸ªå½’çº¦æ“ä½œï¼ˆreductionï¼‰ï¼Œå®ƒå°†è¾“å…¥å¼ é‡çš„å¤šä¸ªå…ƒç´ å½’çº¦ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªæ ‡é‡å€¼ã€‚

**åŠŸèƒ½**ï¼š`out = sum(in[i])` å¯¹æ‰€æœ‰ `i`

**ç‰¹ç‚¹**ï¼š
- **å½’çº¦æ“ä½œ**ï¼šå¤šä¸ªè¾“å…¥å…ƒç´ æ˜ å°„åˆ°ä¸€ä¸ªè¾“å‡ºå…ƒç´ 
- **éœ€è¦åŒæ­¥**ï¼šä¸åŒçº¿ç¨‹å¤„ç†çš„æ•°æ®éœ€è¦åˆå¹¶ç»“æœ
- **ä½¿ç”¨å…±äº«å†…å­˜**ï¼šåœ¨ block å†…éƒ¨è¿›è¡Œéƒ¨åˆ†å½’çº¦
- **å¯èƒ½éœ€è¦å¤šçº§å½’çº¦**ï¼šå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œéœ€è¦å¤šä¸ª kernel launch

### 4.2 æºç å®ç°

#### å…¥å£å‡½æ•°ï¼š`sum_functor`

```13:30:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
template <typename scalar_t, typename acc_t = scalar_t, typename out_t = scalar_t>
struct sum_functor {
  void operator()(TensorIterator& iter) {
    const auto sum_combine = [] GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {
      return a + b;
    };
    constexpr bool is_16_bits = sizeof(scalar_t) == 2;
    if constexpr (is_16_bits) {
      gpu_reduce_kernel<scalar_t, out_t, /*vt0=*/4, /*input_vec_size=*/8>(
        iter, func_wrapper<out_t>(sum_combine)
      );
    } else {
      gpu_reduce_kernel<scalar_t, out_t>(
        iter, func_wrapper<out_t>(sum_combine)
      );
    }
  }
};
```

#### ä»£ç è§£æ

1. **`sum_combine` å‡½æ•°**ï¼š
   - è¿™æ˜¯å½’çº¦æ“ä½œçš„åˆå¹¶å‡½æ•°
   - `GPU_LAMBDA` å®ç¡®ä¿è¿™ä¸ª lambda å¯ä»¥åœ¨ GPU ä¸Šæ‰§è¡Œ
   - å‡½æ•°å®šä¹‰äº†å¦‚ä½•åˆå¹¶ä¸¤ä¸ªå€¼ï¼š`a + b`

2. **`gpu_reduce_kernel`**ï¼š
   - è¿™æ˜¯å½’çº¦æ“ä½œçš„é€šç”¨å†…æ ¸å¯åŠ¨å‡½æ•°
   - ä¸ `gpu_kernel` ä¸åŒï¼Œå®ƒéœ€è¦å¤„ç†çº¿ç¨‹é—´çš„æ•°æ®åˆå¹¶
   - **æ¨¡æ¿å‚æ•°**ï¼š
     - `scalar_t`ï¼šè¾“å…¥å…ƒç´ ç±»å‹
     - `out_t`ï¼šè¾“å‡ºå…ƒç´ ç±»å‹
     - `vt0`ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„å…ƒç´ æ•°é‡ï¼ˆå‘é‡åŒ–å‚æ•°ï¼‰
     - `input_vec_size`ï¼šå‘é‡åŒ–åŠ è½½çš„å…ƒç´ æ•°é‡

### 4.2.1 `gpu_reduce_kernel` æ ¸å¿ƒä»£ç åˆ†æ

#### å…¥å£å‡½æ•°ç­¾å

```1222:1224:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Reduce.cuh
template <typename scalar_t, typename out_scalar_t, int vt0=4, int input_vec_size=vt0, typename ops_t, typename ident_t=double>
inline void gpu_reduce_kernel(TensorIterator& iter, const ops_t& ops, ident_t ident=0,
                              AccumulationBuffer* acc_buf_ptr=nullptr, int64_t base_idx=0) {
```

**å…³é”®å‚æ•°è¯´æ˜**ï¼š
- `iter`: TensorIteratorï¼ŒåŒ…å«è¾“å…¥è¾“å‡ºå¼ é‡ä¿¡æ¯
- `ops`: å½’çº¦æ“ä½œå‡½æ•°å¯¹è±¡ï¼ˆå¦‚ sum çš„ `combine` å‡½æ•°ï¼‰
- `ident`: å½’çº¦çš„å•ä½å…ƒï¼ˆsum ä¸º 0ï¼Œprod ä¸º 1ï¼‰
- `vt0`: å‘é‡åŒ–å‚æ•°ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„å…ƒç´ æ•°

#### æ ¸å¿ƒæ‰§è¡Œæµç¨‹

```1280:1323:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Reduce.cuh
  const char* in_data = (char*)iter.data_ptr(iter.ntensors() - 1);
  char* out_data = (char*)iter.data_ptr(0);
  const auto noutputs = iter.noutputs();
  std::optional<char*> out_data_extra;
  if (noutputs > 1) {
    out_data_extra = (char*)iter.data_ptr(1);
  } else {
    out_data_extra = std::nullopt;
  }
  char* acc_data = acc_buf_ptr->get_acc_slice(out_data);

  ReduceConfig config = setReduceConfig<arg_t, scalar_t, vt0, input_vec_size>(iter);
  at::DataPtr buffer;
  at::DataPtr semaphores;
  if (config.should_global_reduce()) {
    auto& allocator = *c10::cuda::CUDACachingAllocator::get();
    buffer = allocator.allocate(config.global_memory_size());
    semaphores = allocator.allocate(config.semaphore_size());

    auto stream = at::cuda::getCurrentCUDAStream();
    AT_CUDA_CHECK(cudaMemsetAsync(semaphores.get(), 0, config.semaphore_size(), stream));
  }

  AT_ASSERT(can_use_32bit_indexing);
  auto output_calc = make_output_calculator<uint32_t>(iter);
  auto input_calc = make_input_calculator<uint32_t>(iter);
  auto reduce = ReduceOp<scalar_t, ops_t, uint32_t, out_scalar_t, vt0, input_vec_size>(
      ops,
      config,
      input_calc,
      output_calc,
      in_data,
      out_data,
      out_data_extra,
      acc_data,
      buffer.get(),
      (int*)semaphores.get(),
      ident,
      noutputs,
      base_idx);
  reduce.accumulate = iter.should_accumulate();
  reduce.final_output = iter.is_final_output();

  launch_reduce_kernel<mnt_wrapper<scalar_t>::MAX_NUM_THREADS>(config, reduce);
```

**å…³é”®æ­¥éª¤è§£æ**ï¼š

1. **è·å–æ•°æ®æŒ‡é’ˆ**ï¼š
   ```cpp
   const char* in_data = (char*)iter.data_ptr(iter.ntensors() - 1);  // è¾“å…¥æ•°æ®
   char* out_data = (char*)iter.data_ptr(0);  // è¾“å‡ºæ•°æ®
   ```
   - è¾“å…¥åœ¨æœ€åä¸€ä¸ªå¼ é‡ä½ç½®
   - è¾“å‡ºåœ¨ç¬¬ä¸€ä¸ªå¼ é‡ä½ç½®

2. **é…ç½®å½’çº¦å‚æ•°**ï¼ˆ`ReduceConfig`ï¼‰ï¼š
   - è®¡ç®— block å’Œ grid å¤§å°
   - ç¡®å®šæ˜¯å¦éœ€è¦å…¨å±€å½’çº¦ï¼ˆå¤š blockï¼‰
   - è®¾ç½®å…±äº«å†…å­˜å¤§å°

3. **åˆ›å»ºå½’çº¦æ“ä½œå¯¹è±¡**ï¼ˆ`ReduceOp`ï¼‰ï¼š
   - å°è£…æ‰€æœ‰å½’çº¦æ‰€éœ€çš„ä¿¡æ¯
   - åŒ…å«æ“ä½œå‡½æ•°ã€é…ç½®ã€å†…å­˜æŒ‡é’ˆç­‰

4. **å¯åŠ¨å†…æ ¸**ï¼š
   ```cpp
   launch_reduce_kernel<mnt_wrapper<scalar_t>::MAX_NUM_THREADS>(config, reduce);
   ```

#### ReduceOp::run() - æ ¸å¿ƒå½’çº¦æ‰§è¡Œé€»è¾‘

```401:477:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Reduce.cuh
  template <int output_vec_size>
  C10_DEVICE void run() const {
    extern __shared__ char shared_memory[];
    index_t output_idx = config.output_idx<output_vec_size>();
    index_t input_idx = config.input_idx();
    auto base_offsets1 = output_calc.get(output_idx)[1];

    using arg_vec_t = std::array<arg_t, output_vec_size>;
    arg_vec_t value;

    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {
      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);
      value = thread_reduce<output_vec_size>(input_slice);
    }

    if (config.should_block_x_reduce()) {
      value = block_x_reduce<output_vec_size>(value, shared_memory);
    }
    if (config.should_block_y_reduce()) {
      value = block_y_reduce<output_vec_size>(value, shared_memory);
    }
    using out_ptr_vec_t = std::array<out_scalar_t*, output_vec_size>;
    using offset_vec_t = std::array<index_t, output_vec_size>;
    offset_vec_t base_offsets;
    out_ptr_vec_t out;

    #pragma unroll
    for (int i = 0; i < output_vec_size; i++) {
      base_offsets[i] = output_calc.get(output_idx + i)[0];
      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);
    }

    arg_vec_t* acc = nullptr;
    if (acc_buf != nullptr) {
      size_t numerator = sizeof(arg_t);
      size_t denominator = sizeof(out_scalar_t);
      reduce_fraction(numerator, denominator);
      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));
    }

    if (config.should_global_reduce()) {
      value = global_reduce<output_vec_size>(value, acc, shared_memory);
    } else if (config.should_store(output_idx)) {
      if (accumulate) {
        #pragma unroll
        for (int i = 0; i < output_vec_size; i++) {
          value[i] = ops.translate_idx(value[i], base_idx);
        }
      }

      if (acc == nullptr) {
        if (accumulate) {
          value = accumulate_in_output<output_vec_size, can_accumulate_in_output>(out, value);
        }
        if (final_output) {
          set_results_to_output<output_vec_size>(value, base_offsets);
        } else {
          #pragma unroll
          for (int i = 0; i < output_vec_size; i++) {
            *(out[i]) = get_accumulated_output<can_accumulate_in_output>(out[i], value[i]);
          }
        }
      } else {
        if (accumulate) {
          #pragma unroll
          for (int i = 0; i < output_vec_size; i++) {
            value[i] = ops.combine((*acc)[i], value[i]);
          }
        }
        if (final_output) {
          set_results_to_output<output_vec_size>(value, base_offsets);
        } else {
          *acc = value;
        }
      }
    }
  }
```

**æ‰§è¡Œæµç¨‹è§£æ**ï¼š

1. **çº¿ç¨‹çº§å½’çº¦**ï¼ˆ`thread_reduce`ï¼‰ï¼š
   ```cpp
   value = thread_reduce<output_vec_size>(input_slice);
   ```
   - æ¯ä¸ªçº¿ç¨‹åŠ è½½å¹¶å½’çº¦è‡ªå·±è´Ÿè´£çš„è¾“å…¥å…ƒç´ 
   - ä½¿ç”¨å‘é‡åŒ–åŠ è½½æé«˜å¸¦å®½åˆ©ç”¨ç‡

2. **Block å†…å½’çº¦**ï¼ˆ`block_x_reduce` / `block_y_reduce`ï¼‰ï¼š
   ```cpp
   if (config.should_block_x_reduce()) {
     value = block_x_reduce<output_vec_size>(value, shared_memory);
   }
   if (config.should_block_y_reduce()) {
     value = block_y_reduce<output_vec_size>(value, shared_memory);
   }
   ```
   - ä½¿ç”¨å…±äº«å†…å­˜åœ¨ block å†…åˆå¹¶æ‰€æœ‰çº¿ç¨‹çš„ç»“æœ
   - X å’Œ Y ç»´åº¦åˆ†åˆ«å½’çº¦ï¼ˆæ”¯æŒå¤šç»´å½’çº¦ï¼‰

3. **å…¨å±€å½’çº¦**ï¼ˆå¦‚éœ€è¦ï¼Œ`global_reduce`ï¼‰ï¼š
   ```cpp
   if (config.should_global_reduce()) {
     value = global_reduce<output_vec_size>(value, acc, shared_memory);
   }
   ```
   - å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œéœ€è¦å¤šä¸ª block
   - ä½¿ç”¨å…¨å±€å†…å­˜å’ŒåŸå­æ“ä½œåˆå¹¶å¤šä¸ª block çš„ç»“æœ

4. **å†™å›ç»“æœ**ï¼ˆ`set_results_to_output`ï¼‰ï¼š
   - å°†æœ€ç»ˆç»“æœå†™å›è¾“å‡ºå¼ é‡
   - æ”¯æŒç´¯ç§¯æ¨¡å¼ï¼ˆaccumulateï¼‰å’Œæœ€ç»ˆè¾“å‡ºæ¨¡å¼

#### çº¿ç¨‹çº§å½’çº¦æ ¸å¿ƒä»£ç 

```479:558:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Reduce.cuh
  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {
    index_t end = config.num_inputs;

    // Handle the head of input slice where data is not aligned
    arg_t value = ident;
    constexpr int align_bytes = alignof(at::native::memory::aligned_vector<scalar_t, input_vec_size>);
    constexpr int align_elements = align_bytes / sizeof(scalar_t);
    int shift = ((uint64_t)data) % align_bytes / sizeof(scalar_t);
    if (shift > 0) {
      data -= shift;
      end += shift;
      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){
        value = ops.reduce(value, c10::load(data + threadIdx.x), threadIdx.x - shift);
      }
      end -= align_elements;
      data += align_elements;
      shift = align_elements - shift;
    }

    // Do the vectorized reduction
    using load_t = at::native::memory::aligned_vector<scalar_t, input_vec_size>;

    index_t idx = config.input_idx();
    const index_t stride = config.step_input;

    // Multiple accumulators to remove dependency between unrolled loops.
    arg_t value_list[input_vec_size];
    value_list[0] = value;

    #pragma unroll
    for (int i = 1; i < input_vec_size; i++) {
      value_list[i] = ident;
    }

    while (idx * input_vec_size + input_vec_size - 1 < end) {
      const auto values_vec = memory::load_vector<input_vec_size>(data, idx);
      #pragma unroll
      for (index_t i = 0; i < input_vec_size; i++) {
        value_list[i] = ops.reduce(value_list[i], values_vec.val[i], shift + idx * input_vec_size + i);
      }
      idx += stride;
    }

    // tail
    index_t tail_start = end - end % input_vec_size;
    if (config.should_reduce_tail()) {
      int idx = tail_start + threadIdx.x;
      if (idx < end) {
        const auto value = c10::load(data + idx);
        value_list[0] = ops.reduce(value_list[0], value, idx + shift);
      }
    }

    // combine accumulators
    #pragma unroll
    for (int i = 1; i < input_vec_size; i++) {
      value_list[0] = ops.combine(value_list[0], value_list[i]);
    }
    return value_list[0];
  }
```

**å…³é”®ä¼˜åŒ–æŠ€æœ¯**ï¼š

1. **å†…å­˜å¯¹é½å¤„ç†**ï¼š
   - æ£€æŸ¥æ•°æ®æ˜¯å¦å¯¹é½åˆ°å‘é‡åŒ–è¾¹ç•Œ
   - ä¸å¯¹é½æ—¶å…ˆå¤„ç†å¤´éƒ¨æ•°æ®

2. **å‘é‡åŒ–åŠ è½½**ï¼š
   ```cpp
   const auto values_vec = memory::load_vector<input_vec_size>(data, idx);
   ```
   - ä¸€æ¬¡åŠ è½½å¤šä¸ªå…ƒç´ ï¼ˆå¦‚ `input_vec_size=8` è¡¨ç¤ºä¸€æ¬¡åŠ è½½ 8 ä¸ª floatï¼‰
   - æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

3. **å¤šä¸ªç´¯åŠ å™¨**ï¼š
   ```cpp
   arg_t value_list[input_vec_size];  // å¤šä¸ªç´¯åŠ å™¨
   ```
   - å‡å°‘å¾ªç¯é—´çš„æ•°æ®ä¾èµ–
   - å…è®¸ç¼–è¯‘å™¨æ›´å¥½åœ°å¹¶è¡ŒåŒ–å¾ªç¯

4. **å¤„ç†å°¾éƒ¨æ•°æ®**ï¼š
   - å‘é‡åŒ–å¤„ç†å®Œæ•´éƒ¨åˆ†
   - å•ç‹¬å¤„ç†å‰©ä½™çš„ä¸å®Œæ•´å‘é‡

#### å†…æ ¸å¯åŠ¨ä»£ç 

```912:933:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/Reduce.cuh
template<int max_threads, typename R>
static void launch_reduce_kernel(const ReduceConfig& config, const R& reduction) {
  dim3 block = config.block();
  dim3 grid = config.grid();

  auto stream = at::cuda::getCurrentCUDAStream();
  int shared_memory = config.shared_memory_size();

  switch(config.output_vec_size) {
  case 4:
    reduce_kernel<max_threads / 4, 4, R><<<grid, block, shared_memory, stream>>>(reduction);
    C10_CUDA_KERNEL_LAUNCH_CHECK();
    break;
  case 2:
    reduce_kernel<max_threads / 2, 2, R><<<grid, block, shared_memory, stream>>>(reduction);
    C10_CUDA_KERNEL_LAUNCH_CHECK();
    break;
  default:
    reduce_kernel<max_threads / 1, 1, R><<<grid, block, shared_memory, stream>>>(reduction);
    C10_CUDA_KERNEL_LAUNCH_CHECK();
  }
}
```

**å…³é”®ç‚¹**ï¼š
- **output_vec_size**ï¼šè¾“å‡ºå‘é‡åŒ–å¤§å°ï¼ˆ1ã€2ã€4ï¼‰
- **å…±äº«å†…å­˜**ï¼šåŠ¨æ€åˆ†é…ç”¨äº block å†…å½’çº¦
- **æ¨¡æ¿å®ä¾‹åŒ–**ï¼šæ ¹æ®å‘é‡åŒ–å¤§å°é€‰æ‹©ä¸åŒçš„å†…æ ¸ç‰ˆæœ¬

### 4.2.2 å½’çº¦æ“ä½œçš„å®Œæ•´æµç¨‹å›¾

```
è¾“å…¥å¼ é‡ [N ä¸ªå…ƒç´ ]
    â†“
[çº¿ç¨‹çº§å½’çº¦]
æ¯ä¸ªçº¿ç¨‹: åŠ è½½å¤šä¸ªå…ƒç´  â†’ å‘é‡åŒ–å½’çº¦ â†’ å±€éƒ¨ç´¯åŠ å™¨
    â†“
[Block å†…å½’çº¦] (å…±äº«å†…å­˜)
Block 0: [thread0, thread1, ...] â†’ tree reduction â†’ ç»“æœ0
Block 1: [thread0, thread1, ...] â†’ tree reduction â†’ ç»“æœ1
...
    â†“
[å…¨å±€å½’çº¦] (å¦‚æœéœ€è¦)
å¤šä¸ª Block çš„ç»“æœ â†’ åŸå­æ“ä½œ/å…¨å±€å†…å­˜ â†’ æœ€ç»ˆç»“æœ
    â†“
è¾“å‡ºå¼ é‡ [1 ä¸ªæˆ–å°‘é‡å…ƒç´ ]
```

### 4.2.3 å…³é”®æ€§èƒ½ä¼˜åŒ–

1. **å‘é‡åŒ–åŠ è½½**ï¼šä¸€æ¬¡åŠ è½½ 4/8 ä¸ªå…ƒç´ ï¼Œæé«˜å¸¦å®½
2. **å¤šä¸ªç´¯åŠ å™¨**ï¼šå‡å°‘å¾ªç¯ä¾èµ–ï¼Œæé«˜å¹¶è¡Œåº¦
3. **å…±äº«å†…å­˜**ï¼šBlock å†…å¿«é€Ÿé€šä¿¡
4. **æ ‘å½¢å½’çº¦**ï¼šO(log n) å¤æ‚åº¦ï¼Œé«˜æ•ˆçš„ block å†…å½’çº¦
5. **å¯¹é½ä¼˜åŒ–**ï¼šå¤„ç†éå¯¹é½å†…å­˜è®¿é—®

3. **16 ä½ç±»å‹çš„ç‰¹æ®Šå¤„ç†**ï¼š
   - `float16` å’Œ `bfloat16` ä½¿ç”¨æ›´æ¿€è¿›çš„å‘é‡åŒ–ç­–ç•¥
   - `input_vec_size=8` è¡¨ç¤ºä¸€æ¬¡åŠ è½½ 8 ä¸ªå…ƒç´ 

### 4.3 å½’çº¦å†…æ ¸çš„å·¥ä½œæµç¨‹

å½’çº¦æ“ä½œæ¯”å…ƒç´ çº§æ“ä½œå¤æ‚å¾—å¤šï¼Œå› ä¸ºå®ƒéœ€è¦ï¼š

1. **åŠ è½½é˜¶æ®µ**ï¼šæ¯ä¸ªçº¿ç¨‹åŠ è½½å¤šä¸ªè¾“å…¥å…ƒç´ 
2. **å±€éƒ¨å½’çº¦**ï¼šæ¯ä¸ªçº¿ç¨‹åœ¨å¯„å­˜å™¨ä¸­å½’çº¦è‡ªå·±åŠ è½½çš„æ•°æ®
3. **Block çº§å½’çº¦**ï¼šä½¿ç”¨å…±äº«å†…å­˜ï¼ˆshared memoryï¼‰åœ¨ block å†…éƒ¨å½’çº¦
4. **å…¨å±€å½’çº¦**ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼šå¤šä¸ª block çš„ç»“æœéœ€è¦å†æ¬¡å½’çº¦

#### ç®€åŒ–çš„å½’çº¦æµç¨‹ç¤ºæ„

```cpp
// ä¼ªä»£ç ç¤ºæ„
__global__ void reduce_kernel(...) {
  // 1. åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨
  acc_t local_sum = 0;
  for (int i = threadIdx.x; i < N; i += blockDim.x) {
    local_sum += input[i];
  }
  
  // 2. å­˜å‚¨åˆ°å…±äº«å†…å­˜
  __shared__ acc_t shared_sum[blockDim.x];
  shared_sum[threadIdx.x] = local_sum;
  __syncthreads();
  
  // 3. Block å†…å½’çº¦ï¼ˆæ ‘å½¢å½’çº¦ï¼‰
  for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
    if (threadIdx.x < stride) {
      shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
    }
    __syncthreads();
  }
  
  // 4. ç¬¬ä¸€ä¸ªçº¿ç¨‹å†™å…¥ç»“æœ
  if (threadIdx.x == 0) {
    output[blockIdx.x] = shared_sum[0];
  }
}
```

#### å®é™…çš„å½’çº¦å®ç°

å®é™…çš„å½’çº¦å®ç°æ›´å¤æ‚ï¼Œåœ¨ `Reduce.cuh` ä¸­ï¼š

- **å‘é‡åŒ–åŠ è½½**ï¼šä¸€æ¬¡åŠ è½½å¤šä¸ªå…ƒç´ ä»¥æé«˜å¸¦å®½åˆ©ç”¨ç‡
- **å¤šçº§å½’çº¦**ï¼šæ”¯æŒ block å†…çš„å¤šç»´åº¦å½’çº¦
- **åŸå­æ“ä½œ**ï¼šå¤„ç†å¤š block ä¹‹é—´çš„å½’çº¦
- **ä¸åŒå½’çº¦ç±»å‹**ï¼šsum, prod, min, max ç­‰ä½¿ç”¨ä¸åŒçš„åˆå¹¶å‡½æ•°

---

## 5ï¸âƒ£ å¤æ‚ç®—å­ï¼šç´¢å¼•æ“ä½œï¼ˆIndexï¼‰

### 5.1 ç®—å­è¯´æ˜

**ç´¢å¼•æ“ä½œ**å…è®¸ä½¿ç”¨å¼ é‡ç´¢å¼•æ¥è®¿é—®æˆ–ä¿®æ”¹å¦ä¸€ä¸ªå¼ é‡çš„å…ƒç´ ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å¯¹å¤æ‚çš„æ“ä½œï¼Œå› ä¸ºï¼š

- **ä¸è§„åˆ™è®¿é—®æ¨¡å¼**ï¼šæ¯ä¸ªè¾“å‡ºå…ƒç´ å¯èƒ½éœ€è¦è®¿é—®è¾“å…¥çš„ä¸åŒä½ç½®
- **éœ€è¦è¾¹ç•Œæ£€æŸ¥**ï¼šç´¢å¼•å¯èƒ½è¶Šç•Œ
- **å†…å­˜è®¿é—®æ¨¡å¼å¤æ‚**ï¼šéš¾ä»¥å‘é‡åŒ–

**åŠŸèƒ½ç¤ºä¾‹**ï¼š`out[i] = input[indices[i]]`

### 5.2 æºç å®ç°ç‰‡æ®µ

```28:54:Pytorchå­¦ä¹ /pytorch/aten/src/ATen/native/cuda/IndexKernel.cu
template<int nt, int vt, typename func_t>
C10_LAUNCH_BOUNDS_2(nt, launch_bound2)
__global__ void index_elementwise_kernel(const int64_t N, const func_t f) {
  const auto tid = threadIdx.x;
  const auto nv = nt * vt;
  auto idx = nv * blockIdx.x + tid;
  #pragma unroll
  for (int i = 0; i < vt; i++) {
    if (idx < N) {
      f(idx);
      idx += nt;
    }
  }
}

template<int nt, int vt, typename func_t>
static void launch_kernel(const int64_t N, const func_t& f) {
  TORCH_INTERNAL_ASSERT(N >= 0 && N <= std::numeric_limits<int32_t>::max());
  if (N == 0) {
    return;
  }
  const dim3 block(nt);
  const dim3 grid((N + block.x * vt - 1) / (block.x * vt));
  const auto stream = at::cuda::getCurrentCUDAStream();
  index_elementwise_kernel<nt, vt, func_t><<<grid, block, 0, stream>>>(N, f);
  C10_CUDA_KERNEL_LAUNCH_CHECK();
}
```

#### ä»£ç è§£æ

1. **`nt` å’Œ `vt` æ¨¡æ¿å‚æ•°**ï¼š
   - `nt`ï¼šæ¯ä¸ª block çš„çº¿ç¨‹æ•°ï¼ˆnumber of threadsï¼‰
   - `vt`ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„å…ƒç´ æ•°ï¼ˆvalues per threadï¼‰
   - è¿™ç§è®¾è®¡å…è®¸è°ƒæ•´æ¯ä¸ªçº¿ç¨‹çš„å·¥ä½œé‡

2. **Grid-Stride Loop æ¨¡å¼**ï¼š
   ```cpp
   auto idx = nv * blockIdx.x + tid;
   for (int i = 0; i < vt; i++) {
     if (idx < N) {
       f(idx);
       idx += nt;  // ä¸‹ä¸€ä¸ªå…ƒç´ 
     }
   }
   ```
   - è¿™ç§æ¨¡å¼å…è®¸ä»»æ„å¤§å°çš„æ•°ç»„ï¼Œä¸éœ€è¦å®Œç¾å¯¹é½
   - æ¯ä¸ªçº¿ç¨‹å¤„ç† `vt` ä¸ªå…ƒç´ ï¼Œå…ƒç´ ä¹‹é—´çš„é—´éš”æ˜¯ `nt`

3. **`C10_LAUNCH_BOUNDS_2`**ï¼š
   - è¿™æ˜¯ CUDA çš„ launch boundsï¼Œå‘Šè¯‰ç¼–è¯‘å™¨æ¯ä¸ª block çš„æœ€å¤§çº¿ç¨‹æ•°
   - å¸®åŠ©ç¼–è¯‘å™¨è¿›è¡Œå¯„å­˜å™¨åˆ†é…ä¼˜åŒ–

---

## ğŸ”§ TensorIteratorï¼šç»Ÿä¸€çš„å†…å­˜è®¿é—®æŠ½è±¡

### ä»€ä¹ˆæ˜¯ TensorIteratorï¼Ÿ

`TensorIterator` æ˜¯ PyTorch ä¸­ç”¨äºç»Ÿä¸€å¤„ç†ä¸åŒå½¢çŠ¶ã€å†…å­˜å¸ƒå±€çš„å¼ é‡è¿­ä»£çš„å·¥å…·ã€‚å®ƒè‡ªåŠ¨å¤„ç†ï¼š

1. **å¹¿æ’­**ï¼šè‡ªåŠ¨æ‰©å±•ç»´åº¦ä»¥åŒ¹é…å½¢çŠ¶
2. **å†…å­˜å¸ƒå±€**ï¼šå¤„ç† contiguousã€channels_last ç­‰ä¸åŒå¸ƒå±€
3. **ç±»å‹è½¬æ¢**ï¼šå¤„ç†ä¸åŒç±»å‹ä¹‹é—´çš„è¿ç®—
4. **åç§»è®¡ç®—**ï¼šè‡ªåŠ¨è®¡ç®—æ¯ä¸ªå…ƒç´ çš„æ­£ç¡®å†…å­˜åç§»

### TensorIterator çš„å·¥ä½œæ–¹å¼

å½“æˆ‘ä»¬è°ƒç”¨ `gpu_kernel(iter, functor)` æ—¶ï¼š

1. `TensorIterator` åˆ†æè¾“å…¥è¾“å‡ºå¼ é‡çš„å½¢çŠ¶å’Œå†…å­˜å¸ƒå±€
2. ç”Ÿæˆç»Ÿä¸€çš„å†…å­˜è®¿é—®æ¨¡å¼
3. è‡ªåŠ¨å¤„ç†å¹¿æ’­å’Œç±»å‹è½¬æ¢
4. å¯åŠ¨ CUDA kernelï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªæˆ–å¤šä¸ªå…ƒç´ 

### ç¤ºä¾‹ï¼šä¸ºä»€ä¹ˆéœ€è¦ TensorIterator

è€ƒè™‘ä¸¤ä¸ªå½¢çŠ¶ä¸åŒçš„å¼ é‡ç›¸åŠ ï¼š

```python
a = torch.randn(3, 1, 5)  # shape: (3, 1, 5)
b = torch.randn(3, 4, 5)  # shape: (3, 4, 5)
c = a + b  # å¹¿æ’­å shape: (3, 4, 5)
```

`TensorIterator` ä¼šè‡ªåŠ¨ï¼š
- æ‰©å±• `a` çš„ç¬¬äºŒä¸ªç»´åº¦ï¼ˆä» 1 åˆ° 4ï¼‰
- è®¡ç®—æ­£ç¡®çš„å†…å­˜åç§»ï¼ˆ`a` çš„ stride ä¸ `b` ä¸åŒï¼‰
- ç¡®ä¿æ¯ä¸ªçº¿ç¨‹è®¿é—®æ­£ç¡®çš„å…ƒç´ 

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å‘é‡åŒ–ï¼ˆVectorizationï¼‰

PyTorch å¤§é‡ä½¿ç”¨å‘é‡åŒ–æ¥æé«˜æ€§èƒ½ï¼š

- **åŠ è½½å‘é‡åŒ–**ï¼šä¸€æ¬¡åŠ è½½ 4 ä¸ªæˆ– 8 ä¸ªå…ƒç´ ï¼ˆå¦‚ `float4`, `float8`ï¼‰
- **è®¡ç®—å‘é‡åŒ–**ï¼šåœ¨å¯„å­˜å™¨ä¸­å¹¶è¡Œå¤„ç†å¤šä¸ªå…ƒç´ 
- **å­˜å‚¨å‘é‡åŒ–**ï¼šä¸€æ¬¡å­˜å‚¨å¤šä¸ªç»“æœ

### 2. å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰

å½’çº¦æ“ä½œä½¿ç”¨å…±äº«å†…å­˜ï¼š
- **å¿«é€Ÿé€šä¿¡**ï¼šBlock å†…çº¿ç¨‹ä¹‹é—´å…±äº«æ•°æ®
- **å‡å°‘å…¨å±€å†…å­˜è®¿é—®**ï¼šå…ˆåœ¨å…±äº«å†…å­˜ä¸­å½’çº¦ï¼Œå†å†™å›å…¨å±€å†…å­˜

### 3. å¯„å­˜å™¨ä¼˜åŒ–

- **Launch Bounds**ï¼šå‘Šè¯‰ç¼–è¯‘å™¨é¢„æœŸçš„çº¿ç¨‹é…ç½®ï¼Œä¼˜åŒ–å¯„å­˜å™¨ä½¿ç”¨
- **å‡å°‘å¯„å­˜å™¨å‹åŠ›**ï¼šé¿å…åœ¨ kernel ä¸­ä½¿ç”¨è¿‡å¤šå±€éƒ¨å˜é‡

### 4. å†…å­˜åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰

- **è¿ç»­è®¿é—®**ï¼šå°½é‡è®©çº¿ç¨‹è®¿é—®è¿ç»­çš„å†…å­˜åœ°å€
- **å¯¹é½è®¿é—®**ï¼šå¯¹é½åˆ°å†…å­˜è¾¹ç•Œä»¥æé«˜å¸¦å®½

---

## ğŸ“ å¦‚ä½•å®ç°è‡ªå®šä¹‰ CUDA ç®—å­

### æ­¥éª¤ 1ï¼šå®šä¹‰ Functor

```cpp
template<typename scalar_t>
struct MyCustomFunctor {
  __device__ __forceinline__ scalar_t operator() (const scalar_t a) const {
    // ä½ çš„è®¡ç®—é€»è¾‘
    return a * 2.0f + 1.0f;
  }
};
```

### æ­¥éª¤ 2ï¼šå®ç°å…¥å£å‡½æ•°

```cpp
void my_custom_kernel_cuda(TensorIteratorBase& iter) {
  AT_DISPATCH_ALL_TYPES(iter.dtype(), "my_custom_cuda", [&]() {
    gpu_kernel(iter, MyCustomFunctor<scalar_t>());
  });
}
```

### æ­¥éª¤ 3ï¼šæ³¨å†Œåˆ°åˆ†å‘ç³»ç»Ÿ

```cpp
REGISTER_DISPATCH(my_custom_stub, &my_custom_kernel_cuda)
```

### æ­¥éª¤ 4ï¼šåœ¨ native_functions.yaml ä¸­å®šä¹‰

```yaml
- func: my_custom(Tensor self) -> Tensor
  dispatch:
    CUDA: my_custom_stub
```

---

## ğŸ“Š ç®—å­å¤æ‚åº¦å¯¹æ¯”

| ç®—å­ç±»å‹ | å¤æ‚åº¦ | ä¸»è¦æŒ‘æˆ˜ | ç¤ºä¾‹ |
|---------|--------|----------|------|
| **Fill** | â­ | æ—  | Fill |
| **ä¸€å…ƒå…ƒç´ çº§** | â­â­ | ç±»å‹åˆ†å‘ | Abs, Sin, Log |
| **äºŒå…ƒå…ƒç´ çº§** | â­â­â­ | å¹¿æ’­ã€æ ‡é‡å¤„ç† | Add, Mul, Div |
| **å½’çº¦** | â­â­â­â­ | åŒæ­¥ã€å…±äº«å†…å­˜ | Sum, Max, Mean |
| **ç´¢å¼•** | â­â­â­â­ | ä¸è§„åˆ™è®¿é—® | Index, Gather |
| **å·ç§¯/çŸ©é˜µä¹˜æ³•** | â­â­â­â­â­ | å¤æ‚çš„å†…å­˜è®¿é—®æ¨¡å¼ | Conv2d, Matmul |

---

## ğŸ” æ€»ç»“

PyTorch çš„ CUDA ç®—å­å®ç°éµå¾ªä»¥ä¸‹è®¾è®¡åŸåˆ™ï¼š

1. **åˆ†å±‚æŠ½è±¡**ï¼š
   - ä¸Šå±‚ï¼šPython API å’Œç±»å‹åˆ†å‘
   - ä¸­å±‚ï¼šTensorIterator å’Œé€šç”¨å¯åŠ¨å‡½æ•°
   - åº•å±‚ï¼šå®é™…çš„ CUDA kernel

2. **ä»£ç å¤ç”¨**ï¼š
   - ä½¿ç”¨æ¨¡æ¿å’Œ Functor å‡å°‘é‡å¤ä»£ç 
   - `gpu_kernel` å’Œ `gpu_reduce_kernel` å¤„ç†é€šç”¨é€»è¾‘

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å‘é‡åŒ–å†…å­˜è®¿é—®
   - å…±äº«å†…å­˜ç”¨äºå½’çº¦
   - JIT ç¼–è¯‘å‡å°‘äºŒè¿›åˆ¶å¤§å°

4. **çµæ´»æ€§**ï¼š
   - æ”¯æŒå¤šç§æ•°æ®ç±»å‹
   - è‡ªåŠ¨å¤„ç†å¹¿æ’­å’Œå†…å­˜å¸ƒå±€
   - æ”¯æŒæ ‡é‡å‚æ•°

ç†è§£è¿™äº›ç®—å­çš„å®ç°æ–¹å¼ï¼Œæœ‰åŠ©äºï¼š
- **è°ƒè¯•æ€§èƒ½é—®é¢˜**ï¼šäº†è§£åº•å±‚å®ç°ï¼Œæ‰¾åˆ°ç“¶é¢ˆ
- **å®ç°è‡ªå®šä¹‰ç®—å­**ï¼šéµå¾ªç›¸åŒçš„æ¨¡å¼
- **ä¼˜åŒ–æ¨¡å‹æ€§èƒ½**ï¼šç†è§£ä¸åŒæ“ä½œçš„ä»£ä»·

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **PyTorch æºç ä½ç½®**ï¼š`aten/src/ATen/native/cuda/`
- **å…³é”®å¤´æ–‡ä»¶**ï¼š
  - `Loops.cuh`ï¼šå…ƒç´ çº§æ“ä½œçš„é€šç”¨æ¡†æ¶
  - `Reduce.cuh`ï¼šå½’çº¦æ“ä½œçš„å®ç°
  - `CUDALoops.cuh`ï¼šå‘é‡åŒ–çš„å…ƒç´ çº§æ“ä½œ
- **æ–‡æ¡£**ï¼š
  - `torch/csrc/README.md`ï¼šC++ ä»£ç è¯´æ˜
  - `aten/src/ATen/native/README.md`ï¼šæ“ä½œç¬¦å®ç°æŒ‡å—

