# CUDA åŸºç¡€å›é¡¾

## ğŸ“š å­¦ä¹ ç›®æ ‡

1. å›é¡¾ CUDA ç¼–ç¨‹æ¨¡å‹
2. ç†è§£ GPU å†…å­˜å±‚æ¬¡ç»“æ„
3. æŒæ¡å†…å­˜è®¿é—®ä¼˜åŒ–æŠ€æœ¯
4. ç†è§£ Warp å’Œçº¿ç¨‹å—çš„æ¦‚å¿µ

---

## ğŸ—ï¸ CUDA ç¼–ç¨‹æ¨¡å‹

### ä¸»æœº-è®¾å¤‡æ¶æ„

**ä¸»æœºï¼ˆHostï¼‰**ï¼š
- CPU å’Œä¸»æœºå†…å­˜
- è´Ÿè´£ç¨‹åºæ§åˆ¶å’Œæ•°æ®å‡†å¤‡
- æ‰§è¡Œä¸²è¡Œä»£ç 

**è®¾å¤‡ï¼ˆDeviceï¼‰**ï¼š
- GPU å’Œè®¾å¤‡å†…å­˜
- è´Ÿè´£å¹¶è¡Œè®¡ç®—
- æ‰§è¡Œ CUDA å†…æ ¸å‡½æ•°

### CUDA ç¨‹åºç»“æ„

```cuda
#include <cuda_runtime.h>

// è®¾å¤‡å‡½æ•°ï¼ˆåœ¨ GPU ä¸Šæ‰§è¡Œï¼‰
__global__ void kernel_function(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] = data[idx] * 2.0f;
    }
}

// ä¸»æœºå‡½æ•°ï¼ˆåœ¨ CPU ä¸Šæ‰§è¡Œï¼‰
int main() {
    // 1. åˆ†é…ä¸»æœºå†…å­˜
    float* h_data = (float*)malloc(1024 * sizeof(float));
    
    // 2. åˆ†é…è®¾å¤‡å†…å­˜
    float* d_data;
    cudaMalloc(&d_data, 1024 * sizeof(float));
    
    // 3. å¤åˆ¶æ•°æ®åˆ°è®¾å¤‡
    cudaMemcpy(d_data, h_data, 1024 * sizeof(float), 
               cudaMemcpyHostToDevice);
    
    // 4. å¯åŠ¨å†…æ ¸
    kernel_function<<<4, 256>>>(d_data, 1024);
    
    // 5. å¤åˆ¶ç»“æœå›ä¸»æœº
    cudaMemcpy(h_data, d_data, 1024 * sizeof(float), 
               cudaMemcpyDeviceToHost);
    
    // 6. æ¸…ç†
    cudaFree(d_data);
    free(h_data);
    
    return 0;
}
```

---

## ğŸ§µ çº¿ç¨‹å±‚æ¬¡ç»“æ„

### ä¸‰çº§å±‚æ¬¡ç»“æ„

```
Grid (ç½‘æ ¼)
  â””â”€â”€ Block (çº¿ç¨‹å—)
        â””â”€â”€ Thread (çº¿ç¨‹)
```

### çº¿ç¨‹æ ‡è¯†ç¬¦

**å†…ç½®å˜é‡**ï¼š
- `threadIdx`ï¼šçº¿ç¨‹åœ¨çº¿ç¨‹å—å†…çš„ç´¢å¼•
- `blockIdx`ï¼šçº¿ç¨‹å—åœ¨ç½‘æ ¼å†…çš„ç´¢å¼•
- `blockDim`ï¼šçº¿ç¨‹å—çš„ç»´åº¦
- `gridDim`ï¼šç½‘æ ¼çš„ç»´åº¦

**å…¨å±€çº¿ç¨‹ ID è®¡ç®—**ï¼š
```cuda
// 1D æƒ…å†µ
int global_id = blockIdx.x * blockDim.x + threadIdx.x;

// 2D æƒ…å†µ
int global_id_x = blockIdx.x * blockDim.x + threadIdx.x;
int global_id_y = blockIdx.y * blockDim.y + threadIdx.y;

// 3D æƒ…å†µ
int global_id = (blockIdx.z * gridDim.y * gridDim.x + 
                 blockIdx.y * gridDim.x + 
                 blockIdx.x) * (blockDim.x * blockDim.y * blockDim.z) +
                (threadIdx.z * blockDim.y * blockDim.x +
                 threadIdx.y * blockDim.x +
                 threadIdx.x);
```

### Warp

**Warp å®šä¹‰**ï¼š
- 32 ä¸ªçº¿ç¨‹ç»„æˆä¸€ä¸ª warp
- Warp æ˜¯ GPU è°ƒåº¦çš„åŸºæœ¬å•ä½
- Warp å†…çº¿ç¨‹æ‰§è¡Œ SIMTï¼ˆå•æŒ‡ä»¤å¤šçº¿ç¨‹ï¼‰

**Warp ID å’Œ Lane ID**ï¼š
```cuda
int warp_id = threadIdx.x / 32;
int lane_id = threadIdx.x % 32;
```

**Warp çº§æ“ä½œ**ï¼š
```cuda
// Shuffle æ“ä½œï¼ˆwarp å†…æ•°æ®äº¤æ¢ï¼‰
float val = __shfl_sync(0xffffffff, val, lane_id + 1);

// Warp çº§å½’çº¦
float sum = __shfl_down_sync(0xffffffff, val, 1);
sum += __shfl_down_sync(0xffffffff, sum, 2);
sum += __shfl_down_sync(0xffffffff, sum, 4);
sum += __shfl_down_sync(0xffffffff, sum, 8);
sum += __shfl_down_sync(0xffffffff, sum, 16);
```

---

## ğŸ’¾ GPU å†…å­˜å±‚æ¬¡ç»“æ„

### å†…å­˜å±‚æ¬¡ï¼ˆä»å¿«åˆ°æ…¢ï¼‰

```
å¯„å­˜å™¨ (Registers)
  â†“ (~1000 TB/s)
å…±äº«å†…å­˜ (Shared Memory)
  â†“ (~100 TB/s)
L1 ç¼“å­˜ (L1 Cache)
  â†“ (~10 TB/s)
L2 ç¼“å­˜ (L2 Cache)
  â†“ (~1 TB/s)
å…¨å±€å†…å­˜ (Global Memory)
```

### å¯„å­˜å™¨ï¼ˆRegistersï¼‰

**ç‰¹ç‚¹**ï¼š
- æœ€å¿«çš„å†…å­˜
- æ¯ä¸ªçº¿ç¨‹ç§æœ‰
- å®¹é‡æœ‰é™ï¼ˆæ¯ä¸ªçº¿ç¨‹ ~255 ä¸ªå¯„å­˜å™¨ï¼‰

**ä½¿ç”¨**ï¼š
```cuda
__global__ void kernel() {
    float a = 1.0f;  // å­˜å‚¨åœ¨å¯„å­˜å™¨
    float b = 2.0f;  // å­˜å‚¨åœ¨å¯„å­˜å™¨
    float c = a + b; // å­˜å‚¨åœ¨å¯„å­˜å™¨
}
```

**é™åˆ¶**ï¼š
- å¯„å­˜å™¨æº¢å‡ºä¼šä½¿ç”¨æœ¬åœ°å†…å­˜ï¼ˆL1 ç¼“å­˜ï¼‰
- å½±å“æ€§èƒ½

### å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰

**ç‰¹ç‚¹**ï¼š
- å¾ˆå¿«çš„å†…å­˜ï¼ˆ~100 TB/sï¼‰
- çº¿ç¨‹å—å†…å…±äº«
- å®¹é‡æœ‰é™ï¼ˆæ¯ä¸ª SM 48KB æˆ– 164KBï¼‰

**å£°æ˜**ï¼š
```cuda
__global__ void kernel() {
    __shared__ float shared_data[256];  // å…±äº«å†…å­˜
    
    int tid = threadIdx.x;
    shared_data[tid] = ...;
    __syncthreads();  // åŒæ­¥æ‰€æœ‰çº¿ç¨‹
    ...
}
```

**Bank Conflict**ï¼š
- å…±äº«å†…å­˜åˆ†æˆ 32 ä¸ª bank
- å¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bankï¼Œä¼šäº§ç”Ÿå†²çª
- éœ€è¦é¿å… bank conflict

**é¿å… Bank Conflict**ï¼š
```cuda
// âŒ æœ‰ bank conflict
__shared__ float data[32];
data[threadIdx.x] = ...;  // æ‰€æœ‰çº¿ç¨‹è®¿é—®ä¸åŒ bankï¼Œä½†å¯èƒ½å†²çª

// âœ… æ—  bank conflictï¼ˆä½¿ç”¨ paddingï¼‰
__shared__ float data[33];  // 33 = 32 + 1 (padding)
data[threadIdx.x] = ...;  // é¿å…å†²çª
```

### å…¨å±€å†…å­˜ï¼ˆGlobal Memoryï¼‰

**ç‰¹ç‚¹**ï¼š
- æœ€æ…¢çš„å†…å­˜ï¼ˆ~1 TB/sï¼‰
- æ‰€æœ‰çº¿ç¨‹å¯è®¿é—®
- å®¹é‡å¤§ï¼ˆå‡  GB åˆ°å‡ å GBï¼‰

**å†…å­˜åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰**ï¼š
- ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜ä½ç½®
- ç¡¬ä»¶å¯ä»¥å°†å¤šä¸ªè®¿é—®åˆå¹¶ä¸ºä¸€ä¸ªäº‹åŠ¡
- æ˜¾è‘—æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

**åˆå¹¶è®¿é—®ç¤ºä¾‹**ï¼š
```cuda
// âœ… åˆå¹¶è®¿é—®ï¼ˆç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜ï¼‰
__global__ void coalesced_access(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx] = data[idx] * 2.0f;  // çº¿ç¨‹ 0 è®¿é—® data[0]ï¼Œçº¿ç¨‹ 1 è®¿é—® data[1]ï¼Œ...
}

// âŒ éåˆå¹¶è®¿é—®ï¼ˆçº¿ç¨‹è®¿é—®ä¸è¿ç»­çš„å†…å­˜ï¼‰
__global__ void non_coalesced_access(float* data, int stride) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx * stride] = data[idx * stride] * 2.0f;  // è®¿é—®é—´éš”å¤§
}
```

---

## ğŸš€ å†…å­˜è®¿é—®ä¼˜åŒ–

### å†…å­˜åˆå¹¶è®¿é—®è§„åˆ™

**è§„åˆ™**ï¼š
1. çº¿ç¨‹è®¿é—®çš„å†…å­˜åœ°å€å¿…é¡»è¿ç»­
2. è®¿é—®çš„èµ·å§‹åœ°å€å¿…é¡»å¯¹é½ï¼ˆ128 å­—èŠ‚å¯¹é½ï¼‰
3. è®¿é—®å¤§å°å¿…é¡»æ˜¯ 1ã€2ã€4ã€8 æˆ– 16 å­—èŠ‚

**ç¤ºä¾‹**ï¼š
```cuda
// âœ… 128 å­—èŠ‚å¯¹é½ï¼Œè¿ç»­è®¿é—®
__global__ void aligned_access(float4* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    float4 val = data[idx];  // 16 å­—èŠ‚å¯¹é½è®¿é—®
}

// âŒ æœªå¯¹é½è®¿é—®
__global__ void unaligned_access(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx + 1] = ...;  // æœªå¯¹é½
}
```

### å…±äº«å†…å­˜ä¼˜åŒ–

**ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ•°æ®**ï¼š
```cuda
__global__ void shared_memory_cache(float* input, float* output, int n) {
    __shared__ float cache[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜
    if (idx < n) {
        cache[tid] = input[idx];
    }
    __syncthreads();
    
    // ä»å…±äº«å†…å­˜è¯»å–ï¼ˆæ›´å¿«ï¼‰
    if (idx < n) {
        output[idx] = cache[tid] * 2.0f;
    }
}
```

### é¢„å–ï¼ˆPrefetchingï¼‰

**é¢„å–æ•°æ®åˆ°å…±äº«å†…å­˜**ï¼š
```cuda
__global__ void prefetch_kernel(float* data, int n) {
    __shared__ float tile[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // é¢„å–ä¸‹ä¸€ä¸ªå—çš„æ•°æ®
    if (idx + blockDim.x < n) {
        tile[tid] = data[idx + blockDim.x];
    }
    
    // å¤„ç†å½“å‰æ•°æ®
    float val = data[idx];
    // ... è®¡ç®— ...
    
    __syncthreads();
    
    // ä½¿ç”¨é¢„å–çš„æ•°æ®
    if (idx + blockDim.x < n) {
        float next_val = tile[tid];
        // ... è®¡ç®— ...
    }
}
```

---

## ğŸ”§ Flash-Attention ä¸­çš„å†…å­˜ä¼˜åŒ–

### Qã€Kã€V çš„åŠ è½½

**ç­–ç•¥**ï¼š
1. ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜
2. ä»å…±äº«å†…å­˜åŠ è½½åˆ°å¯„å­˜å™¨
3. åœ¨å¯„å­˜å™¨ä¸­è®¡ç®—

**ä»£ç ç¤ºä¾‹**ï¼ˆç®€åŒ–ï¼‰ï¼š
```cuda
__global__ void flash_attention_kernel(...) {
    __shared__ float q_tile[64][128];  // Q å—
    __shared__ float k_tile[64][128];  // K å—
    __shared__ float v_tile[64][128];  // V å—
    
    // 1. ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜ï¼ˆåˆå¹¶è®¿é—®ï¼‰
    int tid = threadIdx.x;
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    
    // åŠ è½½ Q å—
    for (int i = 0; i < 4; i++) {
        int row = warp_id * 4 + i;
        int col = lane_id * 4;
        if (row < 64 && col < 128) {
            q_tile[row][col] = q_global[row][col];
        }
    }
    __syncthreads();
    
    // 2. ä»å…±äº«å†…å­˜åŠ è½½åˆ°å¯„å­˜å™¨
    float q_reg[4];
    for (int i = 0; i < 4; i++) {
        q_reg[i] = q_tile[warp_id][lane_id * 4 + i];
    }
    
    // 3. è®¡ç®—ï¼ˆåœ¨å¯„å­˜å™¨ä¸­ï¼‰
    // ...
}
```

### å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–

**Flash-Attention çš„ä¼˜åŒ–**ï¼š
1. **åˆå¹¶è®¿é—®**ï¼šQã€Kã€V çš„åŠ è½½ä½¿ç”¨åˆå¹¶è®¿é—®
2. **å…±äº«å†…å­˜ç¼“å­˜**ï¼šä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜å—æ•°æ®
3. **å¯„å­˜å™¨ä¼˜åŒ–**ï¼šä¸­é—´ç»“æœå­˜å‚¨åœ¨å¯„å­˜å™¨
4. **é¿å… Bank Conflict**ï¼šä½¿ç”¨åˆé€‚çš„å…±äº«å†…å­˜å¸ƒå±€

---

## ğŸ“Š æ€§èƒ½åˆ†æå·¥å…·

### Nsight Compute

**åŠŸèƒ½**ï¼š
- åˆ†æå†…æ ¸æ€§èƒ½
- å†…å­˜è®¿é—®åˆ†æ
- å ç”¨ç‡åˆ†æ

**ä½¿ç”¨**ï¼š
```bash
ncu --set full ./your_program
```

### Nsight Systems

**åŠŸèƒ½**ï¼š
- æ•´ä½“æ€§èƒ½åˆ†æ
- æ—¶é—´çº¿åˆ†æ
- å†…å­˜ä½¿ç”¨åˆ†æ

**ä½¿ç”¨**ï¼š
```bash
nsys profile ./your_program
```

---

## ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“

### CUDA ç¼–ç¨‹è¦ç‚¹

1. **å†…å­˜å±‚æ¬¡**ï¼š
   - å¯„å­˜å™¨æœ€å¿«ï¼Œä½†å®¹é‡å°
   - å…±äº«å†…å­˜å¿«ï¼Œä½†éœ€è¦é¿å… bank conflict
   - å…¨å±€å†…å­˜æ…¢ï¼Œä½†éœ€è¦åˆå¹¶è®¿é—®

2. **çº¿ç¨‹ç»„ç»‡**ï¼š
   - Warp æ˜¯è°ƒåº¦çš„åŸºæœ¬å•ä½
   - çº¿ç¨‹å—å†…å¯ä»¥å…±äº«å†…å­˜å’ŒåŒæ­¥
   - åˆç†ç»„ç»‡çº¿ç¨‹å¯ä»¥æé«˜æ€§èƒ½

3. **å†…å­˜ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ•°æ®
   - ç¡®ä¿å†…å­˜åˆå¹¶è®¿é—®
   - é¿å… bank conflict

### Flash-Attention ä¸­çš„åº”ç”¨

1. **Qã€Kã€V åŠ è½½**ï¼šä½¿ç”¨åˆå¹¶è®¿é—®å’Œå…±äº«å†…å­˜
2. **ä¸­é—´ç»“æœ**ï¼šå­˜å‚¨åœ¨å¯„å­˜å™¨ï¼Œé¿å…å†™å›å…¨å±€å†…å­˜
3. **è¾“å‡º**ï¼šåªå†™ä¸€æ¬¡ï¼Œå‡å°‘å†…å­˜è®¿é—®

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥ç‚¹

- [ ] ç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹
- [ ] ç†è§£ GPU å†…å­˜å±‚æ¬¡ç»“æ„
- [ ] ç†è§£å†…å­˜åˆå¹¶è®¿é—®
- [ ] ç†è§£å…±äº«å†…å­˜çš„ä½¿ç”¨
- [ ] ç†è§£ Warp çš„æ¦‚å¿µ
- [ ] èƒ½å¤Ÿåˆ†æå†…å­˜è®¿é—®æ¨¡å¼

---

## ğŸ“š å‚è€ƒèµ„æº

- CUDA C++ Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- CUDA Best Practices Guide: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/

---

**å­¦ä¹ æ—¶é—´**ï¼š1-2 å¤©  
**éš¾åº¦**ï¼šâ­â­â­â˜†â˜†
