# å¼€å‘ç¯å¢ƒæ­å»º

## ğŸ“š å­¦ä¹ ç›®æ ‡

1. äº†è§£ç¯å¢ƒè¦æ±‚
2. å®‰è£… CUDA Toolkit
3. å®‰è£… Flash-Attention
4. éªŒè¯å®‰è£…
5. é…ç½®å¼€å‘å·¥å…·

---

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

**GPU è¦æ±‚**ï¼š
- NVIDIA GPUï¼ˆæ”¯æŒ SM 7.5+ï¼‰
  - **æ¨è**ï¼šRTX 3090, A100, H100
  - **æœ€ä½**ï¼šRTX 2060, GTX 1660 Ti
- **æ˜¾å­˜**ï¼šè‡³å°‘ 8GBï¼ˆæ¨è 16GB+ï¼‰

**æ£€æŸ¥ GPU**ï¼š
```bash
nvidia-smi
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03    Driver Version: 535.54.03    CUDA Version: 12.2  |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA A100 ...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   30C    P0    35W / 250W |      0MiB / 40960MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

### è½¯ä»¶è¦æ±‚

**æ“ä½œç³»ç»Ÿ**ï¼š
- Linuxï¼ˆæ¨è Ubuntu 20.04+ï¼‰
- Windowsï¼ˆéƒ¨åˆ†æ”¯æŒï¼Œéœ€è¦æ›´å¤šæµ‹è¯•ï¼‰

**CUDA Toolkit**ï¼š
- **æ¨è**ï¼šCUDA 11.8+ æˆ– 12.0+
- **æœ€ä½**ï¼šCUDA 11.6

**Python**ï¼š
- Python 3.8+
- pip

**ç¼–è¯‘å™¨**ï¼š
- GCC 7.5+ï¼ˆLinuxï¼‰
- MSVC 2019+ï¼ˆWindowsï¼‰

**å…¶ä»–å·¥å…·**ï¼š
- Git
- CMakeï¼ˆå¯é€‰ï¼‰

---

## ğŸ“¦ å®‰è£…æ­¥éª¤

### æ­¥éª¤ 1ï¼šå®‰è£… CUDA Toolkit

#### Linux

**æ–¹æ³• 1ï¼šä½¿ç”¨åŒ…ç®¡ç†å™¨ï¼ˆæ¨èï¼‰**

```bash
# Ubuntu/Debian
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-2

# è®¾ç½®ç¯å¢ƒå˜é‡
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

**æ–¹æ³• 2ï¼šä» NVIDIA å®˜ç½‘ä¸‹è½½**

1. è®¿é—®ï¼šhttps://developer.nvidia.com/cuda-downloads
2. é€‰æ‹©ä½ çš„ç³»ç»Ÿé…ç½®
3. ä¸‹è½½å¹¶å®‰è£…

**éªŒè¯å®‰è£…**ï¼š
```bash
nvcc --version
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.2, V12.2.140
Build cuda_12.2.r12.2/compiler.33567101_0
```

#### Windows

1. è®¿é—®ï¼šhttps://developer.nvidia.com/cuda-downloads
2. é€‰æ‹© Windows â†’ x86_64 â†’ 10/11 â†’ exe (local)
3. ä¸‹è½½å¹¶è¿è¡Œå®‰è£…ç¨‹åº
4. æŒ‰ç…§æç¤ºå®‰è£…

---

### æ­¥éª¤ 2ï¼šå®‰è£… PyTorch

**åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼š
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

**å®‰è£… PyTorch**ï¼š
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**éªŒè¯ PyTorch**ï¼š
```python
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
PyTorch version: 2.1.0+cu118
CUDA available: True
CUDA version: 11.8
GPU: NVIDIA A100-SXM4-40GB
```

---

### æ­¥éª¤ 3ï¼šå®‰è£…ä¾èµ–

**å®‰è£…å¿…éœ€ä¾èµ–**ï¼š
```bash
pip install packaging ninja
```

**éªŒè¯ ninja**ï¼š
```bash
ninja --version
echo $?  # åº”è¯¥è¿”å› 0
```

**å¦‚æœ ninja æœ‰é—®é¢˜**ï¼š
```bash
pip uninstall -y ninja
pip install ninja
```

---

### æ­¥éª¤ 4ï¼šå®‰è£… Flash-Attention

#### æ–¹æ³• 1ï¼šä» PyPI å®‰è£…ï¼ˆæ¨èï¼Œä½†å¯èƒ½è¾ƒæ…¢ï¼‰

```bash
pip install flash-attn --no-build-isolation
```

**æ³¨æ„**ï¼š
- `--no-build-isolation` æ˜¯å¿…éœ€çš„
- ç¼–è¯‘å¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿ
- éœ€è¦è¶³å¤Ÿçš„ RAMï¼ˆæ¨è 16GB+ï¼‰

#### æ–¹æ³• 2ï¼šä»æºç å®‰è£…ï¼ˆæ¨èï¼Œæ›´çµæ´»ï¼‰

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention

# å®‰è£…
pip install . --no-build-isolation
```

**å¦‚æœå†…å­˜ä¸è¶³**ï¼š
```bash
# é™åˆ¶å¹¶è¡Œç¼–è¯‘ä»»åŠ¡æ•°
MAX_JOBS=4 pip install . --no-build-isolation
```

#### æ–¹æ³• 3ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„ wheelï¼ˆå¦‚æœå¯ç”¨ï¼‰

```bash
# æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç¼–è¯‘ç‰ˆæœ¬
pip install flash-attn --no-build-isolation --no-cache-dir
```

---

### æ­¥éª¤ 5ï¼šéªŒè¯å®‰è£…

**Python éªŒè¯**ï¼š
```python
import flash_attn
print(f"Flash-Attention version: {flash_attn.__version__}")

# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
import torch
from flash_attn import flash_attn_func

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
seq_len = 1024
num_heads = 12
head_dim = 64

q = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
k = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
v = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)

# ä½¿ç”¨ Flash-Attention
out = flash_attn_func(q, k, v)
print(f"Output shape: {out.shape}")
print("âœ… Flash-Attention installed successfully!")
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
cd flash-attention
pytest tests/test_flash_attn.py -v
```

---

## ğŸ› ï¸ å¼€å‘å·¥å…·é…ç½®

### Visual Studio Code

**æ¨èæ‰©å±•**ï¼š
- C/C++ (Microsoft)
- CUDA (NVIDIA)
- Python (Microsoft)

**é…ç½®**ï¼ˆ`.vscode/settings.json`ï¼‰ï¼š
```json
{
    "C_Cpp.default.includePath": [
        "/usr/local/cuda/include",
        "${workspaceFolder}/**"
    ],
    "C_Cpp.default.compilerPath": "/usr/local/cuda/bin/nvcc",
    "files.associations": {
        "*.cu": "cuda-cpp",
        "*.cuh": "cuda-cpp"
    }
}
```

### æ€§èƒ½åˆ†æå·¥å…·

#### Nsight Compute

**å®‰è£…**ï¼š
```bash
# ä¸‹è½½å¹¶å®‰è£… Nsight Compute
# https://developer.nvidia.com/nsight-compute
```

**ä½¿ç”¨**ï¼š
```bash
# åˆ†æå†…æ ¸
ncu --set full ./your_program

# å¯¼å‡ºæŠ¥å‘Š
ncu --export report.ncu-rep ./your_program
```

#### Nsight Systems

**å®‰è£…**ï¼š
```bash
# ä¸‹è½½å¹¶å®‰è£… Nsight Systems
# https://developer.nvidia.com/nsight-systems
```

**ä½¿ç”¨**ï¼š
```bash
# æ€§èƒ½åˆ†æ
nsys profile ./your_program

# ç”ŸæˆæŠ¥å‘Š
nsys report report.qdrep
```

---

## ğŸ› å¸¸è§é—®é¢˜

### é—®é¢˜ 1ï¼šç¼–è¯‘å¤±è´¥

**é”™è¯¯**ï¼š`ninja: build stopped: subcommand failed.`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. æ£€æŸ¥ ninja æ˜¯å¦æ­£ç¡®å®‰è£…
ninja --version

# 2. é‡æ–°å®‰è£… ninja
pip uninstall -y ninja
pip install ninja

# 3. é™åˆ¶å¹¶è¡Œä»»åŠ¡æ•°
MAX_JOBS=4 pip install flash-attn --no-build-isolation
```

### é—®é¢˜ 2ï¼šå†…å­˜ä¸è¶³

**é”™è¯¯**ï¼š`g++: fatal error: Killed signal terminated program cc1plus`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é™åˆ¶å¹¶è¡Œç¼–è¯‘ä»»åŠ¡
MAX_JOBS=2 pip install flash-attn --no-build-isolation

# æˆ–è€…å¢åŠ äº¤æ¢ç©ºé—´
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### é—®é¢˜ 3ï¼šCUDA ç‰ˆæœ¬ä¸åŒ¹é…

**é”™è¯¯**ï¼š`CUDA version mismatch`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version
python -c "import torch; print(torch.version.cuda)"

# é‡æ–°å®‰è£…åŒ¹é…çš„ PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### é—®é¢˜ 4ï¼šæ‰¾ä¸åˆ° CUDA

**é”™è¯¯**ï¼š`Could not find CUDA`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# è®¾ç½® CUDA è·¯å¾„
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# éªŒè¯
nvcc --version
```

---

## âœ… å®‰è£…æ£€æŸ¥æ¸…å•

### åŸºç¡€ç¯å¢ƒ
- [ ] GPU é©±åŠ¨å·²å®‰è£…
- [ ] CUDA Toolkit å·²å®‰è£…
- [ ] `nvcc --version` æ­£å¸¸å·¥ä½œ
- [ ] `nvidia-smi` æ­£å¸¸å·¥ä½œ

### Python ç¯å¢ƒ
- [ ] Python 3.8+ å·²å®‰è£…
- [ ] è™šæ‹Ÿç¯å¢ƒå·²åˆ›å»º
- [ ] PyTorch å·²å®‰è£…
- [ ] PyTorch å¯ä»¥æ£€æµ‹åˆ° GPU

### Flash-Attention
- [ ] ä¾èµ–å·²å®‰è£…ï¼ˆpackaging, ninjaï¼‰
- [ ] Flash-Attention å·²å®‰è£…
- [ ] å¯ä»¥å¯¼å…¥ `flash_attn`
- [ ] æµ‹è¯•ä»£ç å¯ä»¥è¿è¡Œ

### å¼€å‘å·¥å…·
- [ ] ä»£ç ç¼–è¾‘å™¨å·²é…ç½®
- [ ] æ€§èƒ½åˆ†æå·¥å…·å·²å®‰è£…ï¼ˆå¯é€‰ï¼‰
- [ ] Git å·²é…ç½®

---

## ğŸ“ å¿«é€Ÿå¼€å§‹

**åˆ›å»ºæµ‹è¯•è„šæœ¬**ï¼ˆ`test_flash_attn.py`ï¼‰ï¼š
```python
import torch
from flash_attn import flash_attn_func

# é…ç½®
batch_size = 2
seq_len = 1024
num_heads = 12
head_dim = 64

# åˆ›å»ºæ•°æ®
q = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
k = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
v = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)

# è¿è¡Œ Flash-Attention
print("Running Flash-Attention...")
out = flash_attn_func(q, k, v)
print(f"âœ… Success! Output shape: {out.shape}")

# æ€§èƒ½æµ‹è¯•
import time
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    out = flash_attn_func(q, k, v)
torch.cuda.synchronize()
end = time.time()
print(f"Average time: {(end - start) / 100 * 1000:.2f} ms")
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
python test_flash_attn.py
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®‰è£…å®Œæˆåï¼Œå¯ä»¥å¼€å§‹ï¼š

1. **å­¦ä¹ ç®—æ³•**ï¼šé˜…è¯» Flash-Attention è®ºæ–‡
2. **åˆ†æä»£ç **ï¼šæŸ¥çœ‹æºç ç»“æ„
3. **è¿è¡Œç¤ºä¾‹**ï¼šè¿è¡Œ benchmarks
4. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ Nsight å·¥å…·åˆ†ææ€§èƒ½

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- Flash-Attention GitHub: https://github.com/Dao-AILab/flash-attention
- CUDA Toolkit: https://developer.nvidia.com/cuda-toolkit
- PyTorch: https://pytorch.org/

### å®‰è£…æŒ‡å—
- Flash-Attention å®‰è£…: https://github.com/Dao-AILab/flash-attention#installation-and-features
- CUDA å®‰è£…: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/

---

**å­¦ä¹ æ—¶é—´**ï¼š1-2 å¤©  
**éš¾åº¦**ï¼šâ­â­â˜†â˜†â˜†
