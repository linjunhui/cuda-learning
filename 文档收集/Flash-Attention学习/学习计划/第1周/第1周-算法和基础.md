# ç¬¬1å‘¨ï¼šFlash-Attention ç®—æ³•å’Œ CUDA åŸºç¡€

## ğŸ“‹ å­¦ä¹ ç›®æ ‡

1. ç†è§£ Flash-Attention çš„æ ¸å¿ƒç®—æ³•æ€æƒ³
2. å›é¡¾ CUDA ç¼–ç¨‹åŸºç¡€çŸ¥è¯†
3. æ­å»ºå¼€å‘ç¯å¢ƒ
4. è¿è¡Œç¬¬ä¸€ä¸ª Flash-Attention ç¤ºä¾‹

### ç¼–è¯‘ä¸ç®—åŠ›æ¶æ„æç¤º
åœ¨ç¬¬ 1 å‘¨çš„ CUDA å­¦ä¹ ä¸ç»ƒä¹ ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ `nvcc` å‘½ä»¤åŒæ—¶ç”Ÿæˆ Ampere ä¸ Hopper çš„ SASS å’Œ PTXï¼š

```bash
nvcc main.cu -o main \
  --generate-code=arch=compute_80,code=[sm_80,compute_80] \
  --generate-code=arch=compute_90,code=[sm_90,compute_90]
```

- `sm_80` / `compute_80` å¯¹åº” Ampereï¼ˆä¾‹å¦‚ A100ï¼‰ï¼Œ`sm_90` / `compute_90` å¯¹åº” Hopperï¼ˆä¾‹å¦‚ H100ï¼‰ã€‚
- é€šè¿‡ `code=[sm_xx,compute_xx]` å¯ä»¥åŒæ—¶å¾—åˆ°æœ¬åœ° SASS ä¸å¯ç§»æ¤çš„ PTXï¼Œæ–¹ä¾¿åœ¨ä¸åŒ GPU ä¸Šè¿è¡Œæˆ–è¿›è¡Œ JITã€‚
- å¦‚æœç›®æ ‡ GPU ä¸åœ¨å…¶ä¸­ï¼Œå¯æ–°å¢ `--generate-code` å‚æ•°æˆ–è°ƒæ•´ä¸ºç›¸åº”ç®—åŠ›ç‰ˆæœ¬ã€‚
- **SASS**ï¼šé¢å‘ç‰¹å®š `sm_xx` æ¶æ„çš„æœ€ç»ˆæœºå™¨æŒ‡ä»¤ï¼ˆStreaming Assemblerï¼‰ï¼Œæ‰§è¡Œæ•ˆç‡æœ€é«˜ä½†ä¸ç¡¬ä»¶ç»‘å®šã€‚
- **PTX**ï¼šNVIDIA çš„è™šæ‹Ÿ ISAï¼ˆParallel Thread Executionï¼‰ï¼Œå¯åœ¨è¿è¡Œæ—¶ JIT åˆ°ç›®æ ‡ GPUï¼Œå…·å¤‡å¯ç§»æ¤æ€§ï¼›ä¿ç•™ PTX ä¾¿äºåç»­åœ¨æ›´æ–°çš„ GPU ä¸Šå¤ç”¨åŒä¸€äºŒè¿›åˆ¶ã€‚

---

## ğŸ“š å­¦ä¹ å†…å®¹

### Day 1-2: Flash-Attention ç®—æ³•ç†è§£

#### 1.1 æ ‡å‡† Attention çš„ç“¶é¢ˆ

**é—®é¢˜**ï¼š
- å†…å­˜å¤æ‚åº¦ï¼šO(NÂ²) çš„æ³¨æ„åŠ›çŸ©é˜µ
- è®¡ç®—å¤æ‚åº¦ï¼šO(NÂ²d) çš„çŸ©é˜µä¹˜æ³•
- å†…å­˜è®¿é—®ï¼šå¤§é‡ HBM è®¿é—®

**æ ‡å‡†å®ç°æµç¨‹**ï¼š
```
Q, K, V âˆˆ R^(NÃ—d)
S = QK^T âˆˆ R^(NÃ—N)      # æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
P = softmax(S) âˆˆ R^(NÃ—N) # æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
O = PV âˆˆ R^(NÃ—d)         # è¾“å‡º
```

**å†…å­˜å ç”¨**ï¼š
- Q, K, V: 3 Ã— N Ã— d
- S: N Ã— N
- P: N Ã— N
- O: N Ã— d
- **æ€»è®¡**ï¼šO(NÂ²) çš„å†…å­˜å ç”¨

#### 1.2 Flash-Attention æ ¸å¿ƒæ€æƒ³

**æ ¸å¿ƒåˆ›æ–°**ï¼š
1. **Tilingï¼ˆåˆ†å—ï¼‰**ï¼šå°† Qã€Kã€V åˆ†æˆå°å—
2. **Online Softmax**ï¼šåœ¨çº¿è®¡ç®— softmaxï¼Œé¿å…å­˜å‚¨å®Œæ•´çŸ©é˜µ
3. **å†…å­˜é‡è®¡ç®—**ï¼šåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼Œä¸å­˜å‚¨ä¸­é—´ç»“æœ

**ç®—æ³•æµç¨‹**ï¼š
```
å°† Q åˆ†æˆ T_r å—ï¼ŒKã€V åˆ†æˆ T_c å—
for i in range(T_r):
    q_i = Q[i]
    m_i = -âˆ
    l_i = 0
    o_i = 0
    for j in range(T_c):
        k_j = K[j]
        v_j = V[j]
        s_ij = q_i @ k_j^T
        m_ij = max(m_i, rowmax(s_ij))
        p_ij = exp(s_ij - m_ij)
        l_ij = sum(p_ij)
        # Online softmax update
        alpha = exp(m_i - m_ij)
        o_i = alpha * o_i + p_ij @ v_j
        l_i = alpha * l_i + l_ij
        m_i = m_ij
    O[i] = o_i / l_i
```

**å†…å­˜ä¼˜åŠ¿**ï¼š
- åªéœ€è¦å­˜å‚¨ O(N) çš„ä¸­é—´ç»“æœ
- é¿å…äº† O(NÂ²) çš„æ³¨æ„åŠ›çŸ©é˜µå­˜å‚¨

#### 1.3 Online Softmax æ•°å­¦åŸç†

**æ ‡å‡† Softmax**ï¼š
```
P = softmax(S) = exp(S - max(S)) / sum(exp(S - max(S)))
```

**Online Softmax**ï¼š
å¯¹äºä¸¤ä¸ªå— Sâ‚ å’Œ Sâ‚‚ï¼š
```
m = max(Sâ‚, Sâ‚‚)
P = [exp(Sâ‚ - m), exp(Sâ‚‚ - m)] / sum([exp(Sâ‚ - m), exp(Sâ‚‚ - m)])
```

**åœ¨çº¿æ›´æ–°å…¬å¼**ï¼š
```
m_new = max(m_old, m_new_block)
alpha = exp(m_old - m_new)
P_new = alpha * P_old + exp(S_new_block - m_new)
```

---

### Day 3: CUDA åŸºç¡€å›é¡¾

#### 3.1 å†…å­˜å±‚æ¬¡ç»“æ„

**GPU å†…å­˜å±‚æ¬¡**ï¼ˆä»å¿«åˆ°æ…¢ï¼‰ï¼š
1. **å¯„å­˜å™¨ï¼ˆRegistersï¼‰**ï¼šæœ€å¿«ï¼Œæ¯ä¸ªçº¿ç¨‹ç§æœ‰
2. **å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰**ï¼šå—å†…å…±äº«ï¼Œ~100TB/s
3. **L1/L2 ç¼“å­˜**ï¼šç¡¬ä»¶ç®¡ç†
4. **å…¨å±€å†…å­˜ï¼ˆGlobal Memoryï¼‰**ï¼šæœ€æ…¢ï¼Œ~1TB/s

**Flash-Attention çš„å†…å­˜ç­–ç•¥**ï¼š
- Qã€Kã€V å—å­˜å‚¨åœ¨å…±äº«å†…å­˜
- ä¸­é—´ç»“æœå­˜å‚¨åœ¨å¯„å­˜å™¨
- é¿å…å…¨å±€å†…å­˜çš„é¢‘ç¹è®¿é—®

#### 3.2 Warp å’Œçº¿ç¨‹å—

**Warp**ï¼š
- 32 ä¸ªçº¿ç¨‹ç»„æˆä¸€ä¸ª warp
- Warp æ˜¯ GPU è°ƒåº¦çš„åŸºæœ¬å•ä½
- Warp å†…çº¿ç¨‹æ‰§è¡Œ SIMTï¼ˆå•æŒ‡ä»¤å¤šçº¿ç¨‹ï¼‰

**çº¿ç¨‹å—ï¼ˆBlockï¼‰**ï¼š
- å¤šä¸ª warp ç»„æˆä¸€ä¸ªçº¿ç¨‹å—
- çº¿ç¨‹å—å†…å¯ä»¥å…±äº«å†…å­˜å’ŒåŒæ­¥
- Flash-Attention ä½¿ç”¨å¤šä¸ª warp åä½œ

#### 3.3 å†…å­˜åˆå¹¶è®¿é—®

**åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰**ï¼š
- ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜ä½ç½®
- ç¡¬ä»¶å¯ä»¥å°†å¤šä¸ªè®¿é—®åˆå¹¶ä¸ºä¸€ä¸ªäº‹åŠ¡
- æ˜¾è‘—æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

**Flash-Attention çš„è®¿é—®æ¨¡å¼**ï¼š
- Qã€Kã€V çš„åŠ è½½éœ€è¦åˆå¹¶è®¿é—®
- ä½¿ç”¨ CUTLASS åº“ä¼˜åŒ–è®¿é—®æ¨¡å¼

#### 3.4 çº¿ç¨‹å—ä¸ç½‘æ ¼å°ºå¯¸çš„ 2D è§„åˆ’

**ç¤ºä¾‹**ï¼šå‡è®¾ `blockDim = (4, 4, 1)`ï¼Œå³æ¯ä¸ªçº¿ç¨‹å—åŒ…å« 16 ä¸ªçº¿ç¨‹ï¼›è‹¥ä¸€å…±éœ€è¦å¤„ç† 1000 ä¸ªçº¿ç¨‹ï¼š
1. è®¡ç®—æ‰€éœ€çº¿ç¨‹å—æ•°é‡ï¼š`num_blocks = (1000 + 16 - 1) / 16 = 63`ï¼ˆå‘ä¸Šå–æ•´ï¼‰ã€‚
2. å°† 63 ä¸ªçº¿ç¨‹å—æ˜ å°„åˆ° 2D ç½‘æ ¼ï¼Œä¾‹å¦‚ï¼š
   - é€‰æ‹© `gridDim.x = 8`ï¼Œåˆ™ `gridDim.y = (63 + 8 - 1) / 8 = 8`ï¼Œå¾—åˆ° `gridDim = (8, 8, 1)`ï¼Œå…± 64 ä¸ªçº¿ç¨‹å—è¦†ç›–éœ€æ±‚ã€‚
   - æˆ–è€…å…ˆå®š `gridDim.x = 16`ï¼Œè®¡ç®— `gridDim.y = (63 + 16 - 1) / 16 = 4`ï¼Œå¾—åˆ° `gridDim = (16, 4, 1)`ã€‚
3. è§„å¾‹ï¼š
   - `gridDim.x` é€šå¸¸æŒ‘é€‰ä¸º `blockDim.x` çš„å€æ•°æˆ–æ»¡è¶³å†…å­˜è®¿é—®æ¨¡å¼çš„å°ºå¯¸ã€‚
   - `gridDim.y = (num_blocks + gridDim.x - 1) / gridDim.x`ï¼Œç¡®ä¿ `gridDim.x * gridDim.y â‰¥ num_blocks`ã€‚

è¿™æ ·æ—¢èƒ½ä¿æŒäºŒç»´åˆ†å¸ƒçš„å‡è¡¡ï¼Œåˆèƒ½ä¿è¯çº¿ç¨‹æ€»æ•°è¦†ç›–ä»»åŠ¡ã€‚

---

### Day 4: CUTLASS å’Œ CuTe å…¥é—¨

#### 4.1 CUTLASS ç®€ä»‹

**CUTLASS**ï¼š
- NVIDIA çš„ CUDA C++ æ¨¡æ¿åº“
- æä¾›é«˜æ€§èƒ½çš„ GEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰å®ç°
- æ”¯æŒ Tensor Core

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- **Tile**ï¼šè®¡ç®—çš„åŸºæœ¬å•ä½
- **Thread Block Tile**ï¼šçº¿ç¨‹å—å¤„ç†çš„å—å¤§å°
- **Warp Tile**ï¼šWarp å¤„ç†çš„å—å¤§å°
- **Thread Tile**ï¼šçº¿ç¨‹å¤„ç†çš„å—å¤§å°

#### 4.2 CuTe å¼ é‡æŠ½è±¡

**CuTe**ï¼š
- CUTLASS çš„ä¸€éƒ¨åˆ†
- æä¾›å¼ é‡æŠ½è±¡å’Œå¸ƒå±€æè¿°
- ç®€åŒ–å†…å­˜è®¿é—®æ¨¡å¼çš„å®šä¹‰

**å…³é”®æ¦‚å¿µ**ï¼š
- **Tensor**ï¼šå¤šç»´æ•°ç»„çš„æŠ½è±¡
- **Layout**ï¼šæè¿°æ•°æ®åœ¨å†…å­˜ä¸­çš„æ’åˆ—
- **Shape**ï¼šå¼ é‡çš„ç»´åº¦
- **Stride**ï¼šæ¯ä¸ªç»´åº¦çš„æ­¥é•¿

**ç¤ºä¾‹**ï¼š
```cuda
// å®šä¹‰ä¸€ä¸ª 2D å¼ é‡
auto tensor = make_tensor(
    make_gmem_ptr(data_ptr),
    make_shape(M, N),
    make_stride(stride_M, stride_N)
);
```

---

### Day 5: å¼€å‘ç¯å¢ƒæ­å»º

#### 5.1 ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶è¦æ±‚**ï¼š
- NVIDIA GPUï¼ˆæ”¯æŒ SM 7.5+ï¼‰
  - RTX 20/30/40 ç³»åˆ—
  - A100, H100
  - è‡³å°‘ 8GB æ˜¾å­˜

**è½¯ä»¶è¦æ±‚**ï¼š
- CUDA Toolkit >= 11.8
- Python >= 3.8
- PyTorch >= 2.0
- GCC >= 7.5

#### 5.2 å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 3. å®‰è£…ä¾èµ–
pip install packaging ninja
pip install torch torchvision torchaudio

# 4. ç¼–è¯‘å®‰è£…
python setup.py install

# 5. éªŒè¯å®‰è£…
python -c "import flash_attn; print('Flash-Attention installed successfully')"
```

#### 5.3 è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
import torch
import flash_attn

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
seq_len = 1024
num_heads = 12
head_dim = 64

q = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
k = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
v = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)

# ä½¿ç”¨ Flash-Attention
out = flash_attn.flash_attn_func(q, k, v)
print(f"Output shape: {out.shape}")
```

---

## ğŸ“ å­¦ä¹ ä»»åŠ¡

### å¿…åšä»»åŠ¡

1. **é˜…è¯»è®ºæ–‡**ï¼š
   - [ ] FlashAttention è®ºæ–‡ï¼ˆè‡³å°‘é˜…è¯»ç®—æ³•éƒ¨åˆ†ï¼‰
   - [ ] FlashAttention-2 è®ºæ–‡ï¼ˆç†è§£æ”¹è¿›ç‚¹ï¼‰

2. **ç®—æ³•ç†è§£**ï¼š
   - [ ] æ‰‹æ¨ Online Softmax çš„æ•°å­¦å…¬å¼
   - [ ] ç†è§£ Tiling ç­–ç•¥çš„ä½œç”¨
   - [ ] åˆ†æå†…å­˜å¤æ‚åº¦

3. **ä»£ç å®è·µ**ï¼š
   - [ ] æ­å»ºå¼€å‘ç¯å¢ƒ
   - [ ] è¿è¡Œç¤ºä¾‹ä»£ç 
   - [ ] å¯¹æ¯”æ ‡å‡† Attention å’Œ Flash-Attention çš„å†…å­˜å ç”¨

### é€‰åšä»»åŠ¡

1. **å®ç°ç®€åŒ–ç‰ˆ**ï¼š
   - [ ] å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Online Softmax
   - [ ] å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Tiled Attention

2. **æ€§èƒ½æµ‹è¯•**ï¼š
   - [ ] æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦çš„æ€§èƒ½
   - [ ] å¯¹æ¯”å†…å­˜å ç”¨

---

## ğŸ¯ å­¦ä¹ æ£€æŸ¥ç‚¹

### ç®—æ³•ç†è§£æ£€æŸ¥

- [ ] èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆæ ‡å‡† Attention éœ€è¦ O(NÂ²) å†…å­˜
- [ ] èƒ½å¤Ÿè§£é‡Š Flash-Attention å¦‚ä½•å°†å†…å­˜é™åˆ° O(N)
- [ ] èƒ½å¤Ÿæ¨å¯¼ Online Softmax çš„æ›´æ–°å…¬å¼
- [ ] ç†è§£ Tiling ç­–ç•¥å¦‚ä½•å‡å°‘å†…å­˜è®¿é—®

### CUDA åŸºç¡€æ£€æŸ¥

- [ ] ç†è§£ GPU å†…å­˜å±‚æ¬¡ç»“æ„
- [ ] ç†è§£ Warp å’Œçº¿ç¨‹å—çš„æ¦‚å¿µ
- [ ] ç†è§£å†…å­˜åˆå¹¶è®¿é—®çš„é‡è¦æ€§
- [ ] äº†è§£ CUTLASS å’Œ CuTe çš„åŸºæœ¬æ¦‚å¿µ

### å®è·µæ£€æŸ¥

- [ ] æˆåŠŸæ­å»ºå¼€å‘ç¯å¢ƒ
- [ ] èƒ½å¤Ÿè¿è¡Œ Flash-Attention ç¤ºä¾‹
- [ ] èƒ½å¤Ÿä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·

---

## ğŸ“– å‚è€ƒèµ„æº

### è®ºæ–‡
- FlashAttention: https://arxiv.org/abs/2205.14135
- FlashAttention-2: https://tridao.me/publications/flash2/flash2.pdf

### æ–‡æ¡£
- CUDA C++ Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- CUTLASS æ–‡æ¡£: https://github.com/NVIDIA/cutlass

### è§†é¢‘æ•™ç¨‹
- Flash-Attention ä½œè€…è®²è§£ï¼ˆå¦‚æœæœ‰ï¼‰

---

## ğŸ“ å­¦ä¹ ç¬”è®°æ¨¡æ¿

### Flash-Attention ç®—æ³•ç¬”è®°

```markdown
# Flash-Attention ç®—æ³•ç¬”è®°

## æ ¸å¿ƒæ€æƒ³
- [æ ¸å¿ƒæ€æƒ³1]
- [æ ¸å¿ƒæ€æƒ³2]

## ç®—æ³•æµç¨‹
1. [æ­¥éª¤1]
2. [æ­¥éª¤2]

## æ•°å­¦æ¨å¯¼
- [å…¬å¼1]
- [å…¬å¼2]

## å†…å­˜åˆ†æ
- æ ‡å‡†å®ç°ï¼šO(?)
- Flash-Attentionï¼šO(?)

## ç–‘é—®
- [ç–‘é—®1]
- [ç–‘é—®2]
```

---

## ğŸš€ ä¸‹ä¸€å‘¨é¢„å‘Š

**ç¬¬2å‘¨ï¼šä»£ç ç»“æ„åˆ†æ**
- Python æ¥å£å±‚åˆ†æ
- å‚æ•°ç»“æ„ç†è§£
- å†…æ ¸å¯åŠ¨æœºåˆ¶
- å†…æ ¸ç‰¹æ€§ç³»ç»Ÿ

---

**å®Œæˆæ—¶é—´**ï¼šç¬¬1å‘¨  
**é¢„è®¡ç”¨æ—¶**ï¼š20-25 å°æ—¶  
**éš¾åº¦**ï¼šâ­â­â­â˜†â˜†
