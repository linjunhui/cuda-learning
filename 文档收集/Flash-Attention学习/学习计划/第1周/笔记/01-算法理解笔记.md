# Flash-Attention 算法理解笔记

## 📝 学习日期
**开始日期**：________  
**完成日期**：________

---

## 🎯 学习目标回顾

1. 理解标准 Attention 的瓶颈
2. 掌握 Flash-Attention 的核心思想
3. 理解 Online Softmax 的数学原理
4. 理解 Tiling 策略的作用

### 🔧 编译提示
在本周涉及 CUDA 的实验中，如需同时支持 Ampere 与 Hopper，可使用以下命令生成对应算力的 SASS 与 PTX：

```bash
nvcc main.cu -o main \
  --generate-code=arch=compute_80,code=[sm_80,compute_80] \
  --generate-code=arch=compute_90,code=[sm_90,compute_90]
```

- `sm_80` / `compute_80`：针对 Ampere（如 A100）。
- `sm_90` / `compute_90`：针对 Hopper（如 H100）。
- `code=[sm_xx,compute_xx]` 同时输出本地 SASS 与通用 PTX，方便移植或 JIT。
- **SASS（Streaming Assembler）**：针对指定 `sm_xx` 的最终机器码，执行效率高但与硬件强绑定。
- **PTX（Parallel Thread Execution）**：NVIDIA 的中间表示，可在运行时 JIT 到目标 GPU，提供跨架构的灵活性。

---

## 📚 标准 Attention 分析

### 算法流程

```
输入：Q (N×d), K (N×d), V (N×d)
步骤1：S = QK^T  (N×N)
步骤2：P = softmax(S)  (N×N)
步骤3：O = PV  (N×d)
```

### 内存复杂度

**我的理解**：
- Q, K, V: 3 × N × d
- S 矩阵: N × N（这是主要瓶颈）
- P 矩阵: N × N（这也是瓶颈）
- O: N × d

**总内存**：O(N²)，当 N 很大时（如 32K），内存占用巨大

### 实际例子

**GPT-3 规模**：
- N = 2048
- d = 128
- h = 96

**内存占用**：
- S 矩阵：2048 × 2048 × 4 bytes = 16 MB（每个 head）
- P 矩阵：2048 × 2048 × 4 bytes = 16 MB（每个 head）
- 总计：32 MB × 96 heads = 3 GB

**问题**：
- GPU 显存有限
- 内存访问成为瓶颈

---

## ✅ Flash-Attention 核心思想

### 三个关键技术

1. **Tiling（分块）**
   - 将 Q、K、V 分成小块
   - 避免存储完整的注意力矩阵

2. **Online Softmax**
   - 在线计算 softmax
   - 避免存储完整的 S 和 P 矩阵

3. **内存重计算**
   - 反向传播时重新计算
   - 不存储中间激活值

### 算法流程（我的理解）

```
将 Q 分成 T_r 块，K、V 分成 T_c 块
for i in range(T_r):
    q_i = Q[i]
    m_i = -∞
    l_i = 0
    o_i = 0
    for j in range(T_c):
        k_j = K[j]
        v_j = V[j]
        s_ij = q_i @ k_j^T
        # Online Softmax 更新
        m_ij = max(m_i, rowmax(s_ij))
        p_ij = exp(s_ij - m_ij)
        l_ij = sum(p_ij)
        alpha = exp(m_i - m_ij)
        o_i = alpha * o_i + p_ij @ v_j
        l_i = alpha * l_i + l_ij
        m_i = m_ij
    O[i] = o_i / l_i
```

### 内存优势

**Flash-Attention 内存占用**：
- 只需要存储块大小的中间结果
- 总内存：O(block_size × d) = O(1)（相对于 N）

**优势**：
- 从 O(N²) 降到 O(N)
- 可以处理更长的序列

---

## 🧮 Online Softmax 数学推导

### 标准 Softmax

对于向量 **s** = [s₁, s₂, ..., sₙ]：

```
P[i] = exp(s[i] - m) / Σⱼ exp(s[j] - m)
其中 m = max(s)
```

### Online Softmax 更新规则

**关键思想**：维护三个统计量
- **m**：当前最大值
- **l**：归一化因子的分子
- **o**：加权和

**更新公式**（我的推导）：

```
m_new = max(m_old, max(s_new_block))
alpha = exp(m_old - m_new)
l_new = alpha * l_old + sum(exp(s_new_block - m_new))
o_new = alpha * o_old + exp(s_new_block - m_new) @ v_new_block
```

**最终归一化**：
```
O_final = o_new / l_new
```

### 我的理解

**为什么需要 alpha**：
- 当新的最大值出现时，旧的 exp 值需要缩放
- alpha = exp(m_old - m_new) 就是这个缩放因子

**数值稳定性**：
- 减去最大值：exp(s - m) 而不是 exp(s)
- 避免溢出

---

## 🧩 Tiling 策略

### Tiling 的作用

**目标**：
1. 每个块可以放入共享内存
2. 减少全局内存访问
3. 提高内存访问效率

### 块大小选择

**考虑因素**：
1. 共享内存大小（48KB 或 164KB）
2. 寄存器数量
3. Warp 数量
4. 内存带宽

**常见配置**：
- block_size_M = 64 或 128
- block_size_N = 64 或 128
- head_dim = 64, 128, 192, 256

### 内存访问模式

**标准实现**：
```
Q: HBM → 寄存器 → 计算
K: HBM → 寄存器 → 计算
S: 寄存器 → HBM (写入) ❌
P: HBM → 寄存器 → 计算
O: 寄存器 → HBM (写入)
```

**Flash-Attention**：
```
Q 块: HBM → 共享内存 → 寄存器 → 计算
K 块: HBM → 共享内存 → 寄存器 → 计算
中间结果: 寄存器 → 寄存器 (不写回 HBM) ✅
O: 寄存器 → HBM (只写一次) ✅
```

#### 3.3 内存合并访问

**合并访问（Coalesced Access）**：
- 相邻线程访问相邻内存位置
- 硬件可以将多个访问合并为一个事务
- 显著提高内存带宽利用率

**Flash-Attention 的访问模式**：
- Q、K、V 的加载需要合并访问
- 使用 CUTLASS 库优化访问模式

#### 3.4 线程块/网格的 2D 配置示例

- 设定 `blockDim = (4, 4, 1)`，每个线程块共有 `4 × 4 = 16` 个线程。
- 需要覆盖 1000 个线程时：
  1. 线程块数量：`num_blocks = (1000 + 16 - 1) / 16 = 63`（向上取整）。
  2. 将 63 个块映射到二维网格：
     - 选择 `gridDim.x = 8`，则 `gridDim.y = (63 + 8 - 1) / 8 = 8`，即 `gridDim = (8, 8, 1)`；
     - 或者取 `gridDim.x = 16`，则 `gridDim.y = (63 + 16 - 1) / 16 = 4`，即 `gridDim = (16, 4, 1)`。
  3. 核心思路：`gridDim.x * gridDim.y` 覆盖所需块数即可，可根据访问模式或数据布局调整 `x/y` 比例。

---

## 💡 关键洞察

### 我的理解

1. **内存是瓶颈**：
   - 计算能力足够，但内存带宽有限
   - Flash-Attention 通过减少内存访问来提高性能

2. **Online Softmax 是关键**：
   - 允许我们在线计算，不需要存储完整矩阵
   - 数学上等价，但内存效率高得多

3. **Tiling 是基础**：
   - 将大问题分解成小问题
   - 每个小问题可以高效处理

---

## ❓ 疑问和待深入

### 疑问

1. **Online Softmax 的数值精度**：
   - 多次更新是否会累积误差？
   - 如何保证数值稳定性？

2. **Tiling 策略的优化**：
   - 如何选择最优的块大小？
   - 不同硬件是否有不同的最优配置？

3. **反向传播**：
   - 内存重计算如何实现？
   - 如何平衡计算和内存？

### 待深入学习

- [ ] 深入理解 Online Softmax 的数学证明
- [ ] 分析不同块大小的性能影响
- [ ] 学习反向传播的实现
- [ ] 理解硬件特性对性能的影响

---

## 📊 学习总结

### 核心概念

1. **Flash-Attention 的核心**：Tiling + Online Softmax
2. **内存优化**：从 O(N²) 降到 O(N)
3. **性能提升**：减少内存访问，提高计算效率

### 关键公式

1. **Online Softmax 更新**：
   ```
   m_new = max(m_old, max(s_new))
   alpha = exp(m_old - m_new)
   l_new = alpha * l_old + sum(exp(s_new - m_new))
   o_new = alpha * o_old + exp(s_new - m_new) @ v_new
   ```

2. **最终输出**：
   ```
   O = o_new / l_new
   ```

---

## ✅ 学习检查

- [x] 理解标准 Attention 的瓶颈
- [x] 理解 Flash-Attention 的核心思想
- [x] 理解 Online Softmax 的基本原理
- [x] 理解 Tiling 策略的作用
- [ ] 能够独立推导 Online Softmax 公式
- [ ] 能够分析不同配置的性能影响

---

## 📚 参考资源

- FlashAttention 论文
- FlashAttention-2 论文
- 相关博客文章

---

**笔记完成时间**：________

