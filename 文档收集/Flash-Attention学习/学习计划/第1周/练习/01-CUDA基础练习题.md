# CUDA åŸºç¡€ç»ƒä¹ é¢˜

## ğŸ“ è¯´æ˜

æœ¬ç»ƒä¹ åŒ…å«ç¬¬1å‘¨ CUDA åŸºç¡€çŸ¥è¯†çš„é…å¥—é¢˜ç›®ï¼Œæ¯ä¸ªé¢˜ç›®å¯¹åº”ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼Œéš¾åº¦é€‚ä¸­ï¼Œé€‚åˆå·©å›ºå­¦ä¹ ã€‚

### ç¼–è¯‘è¯´æ˜
å¦‚éœ€åŒæ—¶ä¸ºä¸¤ç§ç®—åŠ›æ¶æ„ç”ŸæˆäºŒè¿›åˆ¶ä¸ PTXï¼Œå¯ä½¿ç”¨ç±»ä¼¼å¦‚ä¸‹çš„ `nvcc` å‘½ä»¤ï¼š

```bash
nvcc main.cu -o main \
  --generate-code=arch=compute_80,code=[sm_80,compute_80] \
  --generate-code=arch=compute_90,code=[sm_90,compute_90]
```

- `compute_80` / `sm_80` å¯¹åº” Ampereï¼ˆå¦‚ A100ï¼‰ï¼Œ`compute_90` / `sm_90` å¯¹åº” Hopperï¼ˆå¦‚ H100ï¼‰ã€‚
- `code=[sm_xx,compute_xx]` ä¼šåŒæ—¶ç”Ÿæˆè¯¥æ¶æ„çš„ SASS ä»¥åŠä¿ç•™ PTXï¼Œæ–¹ä¾¿åç»­åœ¨å…¼å®¹ GPU ä¸Š JITã€‚
- å¦‚æœåªéœ€ PTXï¼Œå¯æ”¹ä¸º `code=compute_xx`ï¼›å¦‚åªéœ€ç‰¹å®šæ¶æ„çš„ SASSï¼Œåˆ™æ”¹ä¸º `code=sm_xx`ã€‚

**SASS ä¸ PTX çŸ¥è¯†ç‚¹è¡¥å……**
- **PTXï¼ˆParallel Thread Executionï¼‰**ï¼šNVIDIA çš„ä¸­é—´è¡¨ç¤ºï¼Œç±»ä¼¼â€œè™šæ‹Ÿ ISAâ€ï¼Œå¯åœ¨è¿è¡Œæ—¶é’ˆå¯¹ç›®æ ‡ GPU é‡æ–° JIT ç¼–è¯‘ï¼Œå…·å¤‡è‰¯å¥½çš„å¯ç§»æ¤æ€§ã€‚
- **SASSï¼ˆStreaming Assemblerï¼‰**ï¼šé¢å‘ç‰¹å®š `sm_xx` æ¶æ„çš„æœ€ç»ˆæœºå™¨æŒ‡ä»¤ï¼Œç›´æ¥ç”± GPU æ‰§è¡Œï¼Œæ€§èƒ½å¯æ§ä½†ä¸ç¡¬ä»¶å¼ºç»‘å®šã€‚
- é€šå¸¸åŒæ—¶ç”Ÿæˆ `sm_xx` ä¸ `compute_xx`ï¼Œæ—¢ä¿è¯å½“å‰ç¡¬ä»¶çš„æœ€ä¼˜æ€§èƒ½ï¼Œåˆä¿ç•™æœªæ¥åœ¨ç›¸è¿‘æ¶æ„ä¸Šé‡æ–°ç¼–è¯‘çš„çµæ´»æ€§ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šCUDA ç¼–ç¨‹æ¨¡å‹

### é¢˜ç›® 1ï¼šä¸»æœº-è®¾å¤‡æ¶æ„ç†è§£

**çŸ¥è¯†ç‚¹**ï¼šä¸»æœº-è®¾å¤‡æ¶æ„

**é¢˜ç›®**ï¼š
åœ¨ CUDA ç¨‹åºä¸­ï¼Œä»¥ä¸‹å“ªäº›æ“ä½œåœ¨ä¸»æœºï¼ˆCPUï¼‰ä¸Šæ‰§è¡Œï¼Œå“ªäº›åœ¨è®¾å¤‡ï¼ˆGPUï¼‰ä¸Šæ‰§è¡Œï¼Ÿ

```cuda
// A. åˆ†é…ä¸»æœºå†…å­˜
float* h_data = (float*)malloc(1024 * sizeof(float));

// B. åˆ†é…è®¾å¤‡å†…å­˜
float* d_data;
cudaMalloc(&d_data, 1024 * sizeof(float));

// C. å†…æ ¸å‡½æ•°æ‰§è¡Œ
__global__ void kernel(float* data) {
    data[threadIdx.x] = 1.0f;
}

// D. æ•°æ®å¤åˆ¶
cudaMemcpy(d_data, h_data, 1024 * sizeof(float), cudaMemcpyHostToDevice);
```

**ç­”æ¡ˆ**ï¼š
- A. ä¸»æœºï¼ˆCPUï¼‰
- B. ä¸»æœºï¼ˆCPUï¼‰- è°ƒç”¨åœ¨ä¸»æœºï¼Œå®é™…åˆ†é…åœ¨è®¾å¤‡
- C. è®¾å¤‡ï¼ˆGPUï¼‰
- D. ä¸»æœºï¼ˆCPUï¼‰- è°ƒç”¨åœ¨ä¸»æœºï¼Œæ•°æ®ä¼ è¾“ç”±é©±åŠ¨å®Œæˆ

---

### é¢˜ç›® 2ï¼šCUDA ç¨‹åºç»“æ„

**çŸ¥è¯†ç‚¹**ï¼šCUDA ç¨‹åºåŸºæœ¬ç»“æ„

**é¢˜ç›®**ï¼š
è¯·å°†ä»¥ä¸‹ CUDA ç¨‹åºæ­¥éª¤æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼š

1. å¯åŠ¨å†…æ ¸å‡½æ•°
2. åˆ†é…è®¾å¤‡å†…å­˜
3. å¤åˆ¶æ•°æ®å›ä¸»æœº
4. åˆ†é…ä¸»æœºå†…å­˜
5. å¤åˆ¶æ•°æ®åˆ°è®¾å¤‡
6. é‡Šæ”¾å†…å­˜

**ç­”æ¡ˆ**ï¼š
4 â†’ 2 â†’ 5 â†’ 1 â†’ 3 â†’ 6

**æ ‡å‡†æµç¨‹**ï¼š
```cuda
// 1. åˆ†é…ä¸»æœºå†…å­˜
float* h_data = (float*)malloc(1024 * sizeof(float));

// 2. åˆ†é…è®¾å¤‡å†…å­˜
float* d_data;
cudaMalloc(&d_data, 1024 * sizeof(float));

// 3. å¤åˆ¶æ•°æ®åˆ°è®¾å¤‡
cudaMemcpy(d_data, h_data, 1024 * sizeof(float), cudaMemcpyHostToDevice);

// 4. å¯åŠ¨å†…æ ¸å‡½æ•°
kernel<<<4, 256>>>(d_data);

// 5. å¤åˆ¶æ•°æ®å›ä¸»æœº
cudaMemcpy(h_data, d_data, 1024 * sizeof(float), cudaMemcpyDeviceToHost);

// 6. é‡Šæ”¾å†…å­˜
cudaFree(d_data);
free(h_data);
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šçº¿ç¨‹å±‚æ¬¡ç»“æ„

### é¢˜ç›® 3ï¼šçº¿ç¨‹æ ‡è¯†ç¬¦

**çŸ¥è¯†ç‚¹**ï¼šthreadIdx, blockIdx, blockDim, gridDim

**é¢˜ç›®**ï¼š
ç»™å®šä»¥ä¸‹å†…æ ¸å¯åŠ¨é…ç½®ï¼š
```cuda
kernel<<<4, 256>>>(data);
```

è¯·å¡«å†™ä»¥ä¸‹çº¿ç¨‹çš„æ ‡è¯†ç¬¦å€¼ï¼š

| çº¿ç¨‹ | blockIdx.x | threadIdx.x | blockDim.x | gridDim.x |
|------|------------|-------------|------------|-----------|
| çº¿ç¨‹ 0ï¼ˆå— 0ï¼‰ | ? | ? | ? | ? |
| çº¿ç¨‹ 100ï¼ˆå— 0ï¼‰ | ? | ? | ? | ? |
| çº¿ç¨‹ 0ï¼ˆå— 2ï¼‰ | ? | ? | ? | ? |

**ç­”æ¡ˆ**ï¼š

| çº¿ç¨‹ | blockIdx.x | threadIdx.x | blockDim.x | gridDim.x |
|------|------------|-------------|------------|-----------|
| çº¿ç¨‹ 0ï¼ˆå— 0ï¼‰ | 0 | 0 | 256 | 4 |
| çº¿ç¨‹ 100ï¼ˆå— 0ï¼‰ | 0 | 100 | 256 | 4 |
| çº¿ç¨‹ 0ï¼ˆå— 2ï¼‰ | 2 | 0 | 256 | 4 |

---

### é¢˜ç›® 4ï¼šå…¨å±€çº¿ç¨‹ ID è®¡ç®—ï¼ˆ1Dï¼‰

**çŸ¥è¯†ç‚¹**ï¼šå…¨å±€çº¿ç¨‹ ID è®¡ç®—

**é¢˜ç›®**ï¼š
ç»™å®šå†…æ ¸å¯åŠ¨é…ç½® `kernel<<<4, 256>>>(data)`ï¼Œè¯·è®¡ç®—ä»¥ä¸‹çº¿ç¨‹çš„å…¨å±€çº¿ç¨‹ IDï¼š

1. blockIdx.x = 0, threadIdx.x = 10
2. blockIdx.x = 2, threadIdx.x = 100
3. blockIdx.x = 3, threadIdx.x = 255

**ç­”æ¡ˆ**ï¼š

å…¨å±€çº¿ç¨‹ ID è®¡ç®—å…¬å¼ï¼š`global_id = blockIdx.x * blockDim.x + threadIdx.x`

1. global_id = 0 Ã— 256 + 10 = **10**
2. global_id = 2 Ã— 256 + 100 = **612**
3. global_id = 3 Ã— 256 + 255 = **1023**

---

### é¢˜ç›® 5ï¼šå…¨å±€çº¿ç¨‹ ID è®¡ç®—ï¼ˆ2Dï¼‰

**çŸ¥è¯†ç‚¹**ï¼š2D çº¿ç¨‹ç´¢å¼•è®¡ç®—

**é¢˜ç›®**ï¼š
ç»™å®š 2D å†…æ ¸å¯åŠ¨é…ç½®ï¼š
```cuda
dim3 gridDim(4, 3);    // 4Ã—3 çš„ç½‘æ ¼
dim3 blockDim(8, 8);   // 8Ã—8 çš„çº¿ç¨‹å—
kernel<<<gridDim, blockDim>>>(data);
```

è¯·è®¡ç®—ä»¥ä¸‹çº¿ç¨‹çš„å…¨å±€ 2D åæ ‡ï¼š
1. blockIdx = (1, 1), threadIdx = (2, 3)
2. blockIdx = (3, 2), threadIdx = (7, 7)

**è¡¥å……ä¾‹å­**ï¼š
- å‡è®¾ `blockDim = (4, 4, 1)`ï¼Œæ¯ä¸ªçº¿ç¨‹å—æœ‰ 16 ä¸ªçº¿ç¨‹ï¼›è‹¥å…±æœ‰ 1000 ä¸ªçº¿ç¨‹ï¼š
  1. çº¿ç¨‹å—æ•°é‡ï¼š`num_blocks = (1000 + 16 - 1) / 16 = 63`ï¼›
  2. å¯ä»¥é€‰æ‹© `gridDim = (8, 8, 1)`ï¼Œå› ä¸º `8 Ã— 8 = 64 â‰¥ 63`ï¼›
  3. æˆ–é€‰æ‹© `gridDim = (16, 4, 1)`ï¼Œå› ä¸º `16 Ã— 4 = 64 â‰¥ 63`ï¼›
  4. ä¸€èˆ¬å…¬å¼ï¼š`gridDim.x` å…ˆè¡Œç¡®å®šï¼Œ`gridDim.y = (num_blocks + gridDim.x - 1) / gridDim.x`ï¼ˆå‘ä¸Šå–æ•´ï¼‰ï¼Œç¡®ä¿ `gridDim.x * gridDim.y â‰¥ num_blocks`ã€‚

**ç­”æ¡ˆ**ï¼š

å…¨å±€åæ ‡è®¡ç®—å…¬å¼ï¼š
- `global_x = blockIdx.x * blockDim.x + threadIdx.x`
- `global_y = blockIdx.y * blockDim.y + threadIdx.y`

1. global_x = 1 Ã— 8 + 2 = **10**, global_y = 1 Ã— 8 + 3 = **11**
2. global_x = 3 Ã— 8 + 7 = **31**, global_y = 2 Ã— 8 + 7 = **23**

---

### é¢˜ç›® 6ï¼šWarp æ¦‚å¿µ

**çŸ¥è¯†ç‚¹**ï¼šWarp å’Œ Lane ID

**é¢˜ç›®**ï¼š
åœ¨ä¸€ä¸ªçº¿ç¨‹å—ä¸­ï¼Œæœ‰ 128 ä¸ªçº¿ç¨‹ï¼ˆthreadIdx.x = 0 åˆ° 127ï¼‰ï¼Œè¯·å›ç­”ï¼š

1. è¿™ä¸ªçº¿ç¨‹å—åŒ…å«å¤šå°‘ä¸ª warpï¼Ÿ
2. threadIdx.x = 35 çš„çº¿ç¨‹å±äºç¬¬å‡ ä¸ª warpï¼Ÿ
3. è¯¥çº¿ç¨‹åœ¨è¯¥ warp ä¸­çš„ lane ID æ˜¯å¤šå°‘ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

1. 128 / 32 = **4 ä¸ª warp**
2. warp_id = 35 / 32 = **1**ï¼ˆç¬¬ 2 ä¸ª warpï¼Œä» 0 å¼€å§‹ï¼‰
3. lane_id = 35 % 32 = **3**

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šGPU å†…å­˜å±‚æ¬¡ç»“æ„

### é¢˜ç›® 7ï¼šå†…å­˜å±‚æ¬¡ç»“æ„

**çŸ¥è¯†ç‚¹**ï¼šGPU å†…å­˜å±‚æ¬¡ï¼ˆå¯„å­˜å™¨ã€å…±äº«å†…å­˜ã€å…¨å±€å†…å­˜ï¼‰

**é¢˜ç›®**ï¼š
è¯·å°†ä»¥ä¸‹å†…å­˜ç±»å‹æŒ‰è®¿é—®é€Ÿåº¦ä»å¿«åˆ°æ…¢æ’åºï¼š

A. å…¨å±€å†…å­˜ï¼ˆGlobal Memoryï¼‰
B. å¯„å­˜å™¨ï¼ˆRegistersï¼‰
C. å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰
D. L2 ç¼“å­˜ï¼ˆL2 Cacheï¼‰

**ç­”æ¡ˆ**ï¼š
**Bï¼ˆå¯„å­˜å™¨ï¼‰ > Cï¼ˆå…±äº«å†…å­˜ï¼‰ > Dï¼ˆL2 ç¼“å­˜ï¼‰ > Aï¼ˆå…¨å±€å†…å­˜ï¼‰**

**é€Ÿåº¦å¯¹æ¯”**ï¼ˆè¿‘ä¼¼ï¼‰ï¼š
- å¯„å­˜å™¨ï¼š~1000 TB/s
- å…±äº«å†…å­˜ï¼š~100 TB/s
- L2 ç¼“å­˜ï¼š~1 TB/s
- å…¨å±€å†…å­˜ï¼š~1 TB/sï¼ˆä½†å»¶è¿Ÿæ›´é«˜ï¼‰

---

### é¢˜ç›® 8ï¼šå¯„å­˜å™¨ä½¿ç”¨

**çŸ¥è¯†ç‚¹**ï¼šå¯„å­˜å™¨å­˜å‚¨

**é¢˜ç›®**ï¼š
ä»¥ä¸‹ä»£ç ä¸­ï¼Œå“ªäº›å˜é‡å­˜å‚¨åœ¨å¯„å­˜å™¨ä¸­ï¼Ÿ

```cuda
__global__ void kernel(float* data) {
    float a = 1.0f;           // A
    float b = 2.0f;           // B
    float c = a + b;          // C
    __shared__ float s[256];   // D
    s[threadIdx.x] = c;       // E
}
```

**ç­”æ¡ˆ**ï¼š
- **A, B, C** å­˜å‚¨åœ¨å¯„å­˜å™¨ä¸­
- **D** å­˜å‚¨åœ¨å…±äº«å†…å­˜ä¸­
- **E** æ˜¯èµ‹å€¼æ“ä½œï¼Œ`c` çš„å€¼ä»å¯„å­˜å™¨å¤åˆ¶åˆ°å…±äº«å†…å­˜

---

### é¢˜ç›® 9ï¼šå…±äº«å†…å­˜å£°æ˜

**çŸ¥è¯†ç‚¹**ï¼šå…±äº«å†…å­˜ä½¿ç”¨

**é¢˜ç›®**ï¼š
è¯·ä¿®æ­£ä»¥ä¸‹ä»£ç ä¸­çš„é”™è¯¯ï¼š

```cuda
__global__ void kernel(float* data) {
    float shared_data[256];  // é”™è¯¯ï¼šç¼ºå°‘ __shared__ å…³é”®å­—
    
    int tid = threadIdx.x;
    shared_data[tid] = data[tid];
    // ... ä½¿ç”¨ shared_data
}
```

**ç­”æ¡ˆ**ï¼š
```cuda
__global__ void kernel(float* data) {
    __shared__ float shared_data[256];  // æ­£ç¡®ï¼šæ·»åŠ  __shared__ å…³é”®å­—
    
    int tid = threadIdx.x;
    shared_data[tid] = data[tid];
    __syncthreads();  // å»ºè®®ï¼šæ·»åŠ åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰çº¿ç¨‹å®Œæˆå†™å…¥
    // ... ä½¿ç”¨ shared_data
}
```

---

### é¢˜ç›® 10ï¼šBank Conflict

**çŸ¥è¯†ç‚¹**ï¼šå…±äº«å†…å­˜ Bank Conflict

**é¢˜ç›®**ï¼š
åœ¨å…±äº«å†…å­˜ä¸­ï¼Œå¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bankï¼Œä¼šäº§ç”Ÿ bank conflictã€‚å‡è®¾å…±äº«å†…å­˜æœ‰ 32 ä¸ª bankï¼Œä»¥ä¸‹å“ªç§è®¿é—®æ¨¡å¼ä¼šäº§ç”Ÿ bank conflictï¼Ÿ

A. `shared_data[threadIdx.x]`ï¼ˆthreadIdx.x = 0, 1, 2, ..., 31ï¼‰
B. `shared_data[threadIdx.x * 2]`ï¼ˆthreadIdx.x = 0, 1, 2, ..., 15ï¼‰
C. `shared_data[threadIdx.x * 32]`ï¼ˆthreadIdx.x = 0, 1, 2, ..., 7ï¼‰

**ç­”æ¡ˆ**ï¼š
- **A**ï¼šä¸ä¼šå†²çªï¼ˆæ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸åŒçš„ bankï¼‰
- **B**ï¼šä¸ä¼šå†²çªï¼ˆè®¿é—® bank 0, 2, 4, ..., 30ï¼Œéƒ½æ˜¯å¶æ•° bankï¼‰
- **C**ï¼š**ä¼šäº§ç”Ÿå†²çª**ï¼ˆæ‰€æœ‰çº¿ç¨‹éƒ½è®¿é—® bank 0ï¼‰

**è§£é‡Š**ï¼šbank ç´¢å¼• = å†…å­˜åœ°å€ % 32ï¼Œæ‰€ä»¥ `threadIdx.x * 32` çš„ bank ç´¢å¼•éƒ½æ˜¯ 0ã€‚

---

## ç¬¬å››éƒ¨åˆ†ï¼šå†…å­˜è®¿é—®ä¼˜åŒ–

### é¢˜ç›® 11ï¼šå†…å­˜åˆå¹¶è®¿é—®

**çŸ¥è¯†ç‚¹**ï¼šå†…å­˜åˆå¹¶è®¿é—®

**é¢˜ç›®**ï¼š
ä»¥ä¸‹å“ªç§å†…å­˜è®¿é—®æ¨¡å¼æ˜¯åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰ï¼Ÿ

```cuda
// A
__global__ void kernel_A(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx] = data[idx] * 2.0f;
}

// B
__global__ void kernel_B(float* data, int stride) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx * stride] = data[idx * stride] * 2.0f;  // stride = 100
}

// C
__global__ void kernel_C(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx * 2] = data[idx * 2] * 2.0f;
}
```

**ç­”æ¡ˆ**ï¼š
- **A**ï¼šâœ… åˆå¹¶è®¿é—®ï¼ˆç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜ï¼‰
- **B**ï¼šâŒ éåˆå¹¶è®¿é—®ï¼ˆè®¿é—®é—´éš”å¤ªå¤§ï¼‰
- **C**ï¼šâŒ éåˆå¹¶è®¿é—®ï¼ˆè®¿é—®é—´éš”ä¸º 2ï¼Œä¸æ˜¯è¿ç»­çš„ï¼‰

---

### é¢˜ç›® 12ï¼šå†…å­˜å¯¹é½

**çŸ¥è¯†ç‚¹**ï¼šå†…å­˜å¯¹é½è¦æ±‚

**é¢˜ç›®**ï¼š
CUDA å†…å­˜åˆå¹¶è®¿é—®è¦æ±‚è®¿é—®çš„èµ·å§‹åœ°å€å¿…é¡»å¯¹é½ã€‚ä»¥ä¸‹å“ªç§è®¿é—®æ»¡è¶³å¯¹é½è¦æ±‚ï¼ˆå‡è®¾æ•°æ®æ˜¯ floatï¼Œ4 å­—èŠ‚ï¼‰ï¼Ÿ

A. `data[0]`, `data[1]`, `data[2]`, ...
B. `data[1]`, `data[2]`, `data[3]`, ...
C. `data[32]`, `data[33]`, `data[34]`, ...

**ç­”æ¡ˆ**ï¼š
- **A**ï¼šâœ… å¯¹é½ï¼ˆä» 0 å¼€å§‹ï¼Œ4 å­—èŠ‚å¯¹é½ï¼‰
- **B**ï¼šâŒ ä¸å¯¹é½ï¼ˆä»åç§» 4 å­—èŠ‚å¼€å§‹ï¼‰
- **C**ï¼šâœ… å¯¹é½ï¼ˆä» 128 å­—èŠ‚å¼€å§‹ï¼Œ128 å­—èŠ‚å¯¹é½ï¼Œæ»¡è¶³ 128 å­—èŠ‚å¯¹é½è¦æ±‚ï¼‰

**æ³¨æ„**ï¼šCUDA åˆå¹¶è®¿é—®è¦æ±‚ 128 å­—èŠ‚å¯¹é½ï¼Œæ‰€ä»¥ä» `data[32]` å¼€å§‹ï¼ˆ32 Ã— 4 = 128 å­—èŠ‚ï¼‰æ˜¯å¯¹é½çš„ã€‚

---

### é¢˜ç›® 13ï¼šå…±äº«å†…å­˜ç¼“å­˜

**çŸ¥è¯†ç‚¹**ï¼šä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ•°æ®

**é¢˜ç›®**ï¼š
è¯·å®Œæˆä»¥ä¸‹ä»£ç ï¼Œä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜å…¨å±€å†…å­˜æ•°æ®ï¼š

```cuda
__global__ void cache_kernel(float* input, float* output, int n) {
    __shared__ float cache[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // TODO: ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜
    
    __syncthreads();
    
    // TODO: ä»å…±äº«å†…å­˜è¯»å–å¹¶å¤„ç†
    if (idx < n) {
        output[idx] = cache[tid] * 2.0f;
    }
}
```

**ç­”æ¡ˆ**ï¼š
```cuda
__global__ void cache_kernel(float* input, float* output, int n) {
    __shared__ float cache[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜
    if (idx < n) {
        cache[tid] = input[idx];
    }
    
    __syncthreads();
    
    // ä»å…±äº«å†…å­˜è¯»å–å¹¶å¤„ç†
    if (idx < n) {
        output[idx] = cache[tid] * 2.0f;
    }
}
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šç»¼åˆåº”ç”¨

### é¢˜ç›® 14ï¼šå‘é‡åŠ æ³•

**çŸ¥è¯†ç‚¹**ï¼šç»¼åˆåº”ç”¨ CUDA åŸºç¡€çŸ¥è¯†

**é¢˜ç›®**ï¼š
è¯·å®Œæˆä¸€ä¸ªç®€å•çš„å‘é‡åŠ æ³•å†…æ ¸å‡½æ•°ï¼Œå°†ä¸¤ä¸ªå‘é‡ `a` å’Œ `b` ç›¸åŠ ï¼Œç»“æœå­˜å‚¨åœ¨ `c` ä¸­ï¼š

```cuda
__global__ void vector_add(float* a, float* b, float* c, int n) {
    // TODO: è®¡ç®—å…¨å±€çº¿ç¨‹ ID
    // TODO: æ£€æŸ¥è¾¹ç•Œ
    // TODO: æ‰§è¡ŒåŠ æ³•
}
```

**ç­”æ¡ˆ**ï¼š
```cuda
__global__ void vector_add(float* a, float* b, float* c, int n) {
    // è®¡ç®—å…¨å±€çº¿ç¨‹ ID
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // æ£€æŸ¥è¾¹ç•Œ
    if (idx < n) {
        // æ‰§è¡ŒåŠ æ³•
        c[idx] = a[idx] + b[idx];
    }
}
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```cuda
int n = 1024;
int threads_per_block = 256;
int num_blocks = (n + threads_per_block - 1) / threads_per_block;
vector_add<<<num_blocks, threads_per_block>>>(d_a, d_b, d_c, n);
```

---

### é¢˜ç›® 15ï¼šé”™è¯¯æ£€æŸ¥

**çŸ¥è¯†ç‚¹**ï¼šCUDA é”™è¯¯å¤„ç†

**é¢˜ç›®**ï¼š
ä»¥ä¸‹ä»£ç ç¼ºå°‘é”™è¯¯æ£€æŸ¥ï¼Œè¯·æ·»åŠ é€‚å½“çš„é”™è¯¯æ£€æŸ¥ï¼š

```cuda
int main() {
    float* d_data;
    cudaMalloc(&d_data, 1024 * sizeof(float));
    
    kernel<<<4, 256>>>(d_data);
    
    cudaMemcpy(h_data, d_data, 1024 * sizeof(float), 
               cudaMemcpyDeviceToHost);
    
    cudaFree(d_data);
    return 0;
}
```

**ç­”æ¡ˆ**ï¼š
```cuda
#include <cuda_runtime.h>
#include <stdio.h>

#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d: %s\n", \
                    __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(1); \
        } \
    } while(0)

int main() {
    float* d_data;
    CUDA_CHECK(cudaMalloc(&d_data, 1024 * sizeof(float)));
    
    kernel<<<4, 256>>>(d_data);
    CUDA_CHECK(cudaGetLastError());  // æ£€æŸ¥å†…æ ¸å¯åŠ¨é”™è¯¯
    
    CUDA_CHECK(cudaDeviceSynchronize());  // ç­‰å¾…å†…æ ¸å®Œæˆ
    
    float* h_data = (float*)malloc(1024 * sizeof(float));
    CUDA_CHECK(cudaMemcpy(h_data, d_data, 1024 * sizeof(float), 
                          cudaMemcpyDeviceToHost));
    
    CUDA_CHECK(cudaFree(d_data));
    free(h_data);
    return 0;
}
```

---

## ğŸ“Š ç»ƒä¹ æ€»ç»“

### çŸ¥è¯†ç‚¹è¦†ç›–

- âœ… CUDA ç¼–ç¨‹æ¨¡å‹
- âœ… çº¿ç¨‹å±‚æ¬¡ç»“æ„
- âœ… çº¿ç¨‹æ ‡è¯†ç¬¦å’Œå…¨å±€ ID è®¡ç®—
- âœ… Warp æ¦‚å¿µ
- âœ… GPU å†…å­˜å±‚æ¬¡ç»“æ„
- âœ… å¯„å­˜å™¨ã€å…±äº«å†…å­˜ã€å…¨å±€å†…å­˜
- âœ… å†…å­˜è®¿é—®ä¼˜åŒ–
- âœ… é”™è¯¯å¤„ç†

### å»ºè®®

1. **å…ˆç†è§£å†åšé¢˜**ï¼šç¡®ä¿ç†è§£äº†ç›¸å…³çŸ¥è¯†ç‚¹åå†åšé¢˜
2. **åŠ¨æ‰‹å®è·µ**ï¼šå°è¯•ç¼–å†™å’Œè¿è¡Œä»£ç éªŒè¯ç­”æ¡ˆ
3. **ä¸¾ä¸€åä¸‰**ï¼šç†è§£åŸç†åå¯ä»¥å°è¯•ä¿®æ”¹é¢˜ç›®
4. **æŸ¥é˜…æ–‡æ¡£**ï¼šé‡åˆ°é—®é¢˜æŸ¥é˜… CUDA å®˜æ–¹æ–‡æ¡£

---

**å®Œæˆæ—¥æœŸ**ï¼š________  
**æ­£ç¡®ç‡**ï¼š____ / 15
