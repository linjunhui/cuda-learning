# Flash-Attention å­¦ä¹ èµ„æºé“¾æ¥

## ğŸ“„ è®ºæ–‡èµ„æº

### æ ¸å¿ƒè®ºæ–‡
1. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**
   - è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2205.14135
   - PDF ä¸‹è½½ï¼šhttps://arxiv.org/pdf/2205.14135.pdf
   - ä½œè€…ï¼šTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©
   - å‘è¡¨æ—¶é—´ï¼š2022å¹´5æœˆ

2. **FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning**
   - è®ºæ–‡é“¾æ¥ï¼šhttps://tridao.me/publications/flash2/flash2.pdf
   - ä½œè€…ï¼šTri Dao
   - å‘è¡¨æ—¶é—´ï¼š2023å¹´7æœˆ

3. **FlashAttention-3: Fast and Accurate Attention with Asynchrony, Low-precision, and Memory Compression**
   - åšå®¢ï¼šhttps://tridao.me/blog/2024/flash3/
   - è®ºæ–‡ï¼šhttps://tridao.me/publications/flash3/flash3.pdf
   - ä½œè€…ï¼šTri Dao, Albert Gu
   - å‘è¡¨æ—¶é—´ï¼š2024å¹´

---

## ğŸ’» ä»£ç èµ„æº

### å®˜æ–¹ä»“åº“
1. **Flash-Attention GitHub**
   - åœ°å€ï¼šhttps://github.com/Dao-AILab/flash-attention
   - ä¸»è¦å®ç°ï¼šFlash-Attention 2
   - è¯­è¨€ï¼šPython + CUDA C++

2. **Flash-Attention 3 (Hopper)**
   - åœ°å€ï¼šhttps://github.com/Dao-AILab/flash-attention/tree/main/hopper
   - é’ˆå¯¹ H100 GPU ä¼˜åŒ–
   - æ”¯æŒ FP8 æ•°æ®ç±»å‹

### ç›¸å…³å®ç°
1. **Triton Flash-Attention**
   - åœ°å€ï¼šhttps://github.com/openai/triton
   - ä½¿ç”¨ Triton å®ç°
   - æ›´æ˜“ç†è§£å’Œä¿®æ”¹

---

## ğŸ“š æ–‡æ¡£èµ„æº

### CUDA æ–‡æ¡£
1. **CUDA C++ Programming Guide**
   - åœ°å€ï¼šhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/
   - ç‰ˆæœ¬ï¼šæœ€æ–°ç‰ˆæœ¬
   - å†…å®¹ï¼šCUDA ç¼–ç¨‹å®Œæ•´æŒ‡å—

2. **CUDA Best Practices Guide**
   - åœ°å€ï¼šhttps://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
   - å†…å®¹ï¼šCUDA ä¼˜åŒ–æœ€ä½³å®è·µ

3. **CUDA Runtime API**
   - åœ°å€ï¼šhttps://docs.nvidia.com/cuda/cuda-runtime-api/
   - å†…å®¹ï¼šCUDA Runtime API å‚è€ƒ

### CUTLASS æ–‡æ¡£
1. **CUTLASS GitHub**
   - åœ°å€ï¼šhttps://github.com/NVIDIA/cutlass
   - å†…å®¹ï¼šCUTLASS åº“æºç å’Œæ–‡æ¡£

2. **CUTLASS æ•™ç¨‹**
   - åœ°å€ï¼šhttps://github.com/NVIDIA/cutlass/tree/main/media/docs
   - å†…å®¹ï¼šCUTLASS ä½¿ç”¨æ•™ç¨‹

3. **CuTe æ–‡æ¡£**
   - åœ°å€ï¼šhttps://github.com/NVIDIA/cutlass/tree/main/cute/doc
   - å†…å®¹ï¼šCuTe å¼ é‡æŠ½è±¡æ–‡æ¡£

---

## ğŸ› ï¸ å·¥å…·èµ„æº

### æ€§èƒ½åˆ†æå·¥å…·
1. **Nsight Compute**
   - ä¸‹è½½ï¼šhttps://developer.nvidia.com/nsight-compute
   - åŠŸèƒ½ï¼šå†…æ ¸æ€§èƒ½åˆ†æ
   - å¹³å°ï¼šLinux, Windows

2. **Nsight Systems**
   - ä¸‹è½½ï¼šhttps://developer.nvidia.com/nsight-systems
   - åŠŸèƒ½ï¼šæ•´ä½“æ€§èƒ½åˆ†æ
   - å¹³å°ï¼šLinux, Windows

3. **CUDA-GDB**
   - æ–‡æ¡£ï¼šhttps://docs.nvidia.com/cuda/cuda-gdb/
   - åŠŸèƒ½ï¼šCUDA ç¨‹åºè°ƒè¯•

### å¼€å‘å·¥å…·
1. **CUDA Toolkit**
   - ä¸‹è½½ï¼šhttps://developer.nvidia.com/cuda-downloads
   - ç‰ˆæœ¬ï¼š11.8+ æˆ– 12.0+

2. **PyTorch**
   - å®˜ç½‘ï¼šhttps://pytorch.org/
   - å®‰è£…ï¼šhttps://pytorch.org/get-started/locally/

---

## ğŸ“ æ•™ç¨‹èµ„æº

### è§†é¢‘æ•™ç¨‹
1. **Flash-Attention ä½œè€…è®²è§£**ï¼ˆå¦‚æœæœ‰ï¼‰
   - æœç´¢ YouTube æˆ– Bilibili
   - å…³é”®è¯ï¼šFlashAttention, Tri Dao

2. **CUDA ç¼–ç¨‹æ•™ç¨‹**
   - NVIDIA å®˜æ–¹æ•™ç¨‹
   - åœ¨çº¿è¯¾ç¨‹å¹³å°

### åšå®¢æ–‡ç« 
1. **Flash-Attention åšå®¢**
   - åœ°å€ï¼šhttps://tridao.me/blog/
   - ä½œè€…ï¼šTri Dao
   - å†…å®¹ï¼šFlash-Attention ç›¸å…³æ–‡ç« 

2. **æŠ€æœ¯åšå®¢**
   - æœç´¢ Medium, Towards Data Science
   - å…³é”®è¯ï¼šFlashAttention, CUDA optimization

---

## ğŸ“Š åŸºå‡†æµ‹è¯•

### æ€§èƒ½åŸºå‡†
1. **Flash-Attention Benchmarks**
   - åœ°å€ï¼šhttps://github.com/Dao-AILab/flash-attention/tree/main/benchmarks
   - å†…å®¹ï¼šæ€§èƒ½æµ‹è¯•è„šæœ¬

2. **MLPerf**
   - åœ°å€ï¼šhttps://mlcommons.org/en/inference-edge-21/
   - å†…å®¹ï¼šFlash-Attention åœ¨ MLPerf ä¸­çš„è¡¨ç°

---

## ğŸ”¬ ç ”ç©¶èµ„æº

### ç›¸å…³è®ºæ–‡
1. **Attention Is All You Need** (Transformer)
   - åœ°å€ï¼šhttps://arxiv.org/abs/1706.03762
   - å†…å®¹ï¼šTransformer åŸå§‹è®ºæ–‡

2. **Efficient Attention**
   - æœç´¢ç›¸å…³è®ºæ–‡
   - å…³é”®è¯ï¼šefficient attention, sparse attention

### å­¦æœ¯ä¼šè®®
1. **NeurIPS**
2. **ICML**
3. **ICLR**

---

## ğŸ’¬ ç¤¾åŒºèµ„æº

### è®¨è®ºåŒº
1. **GitHub Issues**
   - åœ°å€ï¼šhttps://github.com/Dao-AILab/flash-attention/issues
   - å†…å®¹ï¼šé—®é¢˜è®¨è®ºå’Œè§£ç­”

2. **Reddit**
   - r/MachineLearning
   - r/CUDA

3. **Stack Overflow**
   - æ ‡ç­¾ï¼šflash-attention, cuda

### ç¤¾äº¤åª’ä½“
1. **Twitter/X**
   - å…³æ³¨ï¼š@tri_dao
   - å…³é”®è¯ï¼š#FlashAttention

---

## ğŸ“ å­¦ä¹ ç¬”è®°èµ„æº

### ä»–äººç¬”è®°
1. **GitHub å­¦ä¹ ç¬”è®°**
   - æœç´¢ï¼šflash-attention notes, flash-attention tutorial
   - å¹³å°ï¼šGitHub, Medium

2. **ä¸­æ–‡èµ„æº**
   - çŸ¥ä¹ï¼šFlash-Attention ç›¸å…³æ–‡ç« 
   - åšå®¢å›­ï¼šæŠ€æœ¯åšå®¢

---

## ğŸ¯ æ¨èå­¦ä¹ è·¯å¾„

### åˆå­¦è€…
1. é˜…è¯» FlashAttention è®ºæ–‡ï¼ˆç®—æ³•éƒ¨åˆ†ï¼‰
2. é˜…è¯» CUDA ç¼–ç¨‹æŒ‡å—ï¼ˆåŸºç¡€éƒ¨åˆ†ï¼‰
3. è¿è¡Œ Flash-Attention ç¤ºä¾‹
4. é˜…è¯»æºç ï¼ˆä»ç®€å•éƒ¨åˆ†å¼€å§‹ï¼‰

### è¿›é˜¶å­¦ä¹ 
1. æ·±å…¥ç†è§£ Online Softmax
2. å­¦ä¹  CUTLASS/CuTe
3. åˆ†ææ€§èƒ½ä¼˜åŒ–æŠ€æœ¯
4. å°è¯•ä¿®æ”¹å’Œä¼˜åŒ–

### é«˜çº§ç ”ç©¶
1. é˜…è¯» FlashAttention-2/3 è®ºæ–‡
2. ç†è§£ç¡¬ä»¶ç‰¹æ€§
3. å®ç°è‡ªå·±çš„ä¼˜åŒ–ç‰ˆæœ¬
4. å‘è¡¨ç ”ç©¶æˆæœ

---

## ğŸ“… èµ„æºæ›´æ–°

**æœ€åæ›´æ–°**ï¼š2024å¹´

**ç»´æŠ¤è®¡åˆ’**ï¼š
- å®šæœŸæ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
- æ·»åŠ æ–°çš„å­¦ä¹ èµ„æº
- æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯

---

**æç¤º**ï¼šå»ºè®®å°†é‡è¦è®ºæ–‡å’Œæ–‡æ¡£ä¸‹è½½åˆ°æœ¬åœ°ï¼Œæ–¹ä¾¿éšæ—¶æŸ¥é˜…ã€‚
